2022-07-05 02:41:10,926 - INFO  [main:Log4jControllerRegistration$@31] - Registered kafka:type=kafka.Log4jController MBean
2022-07-05 02:41:10,973 - INFO  [main:Environment@109] - Server environment:zookeeper.version=3.5.9-83df9301aa5c2a5d284a9940177808c01bc35cef, built on 01/06/2021 20:03 GMT
2022-07-05 02:41:10,974 - INFO  [main:Environment@109] - Server environment:host.name=razor15
2022-07-05 02:41:10,974 - INFO  [main:Environment@109] - Server environment:java.version=1.8.0_275
2022-07-05 02:41:10,974 - INFO  [main:Environment@109] - Server environment:java.vendor=Private Build
2022-07-05 02:41:10,974 - INFO  [main:Environment@109] - Server environment:java.home=/usr/lib/jvm/java-8-openjdk-amd64/jre
2022-07-05 02:41:10,974 - INFO  [main:Environment@109] - Server environment:java.class.path=/home/tonypan/flaky-reproduction/experiment/kafka-12508/junit-platform-console-standalone-1.7.0.jar
2022-07-05 02:41:10,974 - INFO  [main:Environment@109] - Server environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib
2022-07-05 02:41:10,975 - INFO  [main:Environment@109] - Server environment:java.io.tmpdir=/tmp
2022-07-05 02:41:10,975 - INFO  [main:Environment@109] - Server environment:java.compiler=<NA>
2022-07-05 02:41:10,975 - INFO  [main:Environment@109] - Server environment:os.name=Linux
2022-07-05 02:41:10,975 - INFO  [main:Environment@109] - Server environment:os.arch=amd64
2022-07-05 02:41:10,975 - INFO  [main:Environment@109] - Server environment:os.version=4.15.0-128-generic
2022-07-05 02:41:10,975 - INFO  [main:Environment@109] - Server environment:user.name=tonypan
2022-07-05 02:41:10,975 - INFO  [main:Environment@109] - Server environment:user.home=/home/tonypan
2022-07-05 02:41:10,975 - INFO  [main:Environment@109] - Server environment:user.dir=/home/tonypan/flaky-reproduction/experiment/kafka-12508
2022-07-05 02:41:10,976 - INFO  [main:Environment@109] - Server environment:os.memory.free=400MB
2022-07-05 02:41:10,976 - INFO  [main:Environment@109] - Server environment:os.memory.max=7051MB
2022-07-05 02:41:10,976 - INFO  [main:Environment@109] - Server environment:os.memory.total=475MB
2022-07-05 02:41:10,982 - INFO  [main:FileTxnSnapLog@115] - zookeeper.snapshot.trust.empty : false
2022-07-05 02:41:11,006 - INFO  [main:ZKDatabase@117] - zookeeper.snapshotSizeFactor = 0.33
2022-07-05 02:41:11,012 - INFO  [main:ZooKeeperServer@953] - minSessionTimeout set to 1600
2022-07-05 02:41:11,013 - INFO  [main:ZooKeeperServer@962] - maxSessionTimeout set to 16000
2022-07-05 02:41:11,014 - INFO  [main:ZooKeeperServer@181] - Created server with tickTime 800 minSessionTimeout 1600 maxSessionTimeout 16000 datadir /tmp/kafka-1202998252615321605/version-2 snapdir /tmp/kafka-1190627035544723862/version-2
2022-07-05 02:41:11,026 - INFO  [main:NIOServerCnxnFactory@673] - Configuring NIO connection handler with 10s sessionless connection timeout, 3 selector thread(s), 40 worker threads, and 64 kB direct buffers.
2022-07-05 02:41:11,032 - INFO  [main:NIOServerCnxnFactory@686] - binding to port /127.0.0.1:0
2022-07-05 02:41:11,041 - INFO  [main:FileTxnSnapLog@404] - Snapshotting: 0x0 to /tmp/kafka-1190627035544723862/version-2/snapshot.0
2022-07-05 02:41:11,045 - INFO  [main:FileTxnSnapLog@404] - Snapshotting: 0x0 to /tmp/kafka-1190627035544723862/version-2/snapshot.0
2022-07-05 02:41:11,070 - INFO  [ProcessThread(sid:0 cport:39357)::PrepRequestProcessor@132] - PrepRequestProcessor (sid:0) started, reconfigEnabled=false
2022-07-05 02:41:11,402 - INFO  [main:AbstractConfig@372] - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 0
	host.name = localhost
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.8-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/junit5462504141538786114/junit6112278271730470661
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.8-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000000
	metadata.log.dir = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = -1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 5
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:39357
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 10000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

2022-07-05 02:41:11,423 - INFO  [main:X509Util@79] - Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation
2022-07-05 02:41:11,517 - INFO  [main:Logging@66] - starting
2022-07-05 02:41:11,518 - INFO  [main:Logging@66] - Connecting to zookeeper on 127.0.0.1:39357
2022-07-05 02:41:11,541 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:39357.
2022-07-05 02:41:11,547 - INFO  [main:Environment@109] - Client environment:zookeeper.version=3.5.9-83df9301aa5c2a5d284a9940177808c01bc35cef, built on 01/06/2021 20:03 GMT
2022-07-05 02:41:11,547 - INFO  [main:Environment@109] - Client environment:host.name=razor15
2022-07-05 02:41:11,548 - INFO  [main:Environment@109] - Client environment:java.version=1.8.0_275
2022-07-05 02:41:11,548 - INFO  [main:Environment@109] - Client environment:java.vendor=Private Build
2022-07-05 02:41:11,548 - INFO  [main:Environment@109] - Client environment:java.home=/usr/lib/jvm/java-8-openjdk-amd64/jre
2022-07-05 02:41:11,548 - INFO  [main:Environment@109] - Client environment:java.class.path=/home/tonypan/flaky-reproduction/experiment/kafka-12508/junit-platform-console-standalone-1.7.0.jar
2022-07-05 02:41:11,548 - INFO  [main:Environment@109] - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib
2022-07-05 02:41:11,548 - INFO  [main:Environment@109] - Client environment:java.io.tmpdir=/tmp
2022-07-05 02:41:11,548 - INFO  [main:Environment@109] - Client environment:java.compiler=<NA>
2022-07-05 02:41:11,548 - INFO  [main:Environment@109] - Client environment:os.name=Linux
2022-07-05 02:41:11,548 - INFO  [main:Environment@109] - Client environment:os.arch=amd64
2022-07-05 02:41:11,549 - INFO  [main:Environment@109] - Client environment:os.version=4.15.0-128-generic
2022-07-05 02:41:11,549 - INFO  [main:Environment@109] - Client environment:user.name=tonypan
2022-07-05 02:41:11,549 - INFO  [main:Environment@109] - Client environment:user.home=/home/tonypan
2022-07-05 02:41:11,549 - INFO  [main:Environment@109] - Client environment:user.dir=/home/tonypan/flaky-reproduction/experiment/kafka-12508
2022-07-05 02:41:11,549 - INFO  [main:Environment@109] - Client environment:os.memory.free=337MB
2022-07-05 02:41:11,549 - INFO  [main:Environment@109] - Client environment:os.memory.max=7051MB
2022-07-05 02:41:11,549 - INFO  [main:Environment@109] - Client environment:os.memory.total=443MB
2022-07-05 02:41:11,554 - INFO  [main:ZooKeeper@868] - Initiating client connection, connectString=127.0.0.1:39357 sessionTimeout=10000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@31add175
2022-07-05 02:41:11,558 - INFO  [main:ClientCnxnSocket@237] - jute.maxbuffer value is 4194304 Bytes
2022-07-05 02:41:11,566 - INFO  [main:ClientCnxn@1653] - zookeeper.request.timeout value is 0. feature enabled=
2022-07-05 02:41:11,567 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Waiting until connected.
2022-07-05 02:41:11,574 - INFO  [main-SendThread(127.0.0.1:39357):ClientCnxn$SendThread@1112] - Opening socket connection to server localhost/127.0.0.1:39357. Will not attempt to authenticate using SASL (unknown error)
2022-07-05 02:41:11,575 - INFO  [main-SendThread(127.0.0.1:39357):ClientCnxn$SendThread@959] - Socket connection established, initiating session, client: /127.0.0.1:49812, server: localhost/127.0.0.1:39357
2022-07-05 02:41:11,585 - INFO  [SyncThread:0:FileTxnLog@218] - Creating new log file: log.1
2022-07-05 02:41:11,595 - INFO  [main-SendThread(127.0.0.1:39357):ClientCnxn$SendThread@1394] - Session establishment complete on server localhost/127.0.0.1:39357, sessionid = 0x100a45d6df60000, negotiated timeout = 10000
2022-07-05 02:41:11,600 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Connected.
2022-07-05 02:41:11,713 - INFO  [feature-zk-node-event-process-thread:Logging@66] - [feature-zk-node-event-process-thread]: Starting
2022-07-05 02:41:11,726 - INFO  [feature-zk-node-event-process-thread:Logging@66] - Feature ZK node at path: /feature does not exist
2022-07-05 02:41:11,726 - INFO  [feature-zk-node-event-process-thread:Logging@66] - Cleared cache
2022-07-05 02:41:12,002 - INFO  [main:Logging@66] - Cluster ID = D5RV4S7cRSS7tsr75uDRDw
2022-07-05 02:41:12,006 - WARN  [main:Logging@70] - No meta.properties file under dir /tmp/junit5462504141538786114/junit6112278271730470661/meta.properties
2022-07-05 02:41:12,068 - INFO  [main:AbstractConfig@372] - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 0
	host.name = localhost
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.8-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/junit5462504141538786114/junit6112278271730470661
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.8-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000000
	metadata.log.dir = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = -1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 5
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:39357
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 10000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

2022-07-05 02:41:12,079 - INFO  [main:AbstractConfig@372] - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 0
	host.name = localhost
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.8-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/junit5462504141538786114/junit6112278271730470661
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.8-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000000
	metadata.log.dir = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = -1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 5
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:39357
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 10000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

2022-07-05 02:41:12,112 - INFO  [ThrottledChannelReaper-Fetch:Logging@66] - [ThrottledChannelReaper-Fetch]: Starting
2022-07-05 02:41:12,112 - INFO  [ThrottledChannelReaper-Produce:Logging@66] - [ThrottledChannelReaper-Produce]: Starting
2022-07-05 02:41:12,114 - INFO  [ThrottledChannelReaper-Request:Logging@66] - [ThrottledChannelReaper-Request]: Starting
2022-07-05 02:41:12,115 - INFO  [ThrottledChannelReaper-ControllerMutation:Logging@66] - [ThrottledChannelReaper-ControllerMutation]: Starting
2022-07-05 02:41:12,147 - INFO  [main:Logging@66] - Loading logs from log dirs ArraySeq(/tmp/junit5462504141538786114/junit6112278271730470661)
2022-07-05 02:41:12,150 - INFO  [main:Logging@66] - Attempting recovery for all logs in /tmp/junit5462504141538786114/junit6112278271730470661 since no clean shutdown file was found
2022-07-05 02:41:12,155 - INFO  [main:Logging@66] - Loaded 0 logs in 0ms.
2022-07-05 02:41:12,156 - INFO  [main:Logging@66] - Starting log cleanup with a period of 300000 ms.
2022-07-05 02:41:12,163 - INFO  [main:Logging@66] - Starting log flusher with a default period of 9223372036854775807 ms.
2022-07-05 02:41:12,545 - INFO  [main:Logging@66] - Updated connection-accept-rate max connection creation rate to 2147483647
2022-07-05 02:41:12,548 - INFO  [main:Logging@66] - Awaiting socket connections on localhost:46125.
2022-07-05 02:41:12,587 - INFO  [main:Logging@66] - [SocketServer listenerType=ZK_BROKER, nodeId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT)
2022-07-05 02:41:12,615 - INFO  [BrokerToControllerChannelManager broker=0 name=alterIsr:Logging@66] - [BrokerToControllerChannelManager broker=0 name=alterIsr]: Starting
2022-07-05 02:41:12,638 - INFO  [ExpirationReaper-0-Produce:Logging@66] - [ExpirationReaper-0-Produce]: Starting
2022-07-05 02:41:12,638 - INFO  [ExpirationReaper-0-Fetch:Logging@66] - [ExpirationReaper-0-Fetch]: Starting
2022-07-05 02:41:12,638 - INFO  [ExpirationReaper-0-DeleteRecords:Logging@66] - [ExpirationReaper-0-DeleteRecords]: Starting
2022-07-05 02:41:12,639 - INFO  [ExpirationReaper-0-ElectLeader:Logging@66] - [ExpirationReaper-0-ElectLeader]: Starting
2022-07-05 02:41:12,652 - INFO  [LogDirFailureHandler:Logging@66] - [LogDirFailureHandler]: Starting
2022-07-05 02:41:12,693 - INFO  [main:Logging@66] - Creating /brokers/ids/0 (is it secure? false)
2022-07-05 02:41:12,717 - INFO  [main:Logging@66] - Stat of the created znode at /brokers/ids/0 is: 25,25,1657003272710,1657003272710,1,0,0,72238315221680128,204,0,25

2022-07-05 02:41:12,718 - INFO  [main:Logging@66] - Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://localhost:46125, czxid (broker epoch): 25
2022-07-05 02:41:12,780 - INFO  [ExpirationReaper-0-topic:Logging@66] - [ExpirationReaper-0-topic]: Starting
2022-07-05 02:41:12,785 - INFO  [ExpirationReaper-0-Heartbeat:Logging@66] - [ExpirationReaper-0-Heartbeat]: Starting
2022-07-05 02:41:12,786 - INFO  [ExpirationReaper-0-Rebalance:Logging@66] - [ExpirationReaper-0-Rebalance]: Starting
2022-07-05 02:41:12,788 - INFO  [controller-event-thread:Logging@66] - Successfully created /controller_epoch with initial epoch 0
2022-07-05 02:41:12,799 - INFO  [main-EventThread:Logging@66] - Feature ZK node created at path: /feature
2022-07-05 02:41:12,803 - INFO  [main:Logging@66] - [GroupCoordinator 0]: Starting up.
2022-07-05 02:41:12,807 - INFO  [main:Logging@66] - [GroupCoordinator 0]: Startup complete.
2022-07-05 02:41:12,830 - INFO  [main:Logging@66] - [ProducerId Manager 0]: Acquired new producerId block (brokerId:0,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1
2022-07-05 02:41:12,831 - INFO  [main:Logging@66] - [TransactionCoordinator id=0] Starting up.
2022-07-05 02:41:12,834 - INFO  [TxnMarkerSenderThread-0:Logging@66] - [Transaction Marker Channel Manager 0]: Starting
2022-07-05 02:41:12,834 - INFO  [main:Logging@66] - [TransactionCoordinator id=0] Startup complete.
2022-07-05 02:41:12,834 - INFO  [feature-zk-node-event-process-thread:Logging@66] - Updated cache from existing <empty> to latest FinalizedFeaturesAndEpoch(features=Features{}, epoch=0).
2022-07-05 02:41:12,859 - INFO  [ExpirationReaper-0-AlterAcls:Logging@66] - [ExpirationReaper-0-AlterAcls]: Starting
2022-07-05 02:41:12,885 - INFO  [/config/changes-event-process-thread:Logging@66] - [/config/changes-event-process-thread]: Starting
2022-07-05 02:41:12,892 - INFO  [main:Logging@66] - [SocketServer listenerType=ZK_BROKER, nodeId=0] Starting socket server acceptors and processors
2022-07-05 02:41:12,895 - INFO  [main:Logging@66] - [SocketServer listenerType=ZK_BROKER, nodeId=0] Started data-plane acceptor and processor(s) for endpoint : ListenerName(PLAINTEXT)
2022-07-05 02:41:12,896 - INFO  [main:Logging@66] - [SocketServer listenerType=ZK_BROKER, nodeId=0] Started socket server acceptors and processors
2022-07-05 02:41:12,897 - WARN  [main:AppInfoParser@46] - Error while loading kafka-version.properties: null
2022-07-05 02:41:12,898 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2022-07-05 02:41:12,898 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2022-07-05 02:41:12,898 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1657003270732
2022-07-05 02:41:12,899 - INFO  [main:Logging@66] - [KafkaServer id=0] started
2022-07-05 02:41:12,916 - INFO  [main:AbstractConfig@372] - AdminClientConfig values: 
	bootstrap.servers = [localhost:46125]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2022-07-05 02:41:12,938 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2022-07-05 02:41:12,938 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2022-07-05 02:41:12,938 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1657003272938
2022-07-05 02:41:12,996 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Creating topic inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0))
2022-07-05 02:41:13,026 - INFO  [BrokerToControllerChannelManager broker=0 name=alterIsr:Logging@66] - [BrokerToControllerChannelManager broker=0 name=alterIsr]: Recorded new controller, from now on will use broker localhost:46125 (id: 0 rack: null)
2022-07-05 02:41:13,075 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0)
2022-07-05 02:41:13,143 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0, dir=/tmp/junit5462504141538786114/junit6112278271730470661] Loading producer state till offset 0 with message format version 2
2022-07-05 02:41:13,146 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 in /tmp/junit5462504141538786114/junit6112278271730470661/inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.8-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2022-07-05 02:41:13,148 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 broker=0] No checkpointed highwatermark is found for partition inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0
2022-07-05 02:41:13,148 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 broker=0] Log loaded for partition inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 with initial high watermark 0
2022-07-05 02:41:13,175 - INFO  [kafka-admin-client-thread | adminclient-1:AppInfoParser@83] - App info kafka.admin.client for adminclient-1 unregistered
2022-07-05 02:41:13,178 - INFO  [kafka-admin-client-thread | adminclient-1:Metrics@659] - Metrics scheduler closed
2022-07-05 02:41:13,178 - INFO  [kafka-admin-client-thread | adminclient-1:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:41:13,178 - INFO  [kafka-admin-client-thread | adminclient-1:Metrics@669] - Metrics reporters closed
2022-07-05 02:41:13,181 - INFO  [main:AbstractConfig@372] - AdminClientConfig values: 
	bootstrap.servers = [localhost:46125]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2022-07-05 02:41:13,183 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2022-07-05 02:41:13,183 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2022-07-05 02:41:13,183 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1657003273183
2022-07-05 02:41:13,194 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Creating topic outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0))
2022-07-05 02:41:13,206 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0)
2022-07-05 02:41:13,208 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Log partition=outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0, dir=/tmp/junit5462504141538786114/junit6112278271730470661] Loading producer state till offset 0 with message format version 2
2022-07-05 02:41:13,209 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Created log for partition outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 in /tmp/junit5462504141538786114/junit6112278271730470661/outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.8-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2022-07-05 02:41:13,210 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 broker=0] No checkpointed highwatermark is found for partition outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0
2022-07-05 02:41:13,210 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 broker=0] Log loaded for partition outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 with initial high watermark 0
2022-07-05 02:41:13,214 - INFO  [kafka-admin-client-thread | adminclient-2:AppInfoParser@83] - App info kafka.admin.client for adminclient-2 unregistered
2022-07-05 02:41:13,215 - INFO  [kafka-admin-client-thread | adminclient-2:Metrics@659] - Metrics scheduler closed
2022-07-05 02:41:13,215 - INFO  [kafka-admin-client-thread | adminclient-2:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:41:13,215 - INFO  [kafka-admin-client-thread | adminclient-2:Metrics@669] - Metrics reporters closed
2022-07-05 02:41:13,266 - INFO  [main:AbstractConfig@372] - StreamsConfig values: 
	acceptable.recovery.lag = 10000
	application.id = appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover
	application.server = 
	bootstrap.servers = [localhost:46125]
	buffered.records.per.partition = 1000
	built.in.metrics.version = latest
	cache.max.bytes.buffering = 0
	client.id = 
	commit.interval.ms = 300000
	connections.max.idle.ms = 540000
	default.deserialization.exception.handler = class org.apache.kafka.streams.errors.LogAndFailExceptionHandler
	default.key.serde = class org.apache.kafka.common.serialization.Serdes$IntegerSerde
	default.production.exception.handler = class org.apache.kafka.streams.errors.DefaultProductionExceptionHandler
	default.timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	default.value.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	default.windowed.key.serde.inner = null
	default.windowed.value.serde.inner = null
	max.task.idle.ms = 0
	max.warmup.replicas = 2
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	probing.rebalance.interval.ms = 600000
	processing.guarantee = at_least_once
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retries = 0
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 600000
	state.dir = /tmp/kafka-8405723641700752615
	task.timeout.ms = 300000
	topology.optimization = none
	upgrade.from = null
	window.size.ms = null
	windowstore.changelog.additional.retention.ms = 86400000

2022-07-05 02:41:13,297 - WARN  [main:StateDirectory@138] - Using an OS temp directory in the state.dir property can cause failures with writing the checkpoint file due to the fact that this directory can be cleared by the OS. Resolved state.dir: [/tmp/kafka-8405723641700752615]
2022-07-05 02:41:13,298 - INFO  [main:StateDirectory@212] - No process id found on disk, got fresh process id 08a947c2-b47a-40c4-875b-df1d7bccb953
2022-07-05 02:41:13,331 - INFO  [main:AbstractConfig@372] - AdminClientConfig values: 
	bootstrap.servers = [localhost:46125]
	client.dns.lookup = use_all_dns_ips
	client.id = appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-admin
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2022-07-05 02:41:13,333 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2022-07-05 02:41:13,333 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2022-07-05 02:41:13,333 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1657003273333
2022-07-05 02:41:13,337 - WARN  [main:ClientMetrics@55] - Error while loading kafka-streams-version.properties
java.lang.NullPointerException
	at java.util.Properties$LineReader.readLine(Properties.java:434)
	at java.util.Properties.load0(Properties.java:353)
	at java.util.Properties.load(Properties.java:341)
	at org.apache.kafka.streams.internals.metrics.ClientMetrics.<clinit>(ClientMetrics.java:53)
	at org.apache.kafka.streams.KafkaStreams.<init>(KafkaStreams.java:825)
	at org.apache.kafka.streams.KafkaStreams.<init>(KafkaStreams.java:781)
	at org.apache.kafka.streams.KafkaStreams.<init>(KafkaStreams.java:691)
	at org.apache.kafka.streams.integration.EmitOnChangeIntegrationTest.shouldEmitSameRecordAfterFailover(EmitOnChangeIntegrationTest.java:101)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
2022-07-05 02:41:13,340 - INFO  [main:KafkaStreams@825] - stream-client [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953] Kafka Streams version: unknown
2022-07-05 02:41:13,340 - INFO  [main:KafkaStreams@826] - stream-client [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953] Kafka Streams commit ID: unknown
2022-07-05 02:41:13,348 - INFO  [main:StreamThread@337] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1] Creating restore consumer client
2022-07-05 02:41:13,352 - INFO  [main:AbstractConfig@372] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [localhost:46125]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1-restore-consumer
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = false
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2022-07-05 02:41:13,372 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2022-07-05 02:41:13,373 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2022-07-05 02:41:13,373 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1657003273372
2022-07-05 02:41:13,379 - INFO  [main:ActiveTaskCreator@96] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1] Creating thread producer client
2022-07-05 02:41:13,384 - INFO  [main:AbstractConfig@372] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:46125]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-07-05 02:41:13,396 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2022-07-05 02:41:13,396 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2022-07-05 02:41:13,396 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1657003273396
2022-07-05 02:41:13,402 - INFO  [main:StreamThread@389] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1] Creating consumer client
2022-07-05 02:41:13,403 - INFO  [kafka-producer-network-thread | appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1-producer:Metadata@279] - [Producer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1-producer] Cluster ID: D5RV4S7cRSS7tsr75uDRDw
2022-07-05 02:41:13,405 - INFO  [main:AbstractConfig@372] - ConsumerConfig values: 
	allow.auto.create.topics = false
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:46125]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1-consumer
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = false
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2022-07-05 02:41:13,420 - INFO  [main:AssignorConfiguration@114] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1-consumer] Cooperative rebalancing enabled now
2022-07-05 02:41:13,438 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2022-07-05 02:41:13,438 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2022-07-05 02:41:13,438 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1657003273438
2022-07-05 02:41:13,447 - INFO  [main:KafkaStreams@321] - stream-client [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953] State transition from CREATED to REBALANCING
2022-07-05 02:41:13,448 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:StreamThread@550] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1] Starting
2022-07-05 02:41:13,448 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:StreamThread@230] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1] State transition from CREATED to STARTING
2022-07-05 02:41:13,448 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:KafkaConsumer@968] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Subscribed to topic(s): inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover
2022-07-05 02:41:13,460 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:Metadata@279] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Cluster ID: D5RV4S7cRSS7tsr75uDRDw
2022-07-05 02:41:13,462 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Creating topic __consumer_offsets with configuration {segment.bytes=104857600, cleanup.policy=compact, compression.type=producer} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0))
2022-07-05 02:41:13,478 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(__consumer_offsets-4, __consumer_offsets-3, __consumer_offsets-2, __consumer_offsets-0, __consumer_offsets-1)
2022-07-05 02:41:13,482 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-3, dir=/tmp/junit5462504141538786114/junit6112278271730470661] Loading producer state till offset 0 with message format version 2
2022-07-05 02:41:13,482 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-3 in /tmp/junit5462504141538786114/junit6112278271730470661/__consumer_offsets-3 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.8-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2022-07-05 02:41:13,483 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3
2022-07-05 02:41:13,483 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-3 broker=0] Log loaded for partition __consumer_offsets-3 with initial high watermark 0
2022-07-05 02:41:13,486 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-2, dir=/tmp/junit5462504141538786114/junit6112278271730470661] Loading producer state till offset 0 with message format version 2
2022-07-05 02:41:13,487 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-2 in /tmp/junit5462504141538786114/junit6112278271730470661/__consumer_offsets-2 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.8-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2022-07-05 02:41:13,487 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2
2022-07-05 02:41:13,487 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-2 broker=0] Log loaded for partition __consumer_offsets-2 with initial high watermark 0
2022-07-05 02:41:13,494 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-4, dir=/tmp/junit5462504141538786114/junit6112278271730470661] Loading producer state till offset 0 with message format version 2
2022-07-05 02:41:13,495 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-4 in /tmp/junit5462504141538786114/junit6112278271730470661/__consumer_offsets-4 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.8-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2022-07-05 02:41:13,495 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4
2022-07-05 02:41:13,495 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-4 broker=0] Log loaded for partition __consumer_offsets-4 with initial high watermark 0
2022-07-05 02:41:13,503 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-1, dir=/tmp/junit5462504141538786114/junit6112278271730470661] Loading producer state till offset 0 with message format version 2
2022-07-05 02:41:13,503 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-1 in /tmp/junit5462504141538786114/junit6112278271730470661/__consumer_offsets-1 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.8-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2022-07-05 02:41:13,503 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1
2022-07-05 02:41:13,504 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-1 broker=0] Log loaded for partition __consumer_offsets-1 with initial high watermark 0
2022-07-05 02:41:13,511 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-0, dir=/tmp/junit5462504141538786114/junit6112278271730470661] Loading producer state till offset 0 with message format version 2
2022-07-05 02:41:13,512 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-0 in /tmp/junit5462504141538786114/junit6112278271730470661/__consumer_offsets-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.8-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2022-07-05 02:41:13,512 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0
2022-07-05 02:41:13,512 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-0 broker=0] Log loaded for partition __consumer_offsets-0 with initial high watermark 0
2022-07-05 02:41:13,518 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Elected as the group coordinator for partition 3
2022-07-05 02:41:13,519 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3
2022-07-05 02:41:13,520 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Elected as the group coordinator for partition 2
2022-07-05 02:41:13,520 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2
2022-07-05 02:41:13,520 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Elected as the group coordinator for partition 4
2022-07-05 02:41:13,520 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4
2022-07-05 02:41:13,520 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Elected as the group coordinator for partition 1
2022-07-05 02:41:13,520 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1
2022-07-05 02:41:13,520 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Elected as the group coordinator for partition 0
2022-07-05 02:41:13,520 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0
2022-07-05 02:41:13,523 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 4 milliseconds, of which 1 milliseconds was spent in the scheduler.
2022-07-05 02:41:13,523 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 3 milliseconds, of which 3 milliseconds was spent in the scheduler.
2022-07-05 02:41:13,524 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 4 milliseconds, of which 3 milliseconds was spent in the scheduler.
2022-07-05 02:41:13,524 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 4 milliseconds, of which 4 milliseconds was spent in the scheduler.
2022-07-05 02:41:13,524 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 4 milliseconds, of which 4 milliseconds was spent in the scheduler.
2022-07-05 02:41:13,552 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:StreamThread@829] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update
2022-07-05 02:41:13,557 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:AbstractCoordinator$FindCoordinatorResponseHandler@844] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Discovered group coordinator localhost:46125 (id: 2147483647 rack: null)
2022-07-05 02:41:13,558 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:AbstractCoordinator@534] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] (Re-)joining group
2022-07-05 02:41:13,577 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [GroupCoordinator 0]: Dynamic member with unknown member id joins group appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover in Empty state. Created a new member id appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1-consumer-096ae404-e687-46fa-84db-c2ffa87c44cf and request the member to rejoin with this id.
2022-07-05 02:41:13,581 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:AbstractCoordinator@982] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Request joining group due to: need to re-join with the given member-id
2022-07-05 02:41:13,582 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:AbstractCoordinator@534] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] (Re-)joining group
2022-07-05 02:41:13,586 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover in state PreparingRebalance with old generation 0 (__consumer_offsets-0) (reason: Adding new member appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1-consumer-096ae404-e687-46fa-84db-c2ffa87c44cf with group instance id None)
2022-07-05 02:41:13,591 - INFO  [executor-Rebalance:Logging@66] - [GroupCoordinator 0]: Stabilized group appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover generation 1 (__consumer_offsets-0) with 1 members
2022-07-05 02:41:13,593 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Successfully joined group with generation Generation{generationId=1, memberId='appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1-consumer-096ae404-e687-46fa-84db-c2ffa87c44cf', protocol='stream'}
2022-07-05 02:41:13,609 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Creating topic appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog with configuration {message.timestamp.type=CreateTime, cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0))
2022-07-05 02:41:13,619 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0)
2022-07-05 02:41:13,622 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Log partition=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0, dir=/tmp/junit5462504141538786114/junit6112278271730470661] Loading producer state till offset 0 with message format version 2
2022-07-05 02:41:13,623 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Created log for partition appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0 in /tmp/junit5462504141538786114/junit6112278271730470661/appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.8-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2022-07-05 02:41:13,623 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0 broker=0] No checkpointed highwatermark is found for partition appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0
2022-07-05 02:41:13,623 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0 broker=0] Log loaded for partition appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0 with initial high watermark 0
2022-07-05 02:41:13,635 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:HighAvailabilityTaskAssignor@95] - Decided on assignment: {08a947c2-b47a-40c4-875b-df1d7bccb953=[activeTasks: ([0_0]) standbyTasks: ([]) prevActiveTasks: ([]) prevStandbyTasks: ([]) changelogOffsetTotalsByTask: ([]) taskLagTotals: ([0_0=0]) capacity: 1 assigned: 1]} with no followup probing rebalance.
2022-07-05 02:41:13,635 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:StreamsPartitionAssignor@595] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1-consumer] Assigned tasks [0_0] including stateful [0_0] to clients as: 
08a947c2-b47a-40c4-875b-df1d7bccb953=[activeTasks: ([0_0]) standbyTasks: ([])].
2022-07-05 02:41:13,639 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:StreamsPartitionAssignor@803] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1-consumer] Client 08a947c2-b47a-40c4-875b-df1d7bccb953 per-consumer assignment:
	prev owned active {}
	prev owned standby {appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1-consumer-096ae404-e687-46fa-84db-c2ffa87c44cf=[]}
	assigned active {appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1-consumer-096ae404-e687-46fa-84db-c2ffa87c44cf=[0_0]}
	revoking active {}
	assigned standby {}

2022-07-05 02:41:13,639 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:StreamsPartitionAssignor@822] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1-consumer] Finished stable assignment of tasks, no followup rebalances required.
2022-07-05 02:41:13,640 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:ConsumerCoordinator@626] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Finished assignment for group at generation 1: {appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1-consumer-096ae404-e687-46fa-84db-c2ffa87c44cf=Assignment(partitions=[inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0], userDataSize=48)}
2022-07-05 02:41:13,646 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover for generation 1. The group has 1 members, 0 of which are static.
2022-07-05 02:41:13,690 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:AbstractCoordinator$SyncGroupResponseHandler@756] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Successfully synced group in generation Generation{generationId=1, memberId='appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1-consumer-096ae404-e687-46fa-84db-c2ffa87c44cf', protocol='stream'}
2022-07-05 02:41:13,690 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:ConsumerCoordinator@392] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Updating assignment with
	Assigned partitions:                       [inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0]
	Current owned partitions:                  []
	Added partitions (assigned - owned):       [inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0]
	Revoked partitions (owned - assigned):     []

2022-07-05 02:41:13,690 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:ConsumerCoordinator@276] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Notifying assignor about the new Assignment(partitions=[inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0], userDataSize=48)
2022-07-05 02:41:13,691 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:StreamsPartitionAssignor@1306] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1-consumer] No followup rebalance was requested, resetting the rebalance schedule.
2022-07-05 02:41:13,692 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:TaskManager@254] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1] Handle new assignment with:
	New active tasks: [0_0]
	New standby tasks: []
	Existing active tasks: []
	Existing standby tasks: []
2022-07-05 02:41:13,707 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:ConsumerCoordinator@288] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Adding newly assigned partitions: inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0
2022-07-05 02:41:13,707 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:StreamThread@230] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1] State transition from STARTING to PARTITIONS_ASSIGNED
2022-07-05 02:41:13,716 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:ConsumerCoordinator$OffsetFetchResponseHandler@1352] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Found no committed offset for partition inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0
2022-07-05 02:41:13,728 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:SubscriptionState@398] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Resetting offset for partition inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:46125 (id: 0 rack: null)], epoch=0}}.
2022-07-05 02:41:13,910 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:RocksDBTimestampedStore@100] - Opening store test-store in regular mode
2022-07-05 02:41:13,914 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:ProcessorStateManager@256] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1] task [0_0] State store test-store did not find checkpoint offset, hence would default to the starting offset at changelog appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0
2022-07-05 02:41:13,914 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:StreamTask@235] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1] task [0_0] Initialized
2022-07-05 02:41:13,920 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:KafkaConsumer@1123] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1-restore-consumer, groupId=null] Subscribed to partition(s): appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0
2022-07-05 02:41:13,921 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:SubscriptionState@619] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1-restore-consumer, groupId=null] Seeking to EARLIEST offset of partition appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0
2022-07-05 02:41:13,924 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:Metadata@279] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1-restore-consumer, groupId=null] Cluster ID: D5RV4S7cRSS7tsr75uDRDw
2022-07-05 02:41:13,927 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:SubscriptionState@398] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1-restore-consumer, groupId=null] Resetting offset for partition appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:46125 (id: 0 rack: null)], epoch=0}}.
2022-07-05 02:41:14,028 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:StoreChangelogReader@609] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1] Finished restoring changelog appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0 to store test-store with a total number of 0 records
2022-07-05 02:41:14,030 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:ConsumerCoordinator$OffsetFetchResponseHandler@1352] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Found no committed offset for partition inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0
2022-07-05 02:41:14,034 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:StreamTask@255] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1] task [0_0] Restored and ready to run
2022-07-05 02:41:14,034 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:StreamThread@853] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1] Restoration took 327 ms for all tasks [0_0]
2022-07-05 02:41:14,034 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:StreamThread@230] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1] State transition from PARTITIONS_ASSIGNED to RUNNING
2022-07-05 02:41:14,035 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:KafkaStreams@321] - stream-client [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953] State transition from REBALANCING to RUNNING
2022-07-05 02:41:14,037 - INFO  [main:AbstractConfig@372] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:46125]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.IntegerSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2022-07-05 02:41:14,040 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2022-07-05 02:41:14,040 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2022-07-05 02:41:14,040 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1657003274040
2022-07-05 02:41:14,043 - INFO  [kafka-producer-network-thread | producer-1:Metadata@279] - [Producer clientId=producer-1] Cluster ID: D5RV4S7cRSS7tsr75uDRDw
2022-07-05 02:41:14,046 - INFO  [main:KafkaProducer@1204] - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2022-07-05 02:41:14,061 - INFO  [main:Metrics@659] - Metrics scheduler closed
2022-07-05 02:41:14,061 - INFO  [main:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:41:14,061 - INFO  [main:Metrics@669] - Metrics reporters closed
2022-07-05 02:41:14,062 - INFO  [main:AppInfoParser@83] - App info kafka.producer for producer-1 unregistered
2022-07-05 02:41:14,063 - INFO  [main:AbstractConfig@372] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:46125]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-5d39be77-7bbe-4393-989d-756972712789-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 5d39be77-7bbe-4393-989d-756972712789
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.IntegerDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2022-07-05 02:41:14,065 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2022-07-05 02:41:14,065 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2022-07-05 02:41:14,065 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1657003274065
2022-07-05 02:41:14,067 - INFO  [main:KafkaConsumer@968] - [Consumer clientId=consumer-5d39be77-7bbe-4393-989d-756972712789-1, groupId=5d39be77-7bbe-4393-989d-756972712789] Subscribed to topic(s): outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover
2022-07-05 02:41:14,070 - INFO  [main:Metadata@279] - [Consumer clientId=consumer-5d39be77-7bbe-4393-989d-756972712789-1, groupId=5d39be77-7bbe-4393-989d-756972712789] Cluster ID: D5RV4S7cRSS7tsr75uDRDw
2022-07-05 02:41:14,070 - INFO  [main:AbstractCoordinator$FindCoordinatorResponseHandler@844] - [Consumer clientId=consumer-5d39be77-7bbe-4393-989d-756972712789-1, groupId=5d39be77-7bbe-4393-989d-756972712789] Discovered group coordinator localhost:46125 (id: 2147483647 rack: null)
2022-07-05 02:41:14,071 - INFO  [main:AbstractCoordinator@534] - [Consumer clientId=consumer-5d39be77-7bbe-4393-989d-756972712789-1, groupId=5d39be77-7bbe-4393-989d-756972712789] (Re-)joining group
2022-07-05 02:41:14,073 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupCoordinator 0]: Dynamic member with unknown member id joins group 5d39be77-7bbe-4393-989d-756972712789 in Empty state. Created a new member id consumer-5d39be77-7bbe-4393-989d-756972712789-1-f0d378bf-ffe8-42f1-b334-a8852f026af1 and request the member to rejoin with this id.
2022-07-05 02:41:14,073 - INFO  [main:AbstractCoordinator@982] - [Consumer clientId=consumer-5d39be77-7bbe-4393-989d-756972712789-1, groupId=5d39be77-7bbe-4393-989d-756972712789] Request joining group due to: need to re-join with the given member-id
2022-07-05 02:41:14,073 - INFO  [main:AbstractCoordinator@534] - [Consumer clientId=consumer-5d39be77-7bbe-4393-989d-756972712789-1, groupId=5d39be77-7bbe-4393-989d-756972712789] (Re-)joining group
2022-07-05 02:41:14,074 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group 5d39be77-7bbe-4393-989d-756972712789 in state PreparingRebalance with old generation 0 (__consumer_offsets-2) (reason: Adding new member consumer-5d39be77-7bbe-4393-989d-756972712789-1-f0d378bf-ffe8-42f1-b334-a8852f026af1 with group instance id None)
2022-07-05 02:41:14,075 - INFO  [executor-Rebalance:Logging@66] - [GroupCoordinator 0]: Stabilized group 5d39be77-7bbe-4393-989d-756972712789 generation 1 (__consumer_offsets-2) with 1 members
2022-07-05 02:41:14,075 - INFO  [main:AbstractCoordinator$JoinGroupResponseHandler@590] - [Consumer clientId=consumer-5d39be77-7bbe-4393-989d-756972712789-1, groupId=5d39be77-7bbe-4393-989d-756972712789] Successfully joined group with generation Generation{generationId=1, memberId='consumer-5d39be77-7bbe-4393-989d-756972712789-1-f0d378bf-ffe8-42f1-b334-a8852f026af1', protocol='range'}
2022-07-05 02:41:14,076 - INFO  [main:ConsumerCoordinator@626] - [Consumer clientId=consumer-5d39be77-7bbe-4393-989d-756972712789-1, groupId=5d39be77-7bbe-4393-989d-756972712789] Finished assignment for group at generation 1: {consumer-5d39be77-7bbe-4393-989d-756972712789-1-f0d378bf-ffe8-42f1-b334-a8852f026af1=Assignment(partitions=[outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0])}
2022-07-05 02:41:14,077 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group 5d39be77-7bbe-4393-989d-756972712789 for generation 1. The group has 1 members, 0 of which are static.
2022-07-05 02:41:14,079 - INFO  [main:AbstractCoordinator$SyncGroupResponseHandler@756] - [Consumer clientId=consumer-5d39be77-7bbe-4393-989d-756972712789-1, groupId=5d39be77-7bbe-4393-989d-756972712789] Successfully synced group in generation Generation{generationId=1, memberId='consumer-5d39be77-7bbe-4393-989d-756972712789-1-f0d378bf-ffe8-42f1-b334-a8852f026af1', protocol='range'}
2022-07-05 02:41:14,079 - INFO  [main:ConsumerCoordinator@276] - [Consumer clientId=consumer-5d39be77-7bbe-4393-989d-756972712789-1, groupId=5d39be77-7bbe-4393-989d-756972712789] Notifying assignor about the new Assignment(partitions=[outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0])
2022-07-05 02:41:14,079 - INFO  [main:ConsumerCoordinator@288] - [Consumer clientId=consumer-5d39be77-7bbe-4393-989d-756972712789-1, groupId=5d39be77-7bbe-4393-989d-756972712789] Adding newly assigned partitions: outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0
2022-07-05 02:41:14,080 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1352] - [Consumer clientId=consumer-5d39be77-7bbe-4393-989d-756972712789-1, groupId=5d39be77-7bbe-4393-989d-756972712789] Found no committed offset for partition outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0
2022-07-05 02:41:14,084 - INFO  [main:SubscriptionState@398] - [Consumer clientId=consumer-5d39be77-7bbe-4393-989d-756972712789-1, groupId=5d39be77-7bbe-4393-989d-756972712789] Resetting offset for partition outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:46125 (id: 0 rack: null)], epoch=0}}.
2022-07-05 02:41:14,088 - ERROR [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:TaskManager@1153] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1] Failed to process stream task 0_0 due to the following error:
org.apache.kafka.streams.errors.StreamsException: Exception caught in process. taskId=0_0, processor=KSTREAM-SOURCE-0000000000, topic=inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover, partition=0, offset=0, stacktrace=java.lang.IllegalStateException: Kaboom
	at org.apache.kafka.streams.integration.EmitOnChangeIntegrationTest.lambda$shouldEmitSameRecordAfterFailover$0(EmitOnChangeIntegrationTest.java:94)
	at org.apache.kafka.streams.kstream.internals.KStreamMap$KStreamMapProcessor.process(KStreamMap.java:41)
	at org.apache.kafka.streams.processor.internals.ProcessorAdapter.process(ProcessorAdapter.java:71)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.lambda$process$2(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:884)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forwardInternal(ProcessorContextImpl.java:281)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:260)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:219)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:172)
	at org.apache.kafka.streams.kstream.internals.KStreamMapValues$KStreamMapProcessor.process(KStreamMapValues.java:41)
	at org.apache.kafka.streams.processor.internals.ProcessorAdapter.process(ProcessorAdapter.java:71)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.lambda$process$2(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:884)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forwardInternal(ProcessorContextImpl.java:281)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:260)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:219)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:172)
	at org.apache.kafka.streams.kstream.internals.TimestampedTupleForwarder.maybeForward(TimestampedTupleForwarder.java:51)
	at org.apache.kafka.streams.kstream.internals.KTableSource$KTableSourceProcessor.process(KTableSource.java:145)
	at org.apache.kafka.streams.processor.internals.ProcessorAdapter.process(ProcessorAdapter.java:71)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.lambda$process$2(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:884)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forwardInternal(ProcessorContextImpl.java:281)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:260)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:219)
	at org.apache.kafka.streams.processor.internals.SourceNode.process(SourceNode.java:86)
	at org.apache.kafka.streams.processor.internals.StreamTask.lambda$process$1(StreamTask.java:706)
	at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:884)
	at org.apache.kafka.streams.processor.internals.StreamTask.process(StreamTask.java:706)
	at org.apache.kafka.streams.processor.internals.TaskManager.process(TaskManager.java:1136)
	at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:755)
	at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:585)
	at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:557)

	at org.apache.kafka.streams.processor.internals.StreamTask.process(StreamTask.java:733)
	at org.apache.kafka.streams.processor.internals.TaskManager.process(TaskManager.java:1136)
	at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:755)
	at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:585)
	at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:557)
Caused by: java.lang.IllegalStateException: Kaboom
	at org.apache.kafka.streams.integration.EmitOnChangeIntegrationTest.lambda$shouldEmitSameRecordAfterFailover$0(EmitOnChangeIntegrationTest.java:94)
	at org.apache.kafka.streams.kstream.internals.KStreamMap$KStreamMapProcessor.process(KStreamMap.java:41)
	at org.apache.kafka.streams.processor.internals.ProcessorAdapter.process(ProcessorAdapter.java:71)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.lambda$process$2(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:884)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forwardInternal(ProcessorContextImpl.java:281)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:260)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:219)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:172)
	at org.apache.kafka.streams.kstream.internals.KStreamMapValues$KStreamMapProcessor.process(KStreamMapValues.java:41)
	at org.apache.kafka.streams.processor.internals.ProcessorAdapter.process(ProcessorAdapter.java:71)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.lambda$process$2(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:884)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forwardInternal(ProcessorContextImpl.java:281)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:260)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:219)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:172)
	at org.apache.kafka.streams.kstream.internals.TimestampedTupleForwarder.maybeForward(TimestampedTupleForwarder.java:51)
	at org.apache.kafka.streams.kstream.internals.KTableSource$KTableSourceProcessor.process(KTableSource.java:145)
	at org.apache.kafka.streams.processor.internals.ProcessorAdapter.process(ProcessorAdapter.java:71)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.lambda$process$2(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:884)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forwardInternal(ProcessorContextImpl.java:281)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:260)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:219)
	at org.apache.kafka.streams.processor.internals.SourceNode.process(SourceNode.java:86)
	at org.apache.kafka.streams.processor.internals.StreamTask.lambda$process$1(StreamTask.java:706)
	at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:884)
	at org.apache.kafka.streams.processor.internals.StreamTask.process(StreamTask.java:706)
	... 4 more
2022-07-05 02:41:14,089 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:StreamThread@1057] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1] Informed to shut down
2022-07-05 02:41:14,089 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:StreamThread@230] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1] State transition from RUNNING to PENDING_SHUTDOWN
2022-07-05 02:41:14,090 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:StreamThread@337] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2] Creating restore consumer client
2022-07-05 02:41:14,090 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:AbstractConfig@372] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [localhost:46125]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2-restore-consumer
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = false
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2022-07-05 02:41:14,093 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2022-07-05 02:41:14,093 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2022-07-05 02:41:14,093 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1657003274093
2022-07-05 02:41:14,093 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:ActiveTaskCreator@96] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2] Creating thread producer client
2022-07-05 02:41:14,094 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:AbstractConfig@372] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:46125]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2-producer
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-07-05 02:41:14,096 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2022-07-05 02:41:14,096 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2022-07-05 02:41:14,096 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1657003274096
2022-07-05 02:41:14,096 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:StreamThread@389] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2] Creating consumer client
2022-07-05 02:41:14,096 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:AbstractConfig@372] - ConsumerConfig values: 
	allow.auto.create.topics = false
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:46125]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2-consumer
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = false
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2022-07-05 02:41:14,098 - INFO  [kafka-producer-network-thread | appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2-producer:Metadata@279] - [Producer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2-producer] Cluster ID: D5RV4S7cRSS7tsr75uDRDw
2022-07-05 02:41:14,099 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:AssignorConfiguration@114] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2-consumer] Cooperative rebalancing enabled now
2022-07-05 02:41:14,100 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2022-07-05 02:41:14,100 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2022-07-05 02:41:14,100 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1657003274100
2022-07-05 02:41:14,101 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:StreamThread@1071] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1] Shutting down
2022-07-05 02:41:14,101 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:StreamThread@550] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2] Starting
2022-07-05 02:41:14,101 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:StreamThread@230] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2] State transition from CREATED to STARTING
2022-07-05 02:41:14,101 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:KafkaConsumer@968] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Subscribed to topic(s): inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover
2022-07-05 02:41:14,102 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:StreamTask@1144] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1] task [0_0] Suspended RUNNING
2022-07-05 02:41:14,102 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:StreamTask@290] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1] task [0_0] Suspended running
2022-07-05 02:41:14,103 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:KafkaConsumer@1077] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions
2022-07-05 02:41:14,104 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:Metadata@279] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Cluster ID: D5RV4S7cRSS7tsr75uDRDw
2022-07-05 02:41:14,105 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:AbstractCoordinator$FindCoordinatorResponseHandler@844] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Discovered group coordinator localhost:46125 (id: 2147483647 rack: null)
2022-07-05 02:41:14,105 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:AbstractCoordinator@534] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] (Re-)joining group
2022-07-05 02:41:14,108 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupCoordinator 0]: Dynamic member with unknown member id joins group appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover in Stable state. Created a new member id appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2-consumer-2eab5007-ae23-473f-812a-9b0f4f1470c9 and request the member to rejoin with this id.
2022-07-05 02:41:14,109 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:AbstractCoordinator@982] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Request joining group due to: need to re-join with the given member-id
2022-07-05 02:41:14,109 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:AbstractCoordinator@534] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] (Re-)joining group
2022-07-05 02:41:14,110 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover in state PreparingRebalance with old generation 1 (__consumer_offsets-0) (reason: Adding new member appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2-consumer-2eab5007-ae23-473f-812a-9b0f4f1470c9 with group instance id None)
2022-07-05 02:41:14,116 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:RecordCollectorImpl@283] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1] task [0_0] Closing record collector dirty
2022-07-05 02:41:14,116 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:StreamTask@515] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1] task [0_0] Closed dirty
2022-07-05 02:41:14,117 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:KafkaProducer@1204] - [Producer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1-producer] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2022-07-05 02:41:14,121 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:Metrics@659] - Metrics scheduler closed
2022-07-05 02:41:14,121 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:41:14,121 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:Metrics@669] - Metrics reporters closed
2022-07-05 02:41:14,121 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:AppInfoParser@83] - App info kafka.producer for appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1-producer unregistered
2022-07-05 02:41:14,122 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:KafkaConsumer@1077] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions
2022-07-05 02:41:14,122 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:Metrics@659] - Metrics scheduler closed
2022-07-05 02:41:14,122 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:41:14,123 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:Metrics@669] - Metrics reporters closed
2022-07-05 02:41:14,124 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:AppInfoParser@83] - App info kafka.consumer for appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1-consumer unregistered
2022-07-05 02:41:14,124 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:Metrics@659] - Metrics scheduler closed
2022-07-05 02:41:14,124 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:41:14,124 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:Metrics@669] - Metrics reporters closed
2022-07-05 02:41:14,125 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:AppInfoParser@83] - App info kafka.consumer for appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1-restore-consumer unregistered
2022-07-05 02:41:14,126 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:StreamThread@230] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1] State transition from PENDING_SHUTDOWN to DEAD
2022-07-05 02:41:14,126 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:StreamThread@1100] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1] Shutdown complete
2022-07-05 02:41:14,126 - ERROR [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1:NIOServerCnxnFactory$1@92] - Thread 	StreamsThread threadId: appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1
TaskManager
	MetadataState:
	Tasks:
 died
org.apache.kafka.streams.errors.StreamsException: Exception caught in process. taskId=0_0, processor=KSTREAM-SOURCE-0000000000, topic=inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover, partition=0, offset=0, stacktrace=java.lang.IllegalStateException: Kaboom
	at org.apache.kafka.streams.integration.EmitOnChangeIntegrationTest.lambda$shouldEmitSameRecordAfterFailover$0(EmitOnChangeIntegrationTest.java:94)
	at org.apache.kafka.streams.kstream.internals.KStreamMap$KStreamMapProcessor.process(KStreamMap.java:41)
	at org.apache.kafka.streams.processor.internals.ProcessorAdapter.process(ProcessorAdapter.java:71)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.lambda$process$2(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:884)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forwardInternal(ProcessorContextImpl.java:281)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:260)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:219)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:172)
	at org.apache.kafka.streams.kstream.internals.KStreamMapValues$KStreamMapProcessor.process(KStreamMapValues.java:41)
	at org.apache.kafka.streams.processor.internals.ProcessorAdapter.process(ProcessorAdapter.java:71)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.lambda$process$2(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:884)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forwardInternal(ProcessorContextImpl.java:281)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:260)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:219)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:172)
	at org.apache.kafka.streams.kstream.internals.TimestampedTupleForwarder.maybeForward(TimestampedTupleForwarder.java:51)
	at org.apache.kafka.streams.kstream.internals.KTableSource$KTableSourceProcessor.process(KTableSource.java:145)
	at org.apache.kafka.streams.processor.internals.ProcessorAdapter.process(ProcessorAdapter.java:71)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.lambda$process$2(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:884)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forwardInternal(ProcessorContextImpl.java:281)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:260)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:219)
	at org.apache.kafka.streams.processor.internals.SourceNode.process(SourceNode.java:86)
	at org.apache.kafka.streams.processor.internals.StreamTask.lambda$process$1(StreamTask.java:706)
	at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:884)
	at org.apache.kafka.streams.processor.internals.StreamTask.process(StreamTask.java:706)
	at org.apache.kafka.streams.processor.internals.TaskManager.process(TaskManager.java:1136)
	at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:755)
	at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:585)
	at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:557)

	at org.apache.kafka.streams.processor.internals.StreamTask.process(StreamTask.java:733)
	at org.apache.kafka.streams.processor.internals.TaskManager.process(TaskManager.java:1136)
	at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:755)
	at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:585)
	at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:557)
Caused by: java.lang.IllegalStateException: Kaboom
	at org.apache.kafka.streams.integration.EmitOnChangeIntegrationTest.lambda$shouldEmitSameRecordAfterFailover$0(EmitOnChangeIntegrationTest.java:94)
	at org.apache.kafka.streams.kstream.internals.KStreamMap$KStreamMapProcessor.process(KStreamMap.java:41)
	at org.apache.kafka.streams.processor.internals.ProcessorAdapter.process(ProcessorAdapter.java:71)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.lambda$process$2(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:884)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forwardInternal(ProcessorContextImpl.java:281)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:260)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:219)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:172)
	at org.apache.kafka.streams.kstream.internals.KStreamMapValues$KStreamMapProcessor.process(KStreamMapValues.java:41)
	at org.apache.kafka.streams.processor.internals.ProcessorAdapter.process(ProcessorAdapter.java:71)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.lambda$process$2(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:884)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forwardInternal(ProcessorContextImpl.java:281)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:260)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:219)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:172)
	at org.apache.kafka.streams.kstream.internals.TimestampedTupleForwarder.maybeForward(TimestampedTupleForwarder.java:51)
	at org.apache.kafka.streams.kstream.internals.KTableSource$KTableSourceProcessor.process(KTableSource.java:145)
	at org.apache.kafka.streams.processor.internals.ProcessorAdapter.process(ProcessorAdapter.java:71)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.lambda$process$2(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:884)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forwardInternal(ProcessorContextImpl.java:281)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:260)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:219)
	at org.apache.kafka.streams.processor.internals.SourceNode.process(SourceNode.java:86)
	at org.apache.kafka.streams.processor.internals.StreamTask.lambda$process$1(StreamTask.java:706)
	at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:884)
	at org.apache.kafka.streams.processor.internals.StreamTask.process(StreamTask.java:706)
	... 4 more
2022-07-05 02:41:14,201 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:StreamThread@829] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update
2022-07-05 02:41:23,691 - INFO  [executor-Heartbeat:Logging@66] - [GroupCoordinator 0]: Member appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1-consumer-096ae404-e687-46fa-84db-c2ffa87c44cf in group appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover has failed, removing it from the group
2022-07-05 02:41:23,692 - INFO  [executor-Heartbeat:Logging@66] - [GroupCoordinator 0]: Stabilized group appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover generation 2 (__consumer_offsets-0) with 1 members
2022-07-05 02:41:23,693 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:AbstractCoordinator$JoinGroupResponseHandler@590] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Successfully joined group with generation Generation{generationId=2, memberId='appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2-consumer-2eab5007-ae23-473f-812a-9b0f4f1470c9', protocol='stream'}
2022-07-05 02:41:23,698 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:HighAvailabilityTaskAssignor@95] - Decided on assignment: {08a947c2-b47a-40c4-875b-df1d7bccb953=[activeTasks: ([0_0]) standbyTasks: ([]) prevActiveTasks: ([]) prevStandbyTasks: ([]) changelogOffsetTotalsByTask: ([]) taskLagTotals: ([0_0=1]) capacity: 1 assigned: 1]} with no followup probing rebalance.
2022-07-05 02:41:23,699 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:StreamsPartitionAssignor@595] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2-consumer] Assigned tasks [0_0] including stateful [0_0] to clients as: 
08a947c2-b47a-40c4-875b-df1d7bccb953=[activeTasks: ([0_0]) standbyTasks: ([])].
2022-07-05 02:41:23,699 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:StreamsPartitionAssignor@803] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2-consumer] Client 08a947c2-b47a-40c4-875b-df1d7bccb953 per-consumer assignment:
	prev owned active {}
	prev owned standby {appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2-consumer-2eab5007-ae23-473f-812a-9b0f4f1470c9=[]}
	assigned active {appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2-consumer-2eab5007-ae23-473f-812a-9b0f4f1470c9=[0_0]}
	revoking active {}
	assigned standby {}

2022-07-05 02:41:23,699 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:StreamsPartitionAssignor@822] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2-consumer] Finished stable assignment of tasks, no followup rebalances required.
2022-07-05 02:41:23,699 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:ConsumerCoordinator@626] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Finished assignment for group at generation 2: {appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2-consumer-2eab5007-ae23-473f-812a-9b0f4f1470c9=Assignment(partitions=[inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0], userDataSize=48)}
2022-07-05 02:41:23,700 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover for generation 2. The group has 1 members, 0 of which are static.
2022-07-05 02:41:23,702 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:AbstractCoordinator$SyncGroupResponseHandler@756] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Successfully synced group in generation Generation{generationId=2, memberId='appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2-consumer-2eab5007-ae23-473f-812a-9b0f4f1470c9', protocol='stream'}
2022-07-05 02:41:23,702 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:ConsumerCoordinator@392] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Updating assignment with
	Assigned partitions:                       [inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0]
	Current owned partitions:                  []
	Added partitions (assigned - owned):       [inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0]
	Revoked partitions (owned - assigned):     []

2022-07-05 02:41:23,702 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:ConsumerCoordinator@276] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Notifying assignor about the new Assignment(partitions=[inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0], userDataSize=48)
2022-07-05 02:41:23,702 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:StreamsPartitionAssignor@1306] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2-consumer] No followup rebalance was requested, resetting the rebalance schedule.
2022-07-05 02:41:23,702 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:TaskManager@254] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2] Handle new assignment with:
	New active tasks: [0_0]
	New standby tasks: []
	Existing active tasks: []
	Existing standby tasks: []
2022-07-05 02:41:23,703 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:ConsumerCoordinator@288] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Adding newly assigned partitions: inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0
2022-07-05 02:41:23,703 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:StreamThread@230] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2] State transition from STARTING to PARTITIONS_ASSIGNED
2022-07-05 02:41:23,703 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:KafkaStreams@321] - stream-client [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953] State transition from RUNNING to REBALANCING
2022-07-05 02:41:23,704 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:ConsumerCoordinator$OffsetFetchResponseHandler@1352] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Found no committed offset for partition inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0
2022-07-05 02:41:23,707 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:SubscriptionState@398] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Resetting offset for partition inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:46125 (id: 0 rack: null)], epoch=0}}.
2022-07-05 02:41:23,763 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:RocksDBTimestampedStore@100] - Opening store test-store in regular mode
2022-07-05 02:41:23,764 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:ProcessorStateManager@256] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2] task [0_0] State store test-store did not find checkpoint offset, hence would default to the starting offset at changelog appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0
2022-07-05 02:41:23,764 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:StreamTask@235] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2] task [0_0] Initialized
2022-07-05 02:41:23,766 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:KafkaConsumer@1123] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2-restore-consumer, groupId=null] Subscribed to partition(s): appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0
2022-07-05 02:41:23,766 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:SubscriptionState@619] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2-restore-consumer, groupId=null] Seeking to EARLIEST offset of partition appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0
2022-07-05 02:41:23,769 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:Metadata@279] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2-restore-consumer, groupId=null] Cluster ID: D5RV4S7cRSS7tsr75uDRDw
2022-07-05 02:41:23,772 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:SubscriptionState@398] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2-restore-consumer, groupId=null] Resetting offset for partition appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:46125 (id: 0 rack: null)], epoch=0}}.
2022-07-05 02:41:23,779 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:StoreChangelogReader@609] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2] Finished restoring changelog appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0 to store test-store with a total number of 1 records
2022-07-05 02:41:23,780 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:ConsumerCoordinator$OffsetFetchResponseHandler@1352] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Found no committed offset for partition inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0
2022-07-05 02:41:23,780 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:StreamTask@255] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2] task [0_0] Restored and ready to run
2022-07-05 02:41:23,780 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:StreamThread@853] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2] Restoration took 77 ms for all tasks [0_0]
2022-07-05 02:41:23,780 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:StreamThread@230] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2] State transition from PARTITIONS_ASSIGNED to RUNNING
2022-07-05 02:41:23,780 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:KafkaStreams@321] - stream-client [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953] State transition from REBALANCING to RUNNING
2022-07-05 02:41:42,163 - INFO  [kafka-scheduler-9:Logging@66] - [ProducerStateManager partition=outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0] Writing producer snapshot at offset 1
2022-07-05 02:41:42,166 - INFO  [kafka-scheduler-9:Logging@66] - [Log partition=outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0, dir=/tmp/junit5462504141538786114/junit6112278271730470661] Rolled new log segment at offset 1 in 0 ms.
2022-07-05 02:41:42,167 - INFO  [kafka-scheduler-9:Logging@66] - [Log partition=outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0, dir=/tmp/junit5462504141538786114/junit6112278271730470661] Deleting segment LogSegment(baseOffset=0, size=73, lastModifiedTime=1657003283000, largestRecordTimestamp=Some(0)) due to retention time 604800000ms breach based on the largest record timestamp in the segment
2022-07-05 02:41:42,169 - INFO  [kafka-scheduler-9:Logging@66] - [Log partition=outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0, dir=/tmp/junit5462504141538786114/junit6112278271730470661] Incremented log start offset to 1 due to segment deletion
2022-07-05 02:41:42,175 - INFO  [kafka-scheduler-9:Logging@66] - [ProducerStateManager partition=inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0] Writing producer snapshot at offset 2
2022-07-05 02:41:42,182 - INFO  [kafka-scheduler-9:Logging@66] - [Log partition=inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0, dir=/tmp/junit5462504141538786114/junit6112278271730470661] Rolled new log segment at offset 2 in 0 ms.
2022-07-05 02:41:42,183 - INFO  [kafka-scheduler-9:Logging@66] - [Log partition=inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0, dir=/tmp/junit5462504141538786114/junit6112278271730470661] Deleting segment LogSegment(baseOffset=0, size=85, lastModifiedTime=1657003274000, largestRecordTimestamp=Some(0)) due to retention time 604800000ms breach based on the largest record timestamp in the segment
2022-07-05 02:41:42,183 - INFO  [kafka-scheduler-9:Logging@66] - [Log partition=inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0, dir=/tmp/junit5462504141538786114/junit6112278271730470661] Incremented log start offset to 2 due to segment deletion
2022-07-05 02:42:14,318 - INFO  [main:ConsumerCoordinator@307] - [Consumer clientId=consumer-5d39be77-7bbe-4393-989d-756972712789-1, groupId=5d39be77-7bbe-4393-989d-756972712789] Revoke previously assigned partitions outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0
2022-07-05 02:42:14,318 - INFO  [main:AbstractCoordinator@1038] - [Consumer clientId=consumer-5d39be77-7bbe-4393-989d-756972712789-1, groupId=5d39be77-7bbe-4393-989d-756972712789] Member consumer-5d39be77-7bbe-4393-989d-756972712789-1-f0d378bf-ffe8-42f1-b334-a8852f026af1 sending LeaveGroup request to coordinator localhost:46125 (id: 2147483647 rack: null) due to the consumer is being closed
2022-07-05 02:42:14,319 - INFO  [main:AbstractCoordinator@961] - [Consumer clientId=consumer-5d39be77-7bbe-4393-989d-756972712789-1, groupId=5d39be77-7bbe-4393-989d-756972712789] Resetting generation due to: consumer pro-actively leaving the group
2022-07-05 02:42:14,319 - INFO  [main:AbstractCoordinator@982] - [Consumer clientId=consumer-5d39be77-7bbe-4393-989d-756972712789-1, groupId=5d39be77-7bbe-4393-989d-756972712789] Request joining group due to: consumer pro-actively leaving the group
2022-07-05 02:42:14,321 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group 5d39be77-7bbe-4393-989d-756972712789 in state PreparingRebalance with old generation 1 (__consumer_offsets-2) (reason: removing member consumer-5d39be77-7bbe-4393-989d-756972712789-1-f0d378bf-ffe8-42f1-b334-a8852f026af1 on LeaveGroup)
2022-07-05 02:42:14,321 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [GroupCoordinator 0]: Group 5d39be77-7bbe-4393-989d-756972712789 with generation 2 is now empty (__consumer_offsets-2)
2022-07-05 02:42:14,323 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [GroupCoordinator 0]: Member MemberMetadata(memberId=consumer-5d39be77-7bbe-4393-989d-756972712789-1-f0d378bf-ffe8-42f1-b334-a8852f026af1, groupInstanceId=None, clientId=consumer-5d39be77-7bbe-4393-989d-756972712789-1, clientHost=/127.0.0.1, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(range)) has left group 5d39be77-7bbe-4393-989d-756972712789 through explicit `LeaveGroup` request
2022-07-05 02:42:14,325 - INFO  [main:Metrics@659] - Metrics scheduler closed
2022-07-05 02:42:14,325 - INFO  [main:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:42:14,325 - INFO  [main:Metrics@669] - Metrics reporters closed
2022-07-05 02:42:14,326 - INFO  [main:AppInfoParser@83] - App info kafka.consumer for consumer-5d39be77-7bbe-4393-989d-756972712789-1 unregistered
2022-07-05 02:42:14,326 - INFO  [main:KafkaStreams@321] - stream-client [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953] State transition from RUNNING to PENDING_SHUTDOWN
2022-07-05 02:42:14,327 - INFO  [kafka-streams-close-thread:StreamThread@1057] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-1] Informed to shut down
2022-07-05 02:42:14,327 - INFO  [kafka-streams-close-thread:StreamThread@1057] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2] Informed to shut down
2022-07-05 02:42:14,327 - INFO  [kafka-streams-close-thread:StreamThread@230] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2] State transition from RUNNING to PENDING_SHUTDOWN
2022-07-05 02:42:14,412 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:StreamThread@729] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2] Thread state is already PENDING_SHUTDOWN, skipping the run once call after poll request
2022-07-05 02:42:14,412 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:StreamThread@1071] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2] Shutting down
2022-07-05 02:42:14,434 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:StreamTask@1144] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2] task [0_0] Suspended RUNNING
2022-07-05 02:42:14,434 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:StreamTask@290] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2] task [0_0] Suspended running
2022-07-05 02:42:14,435 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:KafkaConsumer@1077] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions
2022-07-05 02:42:14,437 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:RecordCollectorImpl@268] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2] task [0_0] Closing record collector clean
2022-07-05 02:42:14,437 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:StreamTask@508] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2] task [0_0] Closed clean
2022-07-05 02:42:14,437 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:KafkaProducer@1204] - [Producer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2-producer] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2022-07-05 02:42:14,438 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:Metrics@659] - Metrics scheduler closed
2022-07-05 02:42:14,438 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:42:14,438 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:Metrics@669] - Metrics reporters closed
2022-07-05 02:42:14,439 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:AppInfoParser@83] - App info kafka.producer for appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2-producer unregistered
2022-07-05 02:42:14,439 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:KafkaConsumer@1077] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions
2022-07-05 02:42:14,439 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:Metrics@659] - Metrics scheduler closed
2022-07-05 02:42:14,439 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:42:14,439 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:Metrics@669] - Metrics reporters closed
2022-07-05 02:42:14,440 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:AppInfoParser@83] - App info kafka.consumer for appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2-consumer unregistered
2022-07-05 02:42:14,440 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:Metrics@659] - Metrics scheduler closed
2022-07-05 02:42:14,441 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:42:14,441 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:Metrics@669] - Metrics reporters closed
2022-07-05 02:42:14,442 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:AppInfoParser@83] - App info kafka.consumer for appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2-restore-consumer unregistered
2022-07-05 02:42:14,442 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:StreamThread@230] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2] State transition from PENDING_SHUTDOWN to DEAD
2022-07-05 02:42:14,442 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2:StreamThread@1100] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-StreamThread-2] Shutdown complete
2022-07-05 02:42:14,442 - ERROR [kafka-streams-close-thread:StateDirectory@414] - Some task directories still locked while closing state, this indicates unclean shutdown: {}
2022-07-05 02:42:14,443 - INFO  [kafka-admin-client-thread | appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-admin:AppInfoParser@83] - App info kafka.admin.client for appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-admin unregistered
2022-07-05 02:42:14,444 - INFO  [kafka-admin-client-thread | appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-admin:Metrics@659] - Metrics scheduler closed
2022-07-05 02:42:14,444 - INFO  [kafka-admin-client-thread | appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-admin:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:42:14,444 - INFO  [kafka-admin-client-thread | appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953-admin:Metrics@669] - Metrics reporters closed
2022-07-05 02:42:14,444 - INFO  [kafka-streams-close-thread:Metrics@659] - Metrics scheduler closed
2022-07-05 02:42:14,444 - INFO  [kafka-streams-close-thread:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:42:14,444 - INFO  [kafka-streams-close-thread:Metrics@669] - Metrics reporters closed
2022-07-05 02:42:14,444 - INFO  [kafka-streams-close-thread:KafkaStreams@321] - stream-client [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953] State transition from PENDING_SHUTDOWN to NOT_RUNNING
2022-07-05 02:42:14,444 - INFO  [main:KafkaStreams@1367] - stream-client [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-08a947c2-b47a-40c4-875b-df1d7bccb953] Streams client stopped completely
2022-07-05 02:42:14,449 - INFO  [main:Logging@66] - [KafkaServer id=0] shutting down
2022-07-05 02:42:14,449 - INFO  [main:Logging@66] - [KafkaServer id=0] Starting controlled shutdown
2022-07-05 02:42:14,459 - INFO  [main:Logging@66] - [KafkaServer id=0] Controlled shutdown succeeded
2022-07-05 02:42:14,461 - INFO  [main:Logging@66] - [/config/changes-event-process-thread]: Shutting down
2022-07-05 02:42:14,461 - INFO  [main:Logging@66] - [/config/changes-event-process-thread]: Shutdown completed
2022-07-05 02:42:14,461 - INFO  [/config/changes-event-process-thread:Logging@66] - [/config/changes-event-process-thread]: Stopped
2022-07-05 02:42:14,462 - INFO  [main:Logging@66] - [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopping socket server request processors
2022-07-05 02:42:14,465 - INFO  [main:Logging@66] - [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopped socket server request processors
2022-07-05 02:42:14,466 - INFO  [main:Logging@66] - [data-plane Kafka Request Handler on Broker 0], shutting down
2022-07-05 02:42:14,467 - INFO  [main:Logging@66] - [data-plane Kafka Request Handler on Broker 0], shut down completely
2022-07-05 02:42:14,468 - INFO  [main:Logging@66] - [ExpirationReaper-0-AlterAcls]: Shutting down
2022-07-05 02:42:14,501 - INFO  [ExpirationReaper-0-AlterAcls:Logging@66] - [ExpirationReaper-0-AlterAcls]: Stopped
2022-07-05 02:42:14,501 - INFO  [main:Logging@66] - [ExpirationReaper-0-AlterAcls]: Shutdown completed
2022-07-05 02:42:14,502 - INFO  [main:Logging@66] - [KafkaApi-0] Shutdown complete.
2022-07-05 02:42:14,502 - INFO  [main:Logging@66] - [ExpirationReaper-0-topic]: Shutting down
2022-07-05 02:42:14,697 - INFO  [ExpirationReaper-0-topic:Logging@66] - [ExpirationReaper-0-topic]: Stopped
2022-07-05 02:42:14,697 - INFO  [main:Logging@66] - [ExpirationReaper-0-topic]: Shutdown completed
2022-07-05 02:42:14,700 - INFO  [main:Logging@66] - [TransactionCoordinator id=0] Shutting down.
2022-07-05 02:42:14,700 - INFO  [main:Logging@66] - [ProducerId Manager 0]: Shutdown complete: last producerId assigned 0
2022-07-05 02:42:14,701 - INFO  [main:Logging@66] - [Transaction State Manager 0]: Shutdown complete
2022-07-05 02:42:14,701 - INFO  [main:Logging@66] - [Transaction Marker Channel Manager 0]: Shutting down
2022-07-05 02:42:14,701 - INFO  [TxnMarkerSenderThread-0:Logging@66] - [Transaction Marker Channel Manager 0]: Stopped
2022-07-05 02:42:14,701 - INFO  [main:Logging@66] - [Transaction Marker Channel Manager 0]: Shutdown completed
2022-07-05 02:42:14,702 - INFO  [main:Logging@66] - [TransactionCoordinator id=0] Shutdown complete.
2022-07-05 02:42:14,702 - INFO  [main:Logging@66] - [GroupCoordinator 0]: Shutting down.
2022-07-05 02:42:14,703 - INFO  [main:Logging@66] - [ExpirationReaper-0-Heartbeat]: Shutting down
2022-07-05 02:42:14,828 - INFO  [ExpirationReaper-0-Heartbeat:Logging@66] - [ExpirationReaper-0-Heartbeat]: Stopped
2022-07-05 02:42:14,828 - INFO  [main:Logging@66] - [ExpirationReaper-0-Heartbeat]: Shutdown completed
2022-07-05 02:42:14,828 - INFO  [main:Logging@66] - [ExpirationReaper-0-Rebalance]: Shutting down
2022-07-05 02:42:14,917 - INFO  [ExpirationReaper-0-Rebalance:Logging@66] - [ExpirationReaper-0-Rebalance]: Stopped
2022-07-05 02:42:14,917 - INFO  [main:Logging@66] - [ExpirationReaper-0-Rebalance]: Shutdown completed
2022-07-05 02:42:14,918 - INFO  [main:Logging@66] - [GroupCoordinator 0]: Shutdown complete.
2022-07-05 02:42:14,918 - INFO  [main:Logging@66] - [ReplicaManager broker=0] Shutting down
2022-07-05 02:42:14,918 - INFO  [main:Logging@66] - [LogDirFailureHandler]: Shutting down
2022-07-05 02:42:14,919 - INFO  [main:Logging@66] - [LogDirFailureHandler]: Shutdown completed
2022-07-05 02:42:14,919 - INFO  [LogDirFailureHandler:Logging@66] - [LogDirFailureHandler]: Stopped
2022-07-05 02:42:14,919 - INFO  [main:Logging@66] - [ReplicaFetcherManager on broker 0] shutting down
2022-07-05 02:42:14,920 - INFO  [main:Logging@66] - [ReplicaFetcherManager on broker 0] shutdown completed
2022-07-05 02:42:14,920 - INFO  [main:Logging@66] - [ReplicaAlterLogDirsManager on broker 0] shutting down
2022-07-05 02:42:14,921 - INFO  [main:Logging@66] - [ReplicaAlterLogDirsManager on broker 0] shutdown completed
2022-07-05 02:42:14,921 - INFO  [main:Logging@66] - [ExpirationReaper-0-Fetch]: Shutting down
2022-07-05 02:42:15,110 - INFO  [ExpirationReaper-0-Fetch:Logging@66] - [ExpirationReaper-0-Fetch]: Stopped
2022-07-05 02:42:15,110 - INFO  [main:Logging@66] - [ExpirationReaper-0-Fetch]: Shutdown completed
2022-07-05 02:42:15,110 - INFO  [main:Logging@66] - [ExpirationReaper-0-Produce]: Shutting down
2022-07-05 02:42:15,298 - INFO  [ExpirationReaper-0-Produce:Logging@66] - [ExpirationReaper-0-Produce]: Stopped
2022-07-05 02:42:15,298 - INFO  [main:Logging@66] - [ExpirationReaper-0-Produce]: Shutdown completed
2022-07-05 02:42:15,298 - INFO  [main:Logging@66] - [ExpirationReaper-0-DeleteRecords]: Shutting down
2022-07-05 02:42:15,498 - INFO  [ExpirationReaper-0-DeleteRecords:Logging@66] - [ExpirationReaper-0-DeleteRecords]: Stopped
2022-07-05 02:42:15,498 - INFO  [main:Logging@66] - [ExpirationReaper-0-DeleteRecords]: Shutdown completed
2022-07-05 02:42:15,498 - INFO  [main:Logging@66] - [ExpirationReaper-0-ElectLeader]: Shutting down
2022-07-05 02:42:15,698 - INFO  [ExpirationReaper-0-ElectLeader:Logging@66] - [ExpirationReaper-0-ElectLeader]: Stopped
2022-07-05 02:42:15,698 - INFO  [main:Logging@66] - [ExpirationReaper-0-ElectLeader]: Shutdown completed
2022-07-05 02:42:15,700 - INFO  [main:Logging@66] - [ReplicaManager broker=0] Shut down completely
2022-07-05 02:42:15,700 - INFO  [main:Logging@66] - [BrokerToControllerChannelManager broker=0 name=alterIsr]: Shutting down
2022-07-05 02:42:15,700 - INFO  [main:Logging@66] - [BrokerToControllerChannelManager broker=0 name=alterIsr]: Shutdown completed
2022-07-05 02:42:15,700 - INFO  [BrokerToControllerChannelManager broker=0 name=alterIsr:Logging@66] - [BrokerToControllerChannelManager broker=0 name=alterIsr]: Stopped
2022-07-05 02:42:15,701 - INFO  [main:Logging@66] - Broker to controller channel manager for alterIsr shutdown
2022-07-05 02:42:15,702 - INFO  [main:Logging@66] - Shutting down.
2022-07-05 02:42:15,713 - INFO  [pool-8-thread-1:Logging@66] - [ProducerStateManager partition=__consumer_offsets-0] Writing producer snapshot at offset 3
2022-07-05 02:42:15,723 - INFO  [pool-8-thread-1:Logging@66] - [ProducerStateManager partition=__consumer_offsets-2] Writing producer snapshot at offset 15
2022-07-05 02:42:15,724 - INFO  [pool-8-thread-1:Logging@66] - [ProducerStateManager partition=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0] Writing producer snapshot at offset 2
2022-07-05 02:42:15,750 - INFO  [main:Logging@66] - Shutdown complete.
2022-07-05 02:42:15,756 - INFO  [main:Logging@66] - [feature-zk-node-event-process-thread]: Shutting down
2022-07-05 02:42:15,756 - INFO  [feature-zk-node-event-process-thread:Logging@66] - [feature-zk-node-event-process-thread]: Stopped
2022-07-05 02:42:15,756 - INFO  [main:Logging@66] - [feature-zk-node-event-process-thread]: Shutdown completed
2022-07-05 02:42:15,757 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Closing.
2022-07-05 02:42:15,860 - INFO  [main:ZooKeeper@1422] - Session: 0x100a45d6df60000 closed
2022-07-05 02:42:15,860 - INFO  [main-EventThread:ClientCnxn$EventThread@524] - EventThread shut down for session: 0x100a45d6df60000
2022-07-05 02:42:15,860 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Closed.
2022-07-05 02:42:15,861 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Fetch]: Shutting down
2022-07-05 02:42:16,121 - INFO  [ThrottledChannelReaper-Fetch:Logging@66] - [ThrottledChannelReaper-Fetch]: Stopped
2022-07-05 02:42:16,121 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Fetch]: Shutdown completed
2022-07-05 02:42:16,121 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Produce]: Shutting down
2022-07-05 02:42:17,121 - INFO  [ThrottledChannelReaper-Produce:Logging@66] - [ThrottledChannelReaper-Produce]: Stopped
2022-07-05 02:42:17,121 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Produce]: Shutdown completed
2022-07-05 02:42:17,121 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Request]: Shutting down
2022-07-05 02:42:17,122 - INFO  [ThrottledChannelReaper-Request:Logging@66] - [ThrottledChannelReaper-Request]: Stopped
2022-07-05 02:42:17,122 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Request]: Shutdown completed
2022-07-05 02:42:17,122 - INFO  [main:Logging@66] - [ThrottledChannelReaper-ControllerMutation]: Shutting down
2022-07-05 02:42:17,123 - INFO  [ThrottledChannelReaper-ControllerMutation:Logging@66] - [ThrottledChannelReaper-ControllerMutation]: Stopped
2022-07-05 02:42:17,124 - INFO  [main:Logging@66] - [ThrottledChannelReaper-ControllerMutation]: Shutdown completed
2022-07-05 02:42:17,125 - INFO  [main:Logging@66] - [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutting down socket server
2022-07-05 02:42:17,145 - INFO  [main:Logging@66] - [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutdown completed
2022-07-05 02:42:17,146 - INFO  [main:Metrics@659] - Metrics scheduler closed
2022-07-05 02:42:17,146 - INFO  [main:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:42:17,146 - INFO  [main:Metrics@669] - Metrics reporters closed
2022-07-05 02:42:17,149 - INFO  [main:Logging@66] - Broker and topic stats closed
2022-07-05 02:42:17,149 - INFO  [main:AppInfoParser@83] - App info kafka.server for 0 unregistered
2022-07-05 02:42:17,150 - INFO  [main:Logging@66] - [KafkaServer id=0] shut down completed
2022-07-05 02:42:17,156 - INFO  [ConnnectionExpirer:NIOServerCnxnFactory$ConnectionExpirerThread@583] - ConnnectionExpirerThread interrupted
2022-07-05 02:42:17,156 - INFO  [NIOServerCxnFactory.AcceptThread:/127.0.0.1:0:NIOServerCnxnFactory$AcceptThread@219] - accept thread exitted run method
2022-07-05 02:42:17,156 - INFO  [NIOServerCxnFactory.SelectorThread-1:NIOServerCnxnFactory$SelectorThread@420] - selector thread exitted run method
2022-07-05 02:42:17,156 - INFO  [NIOServerCxnFactory.SelectorThread-2:NIOServerCnxnFactory$SelectorThread@420] - selector thread exitted run method
2022-07-05 02:42:17,157 - INFO  [NIOServerCxnFactory.SelectorThread-0:NIOServerCnxnFactory$SelectorThread@420] - selector thread exitted run method
2022-07-05 02:42:17,157 - INFO  [main:ZooKeeperServer@573] - shutting down
2022-07-05 02:42:17,157 - INFO  [main:SessionTrackerImpl@237] - Shutting down
2022-07-05 02:42:17,157 - INFO  [main:PrepRequestProcessor@1008] - Shutting down
2022-07-05 02:42:17,157 - INFO  [main:SyncRequestProcessor@191] - Shutting down
2022-07-05 02:42:17,157 - INFO  [ProcessThread(sid:0 cport:39357)::PrepRequestProcessor@156] - PrepRequestProcessor exited loop!
2022-07-05 02:42:17,158 - INFO  [SyncThread:0:SyncRequestProcessor@169] - SyncRequestProcessor exited!
2022-07-05 02:42:17,158 - INFO  [main:FinalRequestProcessor@514] - shutdown of request processor complete
2022-07-05 02:42:34,230 - INFO  [main:Log4jControllerRegistration$@31] - Registered kafka:type=kafka.Log4jController MBean
2022-07-05 02:42:34,278 - INFO  [main:Environment@109] - Server environment:zookeeper.version=3.5.9-83df9301aa5c2a5d284a9940177808c01bc35cef, built on 01/06/2021 20:03 GMT
2022-07-05 02:42:34,278 - INFO  [main:Environment@109] - Server environment:host.name=razor15
2022-07-05 02:42:34,279 - INFO  [main:Environment@109] - Server environment:java.version=1.8.0_275
2022-07-05 02:42:34,279 - INFO  [main:Environment@109] - Server environment:java.vendor=Private Build
2022-07-05 02:42:34,279 - INFO  [main:Environment@109] - Server environment:java.home=/usr/lib/jvm/java-8-openjdk-amd64/jre
2022-07-05 02:42:34,279 - INFO  [main:Environment@109] - Server environment:java.class.path=/home/tonypan/flaky-reproduction/experiment/kafka-12508/junit-platform-console-standalone-1.7.0.jar
2022-07-05 02:42:34,279 - INFO  [main:Environment@109] - Server environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib
2022-07-05 02:42:34,279 - INFO  [main:Environment@109] - Server environment:java.io.tmpdir=/tmp
2022-07-05 02:42:34,279 - INFO  [main:Environment@109] - Server environment:java.compiler=<NA>
2022-07-05 02:42:34,279 - INFO  [main:Environment@109] - Server environment:os.name=Linux
2022-07-05 02:42:34,280 - INFO  [main:Environment@109] - Server environment:os.arch=amd64
2022-07-05 02:42:34,280 - INFO  [main:Environment@109] - Server environment:os.version=4.15.0-128-generic
2022-07-05 02:42:34,280 - INFO  [main:Environment@109] - Server environment:user.name=tonypan
2022-07-05 02:42:34,280 - INFO  [main:Environment@109] - Server environment:user.home=/home/tonypan
2022-07-05 02:42:34,280 - INFO  [main:Environment@109] - Server environment:user.dir=/home/tonypan/flaky-reproduction/experiment/kafka-12508
2022-07-05 02:42:34,281 - INFO  [main:Environment@109] - Server environment:os.memory.free=400MB
2022-07-05 02:42:34,281 - INFO  [main:Environment@109] - Server environment:os.memory.max=7051MB
2022-07-05 02:42:34,281 - INFO  [main:Environment@109] - Server environment:os.memory.total=475MB
2022-07-05 02:42:34,286 - INFO  [main:FileTxnSnapLog@115] - zookeeper.snapshot.trust.empty : false
2022-07-05 02:42:34,311 - INFO  [main:ZKDatabase@117] - zookeeper.snapshotSizeFactor = 0.33
2022-07-05 02:42:34,318 - INFO  [main:ZooKeeperServer@953] - minSessionTimeout set to 1600
2022-07-05 02:42:34,318 - INFO  [main:ZooKeeperServer@962] - maxSessionTimeout set to 16000
2022-07-05 02:42:34,319 - INFO  [main:ZooKeeperServer@181] - Created server with tickTime 800 minSessionTimeout 1600 maxSessionTimeout 16000 datadir /tmp/kafka-6981558214653695432/version-2 snapdir /tmp/kafka-2662248309658638117/version-2
2022-07-05 02:42:34,331 - INFO  [main:NIOServerCnxnFactory@673] - Configuring NIO connection handler with 10s sessionless connection timeout, 3 selector thread(s), 40 worker threads, and 64 kB direct buffers.
2022-07-05 02:42:34,337 - INFO  [main:NIOServerCnxnFactory@686] - binding to port /127.0.0.1:0
2022-07-05 02:42:34,347 - INFO  [main:FileTxnSnapLog@404] - Snapshotting: 0x0 to /tmp/kafka-2662248309658638117/version-2/snapshot.0
2022-07-05 02:42:34,350 - INFO  [main:FileTxnSnapLog@404] - Snapshotting: 0x0 to /tmp/kafka-2662248309658638117/version-2/snapshot.0
2022-07-05 02:42:34,376 - INFO  [ProcessThread(sid:0 cport:33589)::PrepRequestProcessor@132] - PrepRequestProcessor (sid:0) started, reconfigEnabled=false
2022-07-05 02:42:34,710 - INFO  [main:AbstractConfig@372] - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 0
	host.name = localhost
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.8-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/junit6652325244492370660/junit1604483620797807122
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.8-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000000
	metadata.log.dir = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = -1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 5
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:33589
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 10000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

2022-07-05 02:42:34,729 - INFO  [main:X509Util@79] - Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation
2022-07-05 02:42:34,818 - INFO  [main:Logging@66] - starting
2022-07-05 02:42:34,819 - INFO  [main:Logging@66] - Connecting to zookeeper on 127.0.0.1:33589
2022-07-05 02:42:34,840 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:33589.
2022-07-05 02:42:34,847 - INFO  [main:Environment@109] - Client environment:zookeeper.version=3.5.9-83df9301aa5c2a5d284a9940177808c01bc35cef, built on 01/06/2021 20:03 GMT
2022-07-05 02:42:34,847 - INFO  [main:Environment@109] - Client environment:host.name=razor15
2022-07-05 02:42:34,847 - INFO  [main:Environment@109] - Client environment:java.version=1.8.0_275
2022-07-05 02:42:34,847 - INFO  [main:Environment@109] - Client environment:java.vendor=Private Build
2022-07-05 02:42:34,847 - INFO  [main:Environment@109] - Client environment:java.home=/usr/lib/jvm/java-8-openjdk-amd64/jre
2022-07-05 02:42:34,847 - INFO  [main:Environment@109] - Client environment:java.class.path=/home/tonypan/flaky-reproduction/experiment/kafka-12508/junit-platform-console-standalone-1.7.0.jar
2022-07-05 02:42:34,847 - INFO  [main:Environment@109] - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib
2022-07-05 02:42:34,847 - INFO  [main:Environment@109] - Client environment:java.io.tmpdir=/tmp
2022-07-05 02:42:34,847 - INFO  [main:Environment@109] - Client environment:java.compiler=<NA>
2022-07-05 02:42:34,847 - INFO  [main:Environment@109] - Client environment:os.name=Linux
2022-07-05 02:42:34,848 - INFO  [main:Environment@109] - Client environment:os.arch=amd64
2022-07-05 02:42:34,848 - INFO  [main:Environment@109] - Client environment:os.version=4.15.0-128-generic
2022-07-05 02:42:34,848 - INFO  [main:Environment@109] - Client environment:user.name=tonypan
2022-07-05 02:42:34,848 - INFO  [main:Environment@109] - Client environment:user.home=/home/tonypan
2022-07-05 02:42:34,848 - INFO  [main:Environment@109] - Client environment:user.dir=/home/tonypan/flaky-reproduction/experiment/kafka-12508
2022-07-05 02:42:34,848 - INFO  [main:Environment@109] - Client environment:os.memory.free=337MB
2022-07-05 02:42:34,848 - INFO  [main:Environment@109] - Client environment:os.memory.max=7051MB
2022-07-05 02:42:34,848 - INFO  [main:Environment@109] - Client environment:os.memory.total=442MB
2022-07-05 02:42:34,853 - INFO  [main:ZooKeeper@868] - Initiating client connection, connectString=127.0.0.1:33589 sessionTimeout=10000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@31add175
2022-07-05 02:42:34,857 - INFO  [main:ClientCnxnSocket@237] - jute.maxbuffer value is 4194304 Bytes
2022-07-05 02:42:34,864 - INFO  [main:ClientCnxn@1653] - zookeeper.request.timeout value is 0. feature enabled=
2022-07-05 02:42:34,865 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Waiting until connected.
2022-07-05 02:42:34,871 - INFO  [main-SendThread(127.0.0.1:33589):ClientCnxn$SendThread@1112] - Opening socket connection to server localhost/127.0.0.1:33589. Will not attempt to authenticate using SASL (unknown error)
2022-07-05 02:42:34,873 - INFO  [main-SendThread(127.0.0.1:33589):ClientCnxn$SendThread@959] - Socket connection established, initiating session, client: /127.0.0.1:36868, server: localhost/127.0.0.1:33589
2022-07-05 02:42:34,883 - INFO  [SyncThread:0:FileTxnLog@218] - Creating new log file: log.1
2022-07-05 02:42:34,892 - INFO  [main-SendThread(127.0.0.1:33589):ClientCnxn$SendThread@1394] - Session establishment complete on server localhost/127.0.0.1:33589, sessionid = 0x100a45eb35f0000, negotiated timeout = 10000
2022-07-05 02:42:34,898 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Connected.
2022-07-05 02:42:35,013 - INFO  [feature-zk-node-event-process-thread:Logging@66] - [feature-zk-node-event-process-thread]: Starting
2022-07-05 02:42:35,026 - INFO  [feature-zk-node-event-process-thread:Logging@66] - Feature ZK node at path: /feature does not exist
2022-07-05 02:42:35,026 - INFO  [feature-zk-node-event-process-thread:Logging@66] - Cleared cache
2022-07-05 02:42:35,301 - INFO  [main:Logging@66] - Cluster ID = kaU--u7iRi2Y8VqvcjcEiA
2022-07-05 02:42:35,305 - WARN  [main:Logging@70] - No meta.properties file under dir /tmp/junit6652325244492370660/junit1604483620797807122/meta.properties
2022-07-05 02:42:35,366 - INFO  [main:AbstractConfig@372] - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 0
	host.name = localhost
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.8-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/junit6652325244492370660/junit1604483620797807122
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.8-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000000
	metadata.log.dir = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = -1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 5
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:33589
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 10000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

2022-07-05 02:42:35,377 - INFO  [main:AbstractConfig@372] - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 0
	host.name = localhost
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.8-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/junit6652325244492370660/junit1604483620797807122
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.8-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000000
	metadata.log.dir = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = -1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 5
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:33589
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 10000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

2022-07-05 02:42:35,411 - INFO  [ThrottledChannelReaper-Fetch:Logging@66] - [ThrottledChannelReaper-Fetch]: Starting
2022-07-05 02:42:35,411 - INFO  [ThrottledChannelReaper-Produce:Logging@66] - [ThrottledChannelReaper-Produce]: Starting
2022-07-05 02:42:35,413 - INFO  [ThrottledChannelReaper-Request:Logging@66] - [ThrottledChannelReaper-Request]: Starting
2022-07-05 02:42:35,414 - INFO  [ThrottledChannelReaper-ControllerMutation:Logging@66] - [ThrottledChannelReaper-ControllerMutation]: Starting
2022-07-05 02:42:35,445 - INFO  [main:Logging@66] - Loading logs from log dirs ArraySeq(/tmp/junit6652325244492370660/junit1604483620797807122)
2022-07-05 02:42:35,448 - INFO  [main:Logging@66] - Attempting recovery for all logs in /tmp/junit6652325244492370660/junit1604483620797807122 since no clean shutdown file was found
2022-07-05 02:42:35,453 - INFO  [main:Logging@66] - Loaded 0 logs in 0ms.
2022-07-05 02:42:35,453 - INFO  [main:Logging@66] - Starting log cleanup with a period of 300000 ms.
2022-07-05 02:42:35,456 - INFO  [main:Logging@66] - Starting log flusher with a default period of 9223372036854775807 ms.
2022-07-05 02:42:35,866 - INFO  [main:Logging@66] - Updated connection-accept-rate max connection creation rate to 2147483647
2022-07-05 02:42:35,869 - INFO  [main:Logging@66] - Awaiting socket connections on localhost:35723.
2022-07-05 02:42:35,908 - INFO  [main:Logging@66] - [SocketServer listenerType=ZK_BROKER, nodeId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT)
2022-07-05 02:42:35,935 - INFO  [BrokerToControllerChannelManager broker=0 name=alterIsr:Logging@66] - [BrokerToControllerChannelManager broker=0 name=alterIsr]: Starting
2022-07-05 02:42:35,958 - INFO  [ExpirationReaper-0-Produce:Logging@66] - [ExpirationReaper-0-Produce]: Starting
2022-07-05 02:42:35,958 - INFO  [ExpirationReaper-0-Fetch:Logging@66] - [ExpirationReaper-0-Fetch]: Starting
2022-07-05 02:42:35,958 - INFO  [ExpirationReaper-0-DeleteRecords:Logging@66] - [ExpirationReaper-0-DeleteRecords]: Starting
2022-07-05 02:42:35,959 - INFO  [ExpirationReaper-0-ElectLeader:Logging@66] - [ExpirationReaper-0-ElectLeader]: Starting
2022-07-05 02:42:35,972 - INFO  [LogDirFailureHandler:Logging@66] - [LogDirFailureHandler]: Starting
2022-07-05 02:42:36,013 - INFO  [main:Logging@66] - Creating /brokers/ids/0 (is it secure? false)
2022-07-05 02:42:36,037 - INFO  [main:Logging@66] - Stat of the created znode at /brokers/ids/0 is: 25,25,1657003356030,1657003356030,1,0,0,72238320681156608,204,0,25

2022-07-05 02:42:36,038 - INFO  [main:Logging@66] - Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://localhost:35723, czxid (broker epoch): 25
2022-07-05 02:42:36,099 - INFO  [ExpirationReaper-0-topic:Logging@66] - [ExpirationReaper-0-topic]: Starting
2022-07-05 02:42:36,104 - INFO  [ExpirationReaper-0-Heartbeat:Logging@66] - [ExpirationReaper-0-Heartbeat]: Starting
2022-07-05 02:42:36,105 - INFO  [ExpirationReaper-0-Rebalance:Logging@66] - [ExpirationReaper-0-Rebalance]: Starting
2022-07-05 02:42:36,106 - INFO  [controller-event-thread:Logging@66] - Successfully created /controller_epoch with initial epoch 0
2022-07-05 02:42:36,118 - INFO  [main-EventThread:Logging@66] - Feature ZK node created at path: /feature
2022-07-05 02:42:36,121 - INFO  [main:Logging@66] - [GroupCoordinator 0]: Starting up.
2022-07-05 02:42:36,124 - INFO  [main:Logging@66] - [GroupCoordinator 0]: Startup complete.
2022-07-05 02:42:36,147 - INFO  [main:Logging@66] - [ProducerId Manager 0]: Acquired new producerId block (brokerId:0,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1
2022-07-05 02:42:36,148 - INFO  [main:Logging@66] - [TransactionCoordinator id=0] Starting up.
2022-07-05 02:42:36,151 - INFO  [TxnMarkerSenderThread-0:Logging@66] - [Transaction Marker Channel Manager 0]: Starting
2022-07-05 02:42:36,151 - INFO  [main:Logging@66] - [TransactionCoordinator id=0] Startup complete.
2022-07-05 02:42:36,152 - INFO  [feature-zk-node-event-process-thread:Logging@66] - Updated cache from existing <empty> to latest FinalizedFeaturesAndEpoch(features=Features{}, epoch=0).
2022-07-05 02:42:36,177 - INFO  [ExpirationReaper-0-AlterAcls:Logging@66] - [ExpirationReaper-0-AlterAcls]: Starting
2022-07-05 02:42:36,200 - INFO  [/config/changes-event-process-thread:Logging@66] - [/config/changes-event-process-thread]: Starting
2022-07-05 02:42:36,207 - INFO  [main:Logging@66] - [SocketServer listenerType=ZK_BROKER, nodeId=0] Starting socket server acceptors and processors
2022-07-05 02:42:36,211 - INFO  [main:Logging@66] - [SocketServer listenerType=ZK_BROKER, nodeId=0] Started data-plane acceptor and processor(s) for endpoint : ListenerName(PLAINTEXT)
2022-07-05 02:42:36,212 - INFO  [main:Logging@66] - [SocketServer listenerType=ZK_BROKER, nodeId=0] Started socket server acceptors and processors
2022-07-05 02:42:36,213 - WARN  [main:AppInfoParser@46] - Error while loading kafka-version.properties: null
2022-07-05 02:42:36,214 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2022-07-05 02:42:36,214 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2022-07-05 02:42:36,214 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1657003354035
2022-07-05 02:42:36,215 - INFO  [main:Logging@66] - [KafkaServer id=0] started
2022-07-05 02:42:36,232 - INFO  [main:AbstractConfig@372] - AdminClientConfig values: 
	bootstrap.servers = [localhost:35723]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2022-07-05 02:42:36,254 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2022-07-05 02:42:36,254 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2022-07-05 02:42:36,254 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1657003356254
2022-07-05 02:42:36,311 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Creating topic inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0))
2022-07-05 02:42:36,346 - INFO  [BrokerToControllerChannelManager broker=0 name=alterIsr:Logging@66] - [BrokerToControllerChannelManager broker=0 name=alterIsr]: Recorded new controller, from now on will use broker localhost:35723 (id: 0 rack: null)
2022-07-05 02:42:36,381 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0)
2022-07-05 02:42:36,448 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0, dir=/tmp/junit6652325244492370660/junit1604483620797807122] Loading producer state till offset 0 with message format version 2
2022-07-05 02:42:36,452 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 in /tmp/junit6652325244492370660/junit1604483620797807122/inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.8-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2022-07-05 02:42:36,455 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 broker=0] No checkpointed highwatermark is found for partition inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0
2022-07-05 02:42:36,456 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 broker=0] Log loaded for partition inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 with initial high watermark 0
2022-07-05 02:42:36,481 - INFO  [kafka-admin-client-thread | adminclient-1:AppInfoParser@83] - App info kafka.admin.client for adminclient-1 unregistered
2022-07-05 02:42:36,483 - INFO  [kafka-admin-client-thread | adminclient-1:Metrics@659] - Metrics scheduler closed
2022-07-05 02:42:36,483 - INFO  [kafka-admin-client-thread | adminclient-1:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:42:36,484 - INFO  [kafka-admin-client-thread | adminclient-1:Metrics@669] - Metrics reporters closed
2022-07-05 02:42:36,487 - INFO  [main:AbstractConfig@372] - AdminClientConfig values: 
	bootstrap.servers = [localhost:35723]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2022-07-05 02:42:36,489 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2022-07-05 02:42:36,489 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2022-07-05 02:42:36,489 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1657003356489
2022-07-05 02:42:36,501 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Creating topic outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0))
2022-07-05 02:42:36,512 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0)
2022-07-05 02:42:36,515 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Log partition=outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0, dir=/tmp/junit6652325244492370660/junit1604483620797807122] Loading producer state till offset 0 with message format version 2
2022-07-05 02:42:36,515 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Created log for partition outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 in /tmp/junit6652325244492370660/junit1604483620797807122/outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.8-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2022-07-05 02:42:36,516 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 broker=0] No checkpointed highwatermark is found for partition outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0
2022-07-05 02:42:36,516 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 broker=0] Log loaded for partition outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 with initial high watermark 0
2022-07-05 02:42:36,520 - INFO  [kafka-admin-client-thread | adminclient-2:AppInfoParser@83] - App info kafka.admin.client for adminclient-2 unregistered
2022-07-05 02:42:36,521 - INFO  [kafka-admin-client-thread | adminclient-2:Metrics@659] - Metrics scheduler closed
2022-07-05 02:42:36,521 - INFO  [kafka-admin-client-thread | adminclient-2:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:42:36,521 - INFO  [kafka-admin-client-thread | adminclient-2:Metrics@669] - Metrics reporters closed
2022-07-05 02:42:36,575 - INFO  [main:AbstractConfig@372] - StreamsConfig values: 
	acceptable.recovery.lag = 10000
	application.id = appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover
	application.server = 
	bootstrap.servers = [localhost:35723]
	buffered.records.per.partition = 1000
	built.in.metrics.version = latest
	cache.max.bytes.buffering = 0
	client.id = 
	commit.interval.ms = 300000
	connections.max.idle.ms = 540000
	default.deserialization.exception.handler = class org.apache.kafka.streams.errors.LogAndFailExceptionHandler
	default.key.serde = class org.apache.kafka.common.serialization.Serdes$IntegerSerde
	default.production.exception.handler = class org.apache.kafka.streams.errors.DefaultProductionExceptionHandler
	default.timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	default.value.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	default.windowed.key.serde.inner = null
	default.windowed.value.serde.inner = null
	max.task.idle.ms = 0
	max.warmup.replicas = 2
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	probing.rebalance.interval.ms = 600000
	processing.guarantee = at_least_once
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retries = 0
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 600000
	state.dir = /tmp/kafka-8079245588962199859
	task.timeout.ms = 300000
	topology.optimization = none
	upgrade.from = null
	window.size.ms = null
	windowstore.changelog.additional.retention.ms = 86400000

2022-07-05 02:42:36,605 - WARN  [main:StateDirectory@138] - Using an OS temp directory in the state.dir property can cause failures with writing the checkpoint file due to the fact that this directory can be cleared by the OS. Resolved state.dir: [/tmp/kafka-8079245588962199859]
2022-07-05 02:42:36,606 - INFO  [main:StateDirectory@212] - No process id found on disk, got fresh process id bc03ea36-31db-4df8-a3d8-3ba77abb2da3
2022-07-05 02:42:36,639 - INFO  [main:AbstractConfig@372] - AdminClientConfig values: 
	bootstrap.servers = [localhost:35723]
	client.dns.lookup = use_all_dns_ips
	client.id = appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-admin
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2022-07-05 02:42:36,641 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2022-07-05 02:42:36,642 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2022-07-05 02:42:36,642 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1657003356641
2022-07-05 02:42:36,645 - WARN  [main:ClientMetrics@55] - Error while loading kafka-streams-version.properties
java.lang.NullPointerException
	at java.util.Properties$LineReader.readLine(Properties.java:434)
	at java.util.Properties.load0(Properties.java:353)
	at java.util.Properties.load(Properties.java:341)
	at org.apache.kafka.streams.internals.metrics.ClientMetrics.<clinit>(ClientMetrics.java:53)
	at org.apache.kafka.streams.KafkaStreams.<init>(KafkaStreams.java:825)
	at org.apache.kafka.streams.KafkaStreams.<init>(KafkaStreams.java:781)
	at org.apache.kafka.streams.KafkaStreams.<init>(KafkaStreams.java:691)
	at org.apache.kafka.streams.integration.EmitOnChangeIntegrationTest.shouldEmitSameRecordAfterFailover(EmitOnChangeIntegrationTest.java:101)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
2022-07-05 02:42:36,648 - INFO  [main:KafkaStreams@825] - stream-client [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3] Kafka Streams version: unknown
2022-07-05 02:42:36,648 - INFO  [main:KafkaStreams@826] - stream-client [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3] Kafka Streams commit ID: unknown
2022-07-05 02:42:36,657 - INFO  [main:StreamThread@337] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1] Creating restore consumer client
2022-07-05 02:42:36,663 - INFO  [main:AbstractConfig@372] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [localhost:35723]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1-restore-consumer
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = false
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2022-07-05 02:42:36,693 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2022-07-05 02:42:36,693 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2022-07-05 02:42:36,693 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1657003356693
2022-07-05 02:42:36,702 - INFO  [main:ActiveTaskCreator@96] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1] Creating thread producer client
2022-07-05 02:42:36,709 - INFO  [main:AbstractConfig@372] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:35723]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-07-05 02:42:36,727 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2022-07-05 02:42:36,728 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2022-07-05 02:42:36,728 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1657003356727
2022-07-05 02:42:36,733 - INFO  [main:StreamThread@389] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1] Creating consumer client
2022-07-05 02:42:36,735 - INFO  [kafka-producer-network-thread | appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1-producer:Metadata@279] - [Producer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1-producer] Cluster ID: kaU--u7iRi2Y8VqvcjcEiA
2022-07-05 02:42:36,735 - INFO  [main:AbstractConfig@372] - ConsumerConfig values: 
	allow.auto.create.topics = false
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:35723]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1-consumer
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = false
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2022-07-05 02:42:36,745 - INFO  [main:AssignorConfiguration@114] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1-consumer] Cooperative rebalancing enabled now
2022-07-05 02:42:36,757 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2022-07-05 02:42:36,757 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2022-07-05 02:42:36,757 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1657003356757
2022-07-05 02:42:36,764 - INFO  [main:KafkaStreams@321] - stream-client [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3] State transition from CREATED to REBALANCING
2022-07-05 02:42:36,764 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:StreamThread@550] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1] Starting
2022-07-05 02:42:36,765 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:StreamThread@230] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1] State transition from CREATED to STARTING
2022-07-05 02:42:36,765 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:KafkaConsumer@968] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Subscribed to topic(s): inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover
2022-07-05 02:42:36,777 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:Metadata@279] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Cluster ID: kaU--u7iRi2Y8VqvcjcEiA
2022-07-05 02:42:36,779 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Creating topic __consumer_offsets with configuration {segment.bytes=104857600, cleanup.policy=compact, compression.type=producer} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0))
2022-07-05 02:42:36,795 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(__consumer_offsets-4, __consumer_offsets-3, __consumer_offsets-2, __consumer_offsets-0, __consumer_offsets-1)
2022-07-05 02:42:36,798 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-3, dir=/tmp/junit6652325244492370660/junit1604483620797807122] Loading producer state till offset 0 with message format version 2
2022-07-05 02:42:36,799 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-3 in /tmp/junit6652325244492370660/junit1604483620797807122/__consumer_offsets-3 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.8-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2022-07-05 02:42:36,799 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3
2022-07-05 02:42:36,799 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-3 broker=0] Log loaded for partition __consumer_offsets-3 with initial high watermark 0
2022-07-05 02:42:36,802 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-2, dir=/tmp/junit6652325244492370660/junit1604483620797807122] Loading producer state till offset 0 with message format version 2
2022-07-05 02:42:36,803 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-2 in /tmp/junit6652325244492370660/junit1604483620797807122/__consumer_offsets-2 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.8-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2022-07-05 02:42:36,803 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2
2022-07-05 02:42:36,803 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-2 broker=0] Log loaded for partition __consumer_offsets-2 with initial high watermark 0
2022-07-05 02:42:36,811 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-4, dir=/tmp/junit6652325244492370660/junit1604483620797807122] Loading producer state till offset 0 with message format version 2
2022-07-05 02:42:36,811 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-4 in /tmp/junit6652325244492370660/junit1604483620797807122/__consumer_offsets-4 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.8-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2022-07-05 02:42:36,811 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4
2022-07-05 02:42:36,811 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-4 broker=0] Log loaded for partition __consumer_offsets-4 with initial high watermark 0
2022-07-05 02:42:36,819 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-1, dir=/tmp/junit6652325244492370660/junit1604483620797807122] Loading producer state till offset 0 with message format version 2
2022-07-05 02:42:36,819 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-1 in /tmp/junit6652325244492370660/junit1604483620797807122/__consumer_offsets-1 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.8-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2022-07-05 02:42:36,820 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1
2022-07-05 02:42:36,820 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-1 broker=0] Log loaded for partition __consumer_offsets-1 with initial high watermark 0
2022-07-05 02:42:36,827 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-0, dir=/tmp/junit6652325244492370660/junit1604483620797807122] Loading producer state till offset 0 with message format version 2
2022-07-05 02:42:36,828 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-0 in /tmp/junit6652325244492370660/junit1604483620797807122/__consumer_offsets-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.8-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2022-07-05 02:42:36,828 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0
2022-07-05 02:42:36,828 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-0 broker=0] Log loaded for partition __consumer_offsets-0 with initial high watermark 0
2022-07-05 02:42:36,834 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Elected as the group coordinator for partition 3
2022-07-05 02:42:36,835 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3
2022-07-05 02:42:36,836 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Elected as the group coordinator for partition 2
2022-07-05 02:42:36,836 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2
2022-07-05 02:42:36,836 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Elected as the group coordinator for partition 4
2022-07-05 02:42:36,836 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4
2022-07-05 02:42:36,836 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Elected as the group coordinator for partition 1
2022-07-05 02:42:36,836 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1
2022-07-05 02:42:36,836 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Elected as the group coordinator for partition 0
2022-07-05 02:42:36,836 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0
2022-07-05 02:42:36,839 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 4 milliseconds, of which 1 milliseconds was spent in the scheduler.
2022-07-05 02:42:36,839 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 3 milliseconds, of which 3 milliseconds was spent in the scheduler.
2022-07-05 02:42:36,840 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 4 milliseconds, of which 4 milliseconds was spent in the scheduler.
2022-07-05 02:42:36,840 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 4 milliseconds, of which 4 milliseconds was spent in the scheduler.
2022-07-05 02:42:36,840 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 4 milliseconds, of which 4 milliseconds was spent in the scheduler.
2022-07-05 02:42:36,870 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:StreamThread@829] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update
2022-07-05 02:42:36,874 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:AbstractCoordinator$FindCoordinatorResponseHandler@844] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Discovered group coordinator localhost:35723 (id: 2147483647 rack: null)
2022-07-05 02:42:36,876 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:AbstractCoordinator@534] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] (Re-)joining group
2022-07-05 02:42:36,894 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [GroupCoordinator 0]: Dynamic member with unknown member id joins group appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover in Empty state. Created a new member id appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1-consumer-f0d52b7d-fda3-45b8-bcc6-e891995a1682 and request the member to rejoin with this id.
2022-07-05 02:42:36,899 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:AbstractCoordinator@982] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Request joining group due to: need to re-join with the given member-id
2022-07-05 02:42:36,899 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:AbstractCoordinator@534] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] (Re-)joining group
2022-07-05 02:42:36,904 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover in state PreparingRebalance with old generation 0 (__consumer_offsets-0) (reason: Adding new member appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1-consumer-f0d52b7d-fda3-45b8-bcc6-e891995a1682 with group instance id None)
2022-07-05 02:42:36,909 - INFO  [executor-Rebalance:Logging@66] - [GroupCoordinator 0]: Stabilized group appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover generation 1 (__consumer_offsets-0) with 1 members
2022-07-05 02:42:36,911 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Successfully joined group with generation Generation{generationId=1, memberId='appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1-consumer-f0d52b7d-fda3-45b8-bcc6-e891995a1682', protocol='stream'}
2022-07-05 02:42:36,927 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Creating topic appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog with configuration {message.timestamp.type=CreateTime, cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0))
2022-07-05 02:42:36,937 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0)
2022-07-05 02:42:36,940 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Log partition=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0, dir=/tmp/junit6652325244492370660/junit1604483620797807122] Loading producer state till offset 0 with message format version 2
2022-07-05 02:42:36,941 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Created log for partition appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0 in /tmp/junit6652325244492370660/junit1604483620797807122/appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.8-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2022-07-05 02:42:36,941 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0 broker=0] No checkpointed highwatermark is found for partition appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0
2022-07-05 02:42:36,941 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0 broker=0] Log loaded for partition appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0 with initial high watermark 0
2022-07-05 02:42:36,954 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:HighAvailabilityTaskAssignor@95] - Decided on assignment: {bc03ea36-31db-4df8-a3d8-3ba77abb2da3=[activeTasks: ([0_0]) standbyTasks: ([]) prevActiveTasks: ([]) prevStandbyTasks: ([]) changelogOffsetTotalsByTask: ([]) taskLagTotals: ([0_0=0]) capacity: 1 assigned: 1]} with no followup probing rebalance.
2022-07-05 02:42:36,954 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:StreamsPartitionAssignor@595] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1-consumer] Assigned tasks [0_0] including stateful [0_0] to clients as: 
bc03ea36-31db-4df8-a3d8-3ba77abb2da3=[activeTasks: ([0_0]) standbyTasks: ([])].
2022-07-05 02:42:36,958 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:StreamsPartitionAssignor@803] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1-consumer] Client bc03ea36-31db-4df8-a3d8-3ba77abb2da3 per-consumer assignment:
	prev owned active {}
	prev owned standby {appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1-consumer-f0d52b7d-fda3-45b8-bcc6-e891995a1682=[]}
	assigned active {appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1-consumer-f0d52b7d-fda3-45b8-bcc6-e891995a1682=[0_0]}
	revoking active {}
	assigned standby {}

2022-07-05 02:42:36,958 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:StreamsPartitionAssignor@822] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1-consumer] Finished stable assignment of tasks, no followup rebalances required.
2022-07-05 02:42:36,958 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:ConsumerCoordinator@626] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Finished assignment for group at generation 1: {appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1-consumer-f0d52b7d-fda3-45b8-bcc6-e891995a1682=Assignment(partitions=[inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0], userDataSize=48)}
2022-07-05 02:42:36,965 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover for generation 1. The group has 1 members, 0 of which are static.
2022-07-05 02:42:37,020 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:AbstractCoordinator$SyncGroupResponseHandler@756] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Successfully synced group in generation Generation{generationId=1, memberId='appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1-consumer-f0d52b7d-fda3-45b8-bcc6-e891995a1682', protocol='stream'}
2022-07-05 02:42:37,021 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:ConsumerCoordinator@392] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Updating assignment with
	Assigned partitions:                       [inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0]
	Current owned partitions:                  []
	Added partitions (assigned - owned):       [inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0]
	Revoked partitions (owned - assigned):     []

2022-07-05 02:42:37,021 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:ConsumerCoordinator@276] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Notifying assignor about the new Assignment(partitions=[inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0], userDataSize=48)
2022-07-05 02:42:37,022 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:StreamsPartitionAssignor@1306] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1-consumer] No followup rebalance was requested, resetting the rebalance schedule.
2022-07-05 02:42:37,023 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:TaskManager@254] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1] Handle new assignment with:
	New active tasks: [0_0]
	New standby tasks: []
	Existing active tasks: []
	Existing standby tasks: []
2022-07-05 02:42:37,038 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:ConsumerCoordinator@288] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Adding newly assigned partitions: inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0
2022-07-05 02:42:37,039 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:StreamThread@230] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1] State transition from STARTING to PARTITIONS_ASSIGNED
2022-07-05 02:42:37,048 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:ConsumerCoordinator$OffsetFetchResponseHandler@1352] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Found no committed offset for partition inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0
2022-07-05 02:42:37,060 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:SubscriptionState@398] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Resetting offset for partition inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35723 (id: 0 rack: null)], epoch=0}}.
2022-07-05 02:42:37,222 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:RocksDBTimestampedStore@100] - Opening store test-store in regular mode
2022-07-05 02:42:37,226 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:ProcessorStateManager@256] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1] task [0_0] State store test-store did not find checkpoint offset, hence would default to the starting offset at changelog appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0
2022-07-05 02:42:37,226 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:StreamTask@235] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1] task [0_0] Initialized
2022-07-05 02:42:37,232 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:KafkaConsumer@1123] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1-restore-consumer, groupId=null] Subscribed to partition(s): appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0
2022-07-05 02:42:37,233 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:SubscriptionState@619] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1-restore-consumer, groupId=null] Seeking to EARLIEST offset of partition appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0
2022-07-05 02:42:37,244 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:Metadata@279] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1-restore-consumer, groupId=null] Cluster ID: kaU--u7iRi2Y8VqvcjcEiA
2022-07-05 02:42:37,247 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:SubscriptionState@398] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1-restore-consumer, groupId=null] Resetting offset for partition appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35723 (id: 0 rack: null)], epoch=0}}.
2022-07-05 02:42:37,349 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:StoreChangelogReader@609] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1] Finished restoring changelog appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0 to store test-store with a total number of 0 records
2022-07-05 02:42:37,351 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:ConsumerCoordinator$OffsetFetchResponseHandler@1352] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Found no committed offset for partition inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0
2022-07-05 02:42:37,355 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:StreamTask@255] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1] task [0_0] Restored and ready to run
2022-07-05 02:42:37,355 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:StreamThread@853] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1] Restoration took 316 ms for all tasks [0_0]
2022-07-05 02:42:37,355 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:StreamThread@230] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1] State transition from PARTITIONS_ASSIGNED to RUNNING
2022-07-05 02:42:37,356 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:KafkaStreams@321] - stream-client [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3] State transition from REBALANCING to RUNNING
2022-07-05 02:42:37,358 - INFO  [main:AbstractConfig@372] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:35723]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.IntegerSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2022-07-05 02:42:37,360 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2022-07-05 02:42:37,360 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2022-07-05 02:42:37,360 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1657003357360
2022-07-05 02:42:37,363 - INFO  [kafka-producer-network-thread | producer-1:Metadata@279] - [Producer clientId=producer-1] Cluster ID: kaU--u7iRi2Y8VqvcjcEiA
2022-07-05 02:42:37,366 - INFO  [main:KafkaProducer@1204] - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2022-07-05 02:42:37,383 - INFO  [main:Metrics@659] - Metrics scheduler closed
2022-07-05 02:42:37,383 - INFO  [main:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:42:37,383 - INFO  [main:Metrics@669] - Metrics reporters closed
2022-07-05 02:42:37,383 - INFO  [main:AppInfoParser@83] - App info kafka.producer for producer-1 unregistered
2022-07-05 02:42:37,385 - INFO  [main:AbstractConfig@372] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:35723]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-e0579a4c-8286-4010-8b07-55f39759a0c8-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = e0579a4c-8286-4010-8b07-55f39759a0c8
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.IntegerDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2022-07-05 02:42:37,387 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2022-07-05 02:42:37,387 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2022-07-05 02:42:37,387 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1657003357386
2022-07-05 02:42:37,388 - INFO  [main:KafkaConsumer@968] - [Consumer clientId=consumer-e0579a4c-8286-4010-8b07-55f39759a0c8-1, groupId=e0579a4c-8286-4010-8b07-55f39759a0c8] Subscribed to topic(s): outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover
2022-07-05 02:42:37,391 - INFO  [main:Metadata@279] - [Consumer clientId=consumer-e0579a4c-8286-4010-8b07-55f39759a0c8-1, groupId=e0579a4c-8286-4010-8b07-55f39759a0c8] Cluster ID: kaU--u7iRi2Y8VqvcjcEiA
2022-07-05 02:42:37,392 - INFO  [main:AbstractCoordinator$FindCoordinatorResponseHandler@844] - [Consumer clientId=consumer-e0579a4c-8286-4010-8b07-55f39759a0c8-1, groupId=e0579a4c-8286-4010-8b07-55f39759a0c8] Discovered group coordinator localhost:35723 (id: 2147483647 rack: null)
2022-07-05 02:42:37,392 - INFO  [main:AbstractCoordinator@534] - [Consumer clientId=consumer-e0579a4c-8286-4010-8b07-55f39759a0c8-1, groupId=e0579a4c-8286-4010-8b07-55f39759a0c8] (Re-)joining group
2022-07-05 02:42:37,394 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [GroupCoordinator 0]: Dynamic member with unknown member id joins group e0579a4c-8286-4010-8b07-55f39759a0c8 in Empty state. Created a new member id consumer-e0579a4c-8286-4010-8b07-55f39759a0c8-1-0c529527-743a-4077-9b60-653929f7f7eb and request the member to rejoin with this id.
2022-07-05 02:42:37,395 - INFO  [main:AbstractCoordinator@982] - [Consumer clientId=consumer-e0579a4c-8286-4010-8b07-55f39759a0c8-1, groupId=e0579a4c-8286-4010-8b07-55f39759a0c8] Request joining group due to: need to re-join with the given member-id
2022-07-05 02:42:37,395 - INFO  [main:AbstractCoordinator@534] - [Consumer clientId=consumer-e0579a4c-8286-4010-8b07-55f39759a0c8-1, groupId=e0579a4c-8286-4010-8b07-55f39759a0c8] (Re-)joining group
2022-07-05 02:42:37,396 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group e0579a4c-8286-4010-8b07-55f39759a0c8 in state PreparingRebalance with old generation 0 (__consumer_offsets-0) (reason: Adding new member consumer-e0579a4c-8286-4010-8b07-55f39759a0c8-1-0c529527-743a-4077-9b60-653929f7f7eb with group instance id None)
2022-07-05 02:42:37,396 - INFO  [executor-Rebalance:Logging@66] - [GroupCoordinator 0]: Stabilized group e0579a4c-8286-4010-8b07-55f39759a0c8 generation 1 (__consumer_offsets-0) with 1 members
2022-07-05 02:42:37,397 - INFO  [main:AbstractCoordinator$JoinGroupResponseHandler@590] - [Consumer clientId=consumer-e0579a4c-8286-4010-8b07-55f39759a0c8-1, groupId=e0579a4c-8286-4010-8b07-55f39759a0c8] Successfully joined group with generation Generation{generationId=1, memberId='consumer-e0579a4c-8286-4010-8b07-55f39759a0c8-1-0c529527-743a-4077-9b60-653929f7f7eb', protocol='range'}
2022-07-05 02:42:37,398 - INFO  [main:ConsumerCoordinator@626] - [Consumer clientId=consumer-e0579a4c-8286-4010-8b07-55f39759a0c8-1, groupId=e0579a4c-8286-4010-8b07-55f39759a0c8] Finished assignment for group at generation 1: {consumer-e0579a4c-8286-4010-8b07-55f39759a0c8-1-0c529527-743a-4077-9b60-653929f7f7eb=Assignment(partitions=[outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0])}
2022-07-05 02:42:37,398 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group e0579a4c-8286-4010-8b07-55f39759a0c8 for generation 1. The group has 1 members, 0 of which are static.
2022-07-05 02:42:37,401 - INFO  [main:AbstractCoordinator$SyncGroupResponseHandler@756] - [Consumer clientId=consumer-e0579a4c-8286-4010-8b07-55f39759a0c8-1, groupId=e0579a4c-8286-4010-8b07-55f39759a0c8] Successfully synced group in generation Generation{generationId=1, memberId='consumer-e0579a4c-8286-4010-8b07-55f39759a0c8-1-0c529527-743a-4077-9b60-653929f7f7eb', protocol='range'}
2022-07-05 02:42:37,401 - INFO  [main:ConsumerCoordinator@276] - [Consumer clientId=consumer-e0579a4c-8286-4010-8b07-55f39759a0c8-1, groupId=e0579a4c-8286-4010-8b07-55f39759a0c8] Notifying assignor about the new Assignment(partitions=[outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0])
2022-07-05 02:42:37,401 - INFO  [main:ConsumerCoordinator@288] - [Consumer clientId=consumer-e0579a4c-8286-4010-8b07-55f39759a0c8-1, groupId=e0579a4c-8286-4010-8b07-55f39759a0c8] Adding newly assigned partitions: outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0
2022-07-05 02:42:37,402 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1352] - [Consumer clientId=consumer-e0579a4c-8286-4010-8b07-55f39759a0c8-1, groupId=e0579a4c-8286-4010-8b07-55f39759a0c8] Found no committed offset for partition outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0
2022-07-05 02:42:37,406 - INFO  [main:SubscriptionState@398] - [Consumer clientId=consumer-e0579a4c-8286-4010-8b07-55f39759a0c8-1, groupId=e0579a4c-8286-4010-8b07-55f39759a0c8] Resetting offset for partition outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35723 (id: 0 rack: null)], epoch=0}}.
2022-07-05 02:42:37,412 - ERROR [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:TaskManager@1153] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1] Failed to process stream task 0_0 due to the following error:
org.apache.kafka.streams.errors.StreamsException: Exception caught in process. taskId=0_0, processor=KSTREAM-SOURCE-0000000000, topic=inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover, partition=0, offset=0, stacktrace=java.lang.IllegalStateException: Kaboom
	at org.apache.kafka.streams.integration.EmitOnChangeIntegrationTest.lambda$shouldEmitSameRecordAfterFailover$0(EmitOnChangeIntegrationTest.java:94)
	at org.apache.kafka.streams.kstream.internals.KStreamMap$KStreamMapProcessor.process(KStreamMap.java:41)
	at org.apache.kafka.streams.processor.internals.ProcessorAdapter.process(ProcessorAdapter.java:71)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.lambda$process$2(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:884)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forwardInternal(ProcessorContextImpl.java:281)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:260)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:219)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:172)
	at org.apache.kafka.streams.kstream.internals.KStreamMapValues$KStreamMapProcessor.process(KStreamMapValues.java:41)
	at org.apache.kafka.streams.processor.internals.ProcessorAdapter.process(ProcessorAdapter.java:71)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.lambda$process$2(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:884)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forwardInternal(ProcessorContextImpl.java:281)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:260)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:219)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:172)
	at org.apache.kafka.streams.kstream.internals.TimestampedTupleForwarder.maybeForward(TimestampedTupleForwarder.java:51)
	at org.apache.kafka.streams.kstream.internals.KTableSource$KTableSourceProcessor.process(KTableSource.java:145)
	at org.apache.kafka.streams.processor.internals.ProcessorAdapter.process(ProcessorAdapter.java:71)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.lambda$process$2(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:884)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forwardInternal(ProcessorContextImpl.java:281)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:260)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:219)
	at org.apache.kafka.streams.processor.internals.SourceNode.process(SourceNode.java:86)
	at org.apache.kafka.streams.processor.internals.StreamTask.lambda$process$1(StreamTask.java:706)
	at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:884)
	at org.apache.kafka.streams.processor.internals.StreamTask.process(StreamTask.java:706)
	at org.apache.kafka.streams.processor.internals.TaskManager.process(TaskManager.java:1136)
	at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:755)
	at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:585)
	at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:557)

	at org.apache.kafka.streams.processor.internals.StreamTask.process(StreamTask.java:733)
	at org.apache.kafka.streams.processor.internals.TaskManager.process(TaskManager.java:1136)
	at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:755)
	at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:585)
	at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:557)
Caused by: java.lang.IllegalStateException: Kaboom
	at org.apache.kafka.streams.integration.EmitOnChangeIntegrationTest.lambda$shouldEmitSameRecordAfterFailover$0(EmitOnChangeIntegrationTest.java:94)
	at org.apache.kafka.streams.kstream.internals.KStreamMap$KStreamMapProcessor.process(KStreamMap.java:41)
	at org.apache.kafka.streams.processor.internals.ProcessorAdapter.process(ProcessorAdapter.java:71)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.lambda$process$2(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:884)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forwardInternal(ProcessorContextImpl.java:281)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:260)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:219)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:172)
	at org.apache.kafka.streams.kstream.internals.KStreamMapValues$KStreamMapProcessor.process(KStreamMapValues.java:41)
	at org.apache.kafka.streams.processor.internals.ProcessorAdapter.process(ProcessorAdapter.java:71)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.lambda$process$2(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:884)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forwardInternal(ProcessorContextImpl.java:281)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:260)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:219)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:172)
	at org.apache.kafka.streams.kstream.internals.TimestampedTupleForwarder.maybeForward(TimestampedTupleForwarder.java:51)
	at org.apache.kafka.streams.kstream.internals.KTableSource$KTableSourceProcessor.process(KTableSource.java:145)
	at org.apache.kafka.streams.processor.internals.ProcessorAdapter.process(ProcessorAdapter.java:71)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.lambda$process$2(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:884)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forwardInternal(ProcessorContextImpl.java:281)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:260)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:219)
	at org.apache.kafka.streams.processor.internals.SourceNode.process(SourceNode.java:86)
	at org.apache.kafka.streams.processor.internals.StreamTask.lambda$process$1(StreamTask.java:706)
	at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:884)
	at org.apache.kafka.streams.processor.internals.StreamTask.process(StreamTask.java:706)
	... 4 more
2022-07-05 02:42:37,413 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:StreamThread@1057] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1] Informed to shut down
2022-07-05 02:42:37,413 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:StreamThread@230] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1] State transition from RUNNING to PENDING_SHUTDOWN
2022-07-05 02:42:37,414 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:StreamThread@337] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2] Creating restore consumer client
2022-07-05 02:42:37,415 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:AbstractConfig@372] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [localhost:35723]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2-restore-consumer
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = false
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2022-07-05 02:42:37,417 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2022-07-05 02:42:37,417 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2022-07-05 02:42:37,417 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1657003357417
2022-07-05 02:42:37,417 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:ActiveTaskCreator@96] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2] Creating thread producer client
2022-07-05 02:42:37,417 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:AbstractConfig@372] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:35723]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2-producer
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-07-05 02:42:37,419 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2022-07-05 02:42:37,419 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2022-07-05 02:42:37,419 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1657003357419
2022-07-05 02:42:37,420 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:StreamThread@389] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2] Creating consumer client
2022-07-05 02:42:37,421 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:AbstractConfig@372] - ConsumerConfig values: 
	allow.auto.create.topics = false
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:35723]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2-consumer
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = false
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2022-07-05 02:42:37,423 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:AssignorConfiguration@114] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2-consumer] Cooperative rebalancing enabled now
2022-07-05 02:42:37,424 - INFO  [kafka-producer-network-thread | appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2-producer:Metadata@279] - [Producer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2-producer] Cluster ID: kaU--u7iRi2Y8VqvcjcEiA
2022-07-05 02:42:37,424 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2022-07-05 02:42:37,424 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2022-07-05 02:42:37,424 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1657003357424
2022-07-05 02:42:37,425 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:StreamThread@1071] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1] Shutting down
2022-07-05 02:42:37,425 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:StreamThread@550] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2] Starting
2022-07-05 02:42:37,425 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:StreamThread@230] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2] State transition from CREATED to STARTING
2022-07-05 02:42:37,426 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:KafkaConsumer@968] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Subscribed to topic(s): inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover
2022-07-05 02:42:37,429 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:StreamTask@1144] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1] task [0_0] Suspended RUNNING
2022-07-05 02:42:37,429 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:StreamTask@290] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1] task [0_0] Suspended running
2022-07-05 02:42:37,431 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:KafkaConsumer@1077] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions
2022-07-05 02:42:37,431 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:Metadata@279] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Cluster ID: kaU--u7iRi2Y8VqvcjcEiA
2022-07-05 02:42:37,432 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:AbstractCoordinator$FindCoordinatorResponseHandler@844] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Discovered group coordinator localhost:35723 (id: 2147483647 rack: null)
2022-07-05 02:42:37,432 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:AbstractCoordinator@534] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] (Re-)joining group
2022-07-05 02:42:37,436 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupCoordinator 0]: Dynamic member with unknown member id joins group appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover in Stable state. Created a new member id appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2-consumer-30098832-1e73-483a-8e33-45205691fa67 and request the member to rejoin with this id.
2022-07-05 02:42:37,436 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:AbstractCoordinator@982] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Request joining group due to: need to re-join with the given member-id
2022-07-05 02:42:37,436 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:AbstractCoordinator@534] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] (Re-)joining group
2022-07-05 02:42:37,438 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover in state PreparingRebalance with old generation 1 (__consumer_offsets-0) (reason: Adding new member appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2-consumer-30098832-1e73-483a-8e33-45205691fa67 with group instance id None)
2022-07-05 02:42:37,443 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:RecordCollectorImpl@283] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1] task [0_0] Closing record collector dirty
2022-07-05 02:42:37,443 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:StreamTask@515] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1] task [0_0] Closed dirty
2022-07-05 02:42:37,444 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:KafkaProducer@1204] - [Producer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1-producer] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2022-07-05 02:42:37,448 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:Metrics@659] - Metrics scheduler closed
2022-07-05 02:42:37,448 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:42:37,448 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:Metrics@669] - Metrics reporters closed
2022-07-05 02:42:37,448 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:AppInfoParser@83] - App info kafka.producer for appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1-producer unregistered
2022-07-05 02:42:37,449 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:KafkaConsumer@1077] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions
2022-07-05 02:42:37,449 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:Metrics@659] - Metrics scheduler closed
2022-07-05 02:42:37,449 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:42:37,449 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:Metrics@669] - Metrics reporters closed
2022-07-05 02:42:37,451 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:AppInfoParser@83] - App info kafka.consumer for appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1-consumer unregistered
2022-07-05 02:42:37,451 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:Metrics@659] - Metrics scheduler closed
2022-07-05 02:42:37,451 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:42:37,451 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:Metrics@669] - Metrics reporters closed
2022-07-05 02:42:37,452 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:AppInfoParser@83] - App info kafka.consumer for appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1-restore-consumer unregistered
2022-07-05 02:42:37,453 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:StreamThread@230] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1] State transition from PENDING_SHUTDOWN to DEAD
2022-07-05 02:42:37,453 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:StreamThread@1100] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1] Shutdown complete
2022-07-05 02:42:37,453 - ERROR [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1:NIOServerCnxnFactory$1@92] - Thread 	StreamsThread threadId: appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1
TaskManager
	MetadataState:
	Tasks:
 died
org.apache.kafka.streams.errors.StreamsException: Exception caught in process. taskId=0_0, processor=KSTREAM-SOURCE-0000000000, topic=inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover, partition=0, offset=0, stacktrace=java.lang.IllegalStateException: Kaboom
	at org.apache.kafka.streams.integration.EmitOnChangeIntegrationTest.lambda$shouldEmitSameRecordAfterFailover$0(EmitOnChangeIntegrationTest.java:94)
	at org.apache.kafka.streams.kstream.internals.KStreamMap$KStreamMapProcessor.process(KStreamMap.java:41)
	at org.apache.kafka.streams.processor.internals.ProcessorAdapter.process(ProcessorAdapter.java:71)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.lambda$process$2(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:884)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forwardInternal(ProcessorContextImpl.java:281)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:260)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:219)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:172)
	at org.apache.kafka.streams.kstream.internals.KStreamMapValues$KStreamMapProcessor.process(KStreamMapValues.java:41)
	at org.apache.kafka.streams.processor.internals.ProcessorAdapter.process(ProcessorAdapter.java:71)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.lambda$process$2(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:884)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forwardInternal(ProcessorContextImpl.java:281)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:260)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:219)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:172)
	at org.apache.kafka.streams.kstream.internals.TimestampedTupleForwarder.maybeForward(TimestampedTupleForwarder.java:51)
	at org.apache.kafka.streams.kstream.internals.KTableSource$KTableSourceProcessor.process(KTableSource.java:145)
	at org.apache.kafka.streams.processor.internals.ProcessorAdapter.process(ProcessorAdapter.java:71)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.lambda$process$2(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:884)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forwardInternal(ProcessorContextImpl.java:281)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:260)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:219)
	at org.apache.kafka.streams.processor.internals.SourceNode.process(SourceNode.java:86)
	at org.apache.kafka.streams.processor.internals.StreamTask.lambda$process$1(StreamTask.java:706)
	at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:884)
	at org.apache.kafka.streams.processor.internals.StreamTask.process(StreamTask.java:706)
	at org.apache.kafka.streams.processor.internals.TaskManager.process(TaskManager.java:1136)
	at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:755)
	at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:585)
	at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:557)

	at org.apache.kafka.streams.processor.internals.StreamTask.process(StreamTask.java:733)
	at org.apache.kafka.streams.processor.internals.TaskManager.process(TaskManager.java:1136)
	at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:755)
	at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:585)
	at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:557)
Caused by: java.lang.IllegalStateException: Kaboom
	at org.apache.kafka.streams.integration.EmitOnChangeIntegrationTest.lambda$shouldEmitSameRecordAfterFailover$0(EmitOnChangeIntegrationTest.java:94)
	at org.apache.kafka.streams.kstream.internals.KStreamMap$KStreamMapProcessor.process(KStreamMap.java:41)
	at org.apache.kafka.streams.processor.internals.ProcessorAdapter.process(ProcessorAdapter.java:71)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.lambda$process$2(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:884)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forwardInternal(ProcessorContextImpl.java:281)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:260)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:219)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:172)
	at org.apache.kafka.streams.kstream.internals.KStreamMapValues$KStreamMapProcessor.process(KStreamMapValues.java:41)
	at org.apache.kafka.streams.processor.internals.ProcessorAdapter.process(ProcessorAdapter.java:71)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.lambda$process$2(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:884)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forwardInternal(ProcessorContextImpl.java:281)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:260)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:219)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:172)
	at org.apache.kafka.streams.kstream.internals.TimestampedTupleForwarder.maybeForward(TimestampedTupleForwarder.java:51)
	at org.apache.kafka.streams.kstream.internals.KTableSource$KTableSourceProcessor.process(KTableSource.java:145)
	at org.apache.kafka.streams.processor.internals.ProcessorAdapter.process(ProcessorAdapter.java:71)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.lambda$process$2(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:884)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forwardInternal(ProcessorContextImpl.java:281)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:260)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:219)
	at org.apache.kafka.streams.processor.internals.SourceNode.process(SourceNode.java:86)
	at org.apache.kafka.streams.processor.internals.StreamTask.lambda$process$1(StreamTask.java:706)
	at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:884)
	at org.apache.kafka.streams.processor.internals.StreamTask.process(StreamTask.java:706)
	... 4 more
2022-07-05 02:42:37,526 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:StreamThread@829] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update
2022-07-05 02:42:47,022 - INFO  [executor-Heartbeat:Logging@66] - [GroupCoordinator 0]: Member appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1-consumer-f0d52b7d-fda3-45b8-bcc6-e891995a1682 in group appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover has failed, removing it from the group
2022-07-05 02:42:47,024 - INFO  [executor-Heartbeat:Logging@66] - [GroupCoordinator 0]: Stabilized group appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover generation 2 (__consumer_offsets-0) with 1 members
2022-07-05 02:42:47,024 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:AbstractCoordinator$JoinGroupResponseHandler@590] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Successfully joined group with generation Generation{generationId=2, memberId='appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2-consumer-30098832-1e73-483a-8e33-45205691fa67', protocol='stream'}
2022-07-05 02:42:47,030 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:HighAvailabilityTaskAssignor@95] - Decided on assignment: {bc03ea36-31db-4df8-a3d8-3ba77abb2da3=[activeTasks: ([0_0]) standbyTasks: ([]) prevActiveTasks: ([]) prevStandbyTasks: ([]) changelogOffsetTotalsByTask: ([]) taskLagTotals: ([0_0=1]) capacity: 1 assigned: 1]} with no followup probing rebalance.
2022-07-05 02:42:47,030 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:StreamsPartitionAssignor@595] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2-consumer] Assigned tasks [0_0] including stateful [0_0] to clients as: 
bc03ea36-31db-4df8-a3d8-3ba77abb2da3=[activeTasks: ([0_0]) standbyTasks: ([])].
2022-07-05 02:42:47,030 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:StreamsPartitionAssignor@803] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2-consumer] Client bc03ea36-31db-4df8-a3d8-3ba77abb2da3 per-consumer assignment:
	prev owned active {}
	prev owned standby {appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2-consumer-30098832-1e73-483a-8e33-45205691fa67=[]}
	assigned active {appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2-consumer-30098832-1e73-483a-8e33-45205691fa67=[0_0]}
	revoking active {}
	assigned standby {}

2022-07-05 02:42:47,030 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:StreamsPartitionAssignor@822] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2-consumer] Finished stable assignment of tasks, no followup rebalances required.
2022-07-05 02:42:47,030 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:ConsumerCoordinator@626] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Finished assignment for group at generation 2: {appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2-consumer-30098832-1e73-483a-8e33-45205691fa67=Assignment(partitions=[inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0], userDataSize=48)}
2022-07-05 02:42:47,033 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover for generation 2. The group has 1 members, 0 of which are static.
2022-07-05 02:42:47,035 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:AbstractCoordinator$SyncGroupResponseHandler@756] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Successfully synced group in generation Generation{generationId=2, memberId='appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2-consumer-30098832-1e73-483a-8e33-45205691fa67', protocol='stream'}
2022-07-05 02:42:47,035 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:ConsumerCoordinator@392] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Updating assignment with
	Assigned partitions:                       [inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0]
	Current owned partitions:                  []
	Added partitions (assigned - owned):       [inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0]
	Revoked partitions (owned - assigned):     []

2022-07-05 02:42:47,035 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:ConsumerCoordinator@276] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Notifying assignor about the new Assignment(partitions=[inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0], userDataSize=48)
2022-07-05 02:42:47,035 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:StreamsPartitionAssignor@1306] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2-consumer] No followup rebalance was requested, resetting the rebalance schedule.
2022-07-05 02:42:47,036 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:TaskManager@254] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2] Handle new assignment with:
	New active tasks: [0_0]
	New standby tasks: []
	Existing active tasks: []
	Existing standby tasks: []
2022-07-05 02:42:47,036 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:ConsumerCoordinator@288] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Adding newly assigned partitions: inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0
2022-07-05 02:42:47,037 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:StreamThread@230] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2] State transition from STARTING to PARTITIONS_ASSIGNED
2022-07-05 02:42:47,037 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:KafkaStreams@321] - stream-client [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3] State transition from RUNNING to REBALANCING
2022-07-05 02:42:47,038 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:ConsumerCoordinator$OffsetFetchResponseHandler@1352] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Found no committed offset for partition inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0
2022-07-05 02:42:47,041 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:SubscriptionState@398] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Resetting offset for partition inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35723 (id: 0 rack: null)], epoch=0}}.
2022-07-05 02:42:47,090 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:RocksDBTimestampedStore@100] - Opening store test-store in regular mode
2022-07-05 02:42:47,090 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:ProcessorStateManager@256] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2] task [0_0] State store test-store did not find checkpoint offset, hence would default to the starting offset at changelog appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0
2022-07-05 02:42:47,090 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:StreamTask@235] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2] task [0_0] Initialized
2022-07-05 02:42:47,093 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:KafkaConsumer@1123] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2-restore-consumer, groupId=null] Subscribed to partition(s): appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0
2022-07-05 02:42:47,093 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:SubscriptionState@619] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2-restore-consumer, groupId=null] Seeking to EARLIEST offset of partition appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0
2022-07-05 02:42:47,096 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:Metadata@279] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2-restore-consumer, groupId=null] Cluster ID: kaU--u7iRi2Y8VqvcjcEiA
2022-07-05 02:42:47,099 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:SubscriptionState@398] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2-restore-consumer, groupId=null] Resetting offset for partition appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35723 (id: 0 rack: null)], epoch=0}}.
2022-07-05 02:42:47,106 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:StoreChangelogReader@609] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2] Finished restoring changelog appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0 to store test-store with a total number of 1 records
2022-07-05 02:42:47,107 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:ConsumerCoordinator$OffsetFetchResponseHandler@1352] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Found no committed offset for partition inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0
2022-07-05 02:42:47,107 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:StreamTask@255] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2] task [0_0] Restored and ready to run
2022-07-05 02:42:47,107 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:StreamThread@853] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2] Restoration took 70 ms for all tasks [0_0]
2022-07-05 02:42:47,107 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:StreamThread@230] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2] State transition from PARTITIONS_ASSIGNED to RUNNING
2022-07-05 02:42:47,107 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:KafkaStreams@321] - stream-client [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3] State transition from REBALANCING to RUNNING
2022-07-05 02:43:05,461 - INFO  [kafka-scheduler-5:Logging@66] - [ProducerStateManager partition=outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0] Writing producer snapshot at offset 1
2022-07-05 02:43:05,469 - INFO  [kafka-scheduler-5:Logging@66] - [Log partition=outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0, dir=/tmp/junit6652325244492370660/junit1604483620797807122] Rolled new log segment at offset 1 in 0 ms.
2022-07-05 02:43:05,469 - INFO  [kafka-scheduler-5:Logging@66] - [Log partition=outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0, dir=/tmp/junit6652325244492370660/junit1604483620797807122] Deleting segment LogSegment(baseOffset=0, size=73, lastModifiedTime=1657003367000, largestRecordTimestamp=Some(0)) due to retention time 604800000ms breach based on the largest record timestamp in the segment
2022-07-05 02:43:05,471 - INFO  [kafka-scheduler-5:Logging@66] - [Log partition=outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0, dir=/tmp/junit6652325244492370660/junit1604483620797807122] Incremented log start offset to 1 due to segment deletion
2022-07-05 02:43:05,477 - INFO  [kafka-scheduler-5:Logging@66] - [ProducerStateManager partition=inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0] Writing producer snapshot at offset 2
2022-07-05 02:43:05,485 - INFO  [kafka-scheduler-5:Logging@66] - [Log partition=inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0, dir=/tmp/junit6652325244492370660/junit1604483620797807122] Rolled new log segment at offset 2 in 0 ms.
2022-07-05 02:43:05,485 - INFO  [kafka-scheduler-5:Logging@66] - [Log partition=inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0, dir=/tmp/junit6652325244492370660/junit1604483620797807122] Deleting segment LogSegment(baseOffset=0, size=85, lastModifiedTime=1657003357000, largestRecordTimestamp=Some(0)) due to retention time 604800000ms breach based on the largest record timestamp in the segment
2022-07-05 02:43:05,485 - INFO  [kafka-scheduler-5:Logging@66] - [Log partition=inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0, dir=/tmp/junit6652325244492370660/junit1604483620797807122] Incremented log start offset to 2 due to segment deletion
2022-07-05 02:43:37,658 - INFO  [main:ConsumerCoordinator@307] - [Consumer clientId=consumer-e0579a4c-8286-4010-8b07-55f39759a0c8-1, groupId=e0579a4c-8286-4010-8b07-55f39759a0c8] Revoke previously assigned partitions outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0
2022-07-05 02:43:37,659 - INFO  [main:AbstractCoordinator@1038] - [Consumer clientId=consumer-e0579a4c-8286-4010-8b07-55f39759a0c8-1, groupId=e0579a4c-8286-4010-8b07-55f39759a0c8] Member consumer-e0579a4c-8286-4010-8b07-55f39759a0c8-1-0c529527-743a-4077-9b60-653929f7f7eb sending LeaveGroup request to coordinator localhost:35723 (id: 2147483647 rack: null) due to the consumer is being closed
2022-07-05 02:43:37,659 - INFO  [main:AbstractCoordinator@961] - [Consumer clientId=consumer-e0579a4c-8286-4010-8b07-55f39759a0c8-1, groupId=e0579a4c-8286-4010-8b07-55f39759a0c8] Resetting generation due to: consumer pro-actively leaving the group
2022-07-05 02:43:37,659 - INFO  [main:AbstractCoordinator@982] - [Consumer clientId=consumer-e0579a4c-8286-4010-8b07-55f39759a0c8-1, groupId=e0579a4c-8286-4010-8b07-55f39759a0c8] Request joining group due to: consumer pro-actively leaving the group
2022-07-05 02:43:37,662 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group e0579a4c-8286-4010-8b07-55f39759a0c8 in state PreparingRebalance with old generation 1 (__consumer_offsets-0) (reason: removing member consumer-e0579a4c-8286-4010-8b07-55f39759a0c8-1-0c529527-743a-4077-9b60-653929f7f7eb on LeaveGroup)
2022-07-05 02:43:37,663 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [GroupCoordinator 0]: Group e0579a4c-8286-4010-8b07-55f39759a0c8 with generation 2 is now empty (__consumer_offsets-0)
2022-07-05 02:43:37,665 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [GroupCoordinator 0]: Member MemberMetadata(memberId=consumer-e0579a4c-8286-4010-8b07-55f39759a0c8-1-0c529527-743a-4077-9b60-653929f7f7eb, groupInstanceId=None, clientId=consumer-e0579a4c-8286-4010-8b07-55f39759a0c8-1, clientHost=/127.0.0.1, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(range)) has left group e0579a4c-8286-4010-8b07-55f39759a0c8 through explicit `LeaveGroup` request
2022-07-05 02:43:37,668 - INFO  [main:Metrics@659] - Metrics scheduler closed
2022-07-05 02:43:37,668 - INFO  [main:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:43:37,668 - INFO  [main:Metrics@669] - Metrics reporters closed
2022-07-05 02:43:37,669 - INFO  [main:AppInfoParser@83] - App info kafka.consumer for consumer-e0579a4c-8286-4010-8b07-55f39759a0c8-1 unregistered
2022-07-05 02:43:37,669 - INFO  [main:KafkaStreams@321] - stream-client [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3] State transition from RUNNING to PENDING_SHUTDOWN
2022-07-05 02:43:37,670 - INFO  [kafka-streams-close-thread:StreamThread@1057] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-1] Informed to shut down
2022-07-05 02:43:37,670 - INFO  [kafka-streams-close-thread:StreamThread@1057] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2] Informed to shut down
2022-07-05 02:43:37,670 - INFO  [kafka-streams-close-thread:StreamThread@230] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2] State transition from RUNNING to PENDING_SHUTDOWN
2022-07-05 02:43:37,756 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:StreamThread@729] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2] Thread state is already PENDING_SHUTDOWN, skipping the run once call after poll request
2022-07-05 02:43:37,756 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:StreamThread@1071] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2] Shutting down
2022-07-05 02:43:37,778 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:StreamTask@1144] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2] task [0_0] Suspended RUNNING
2022-07-05 02:43:37,778 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:StreamTask@290] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2] task [0_0] Suspended running
2022-07-05 02:43:37,779 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:KafkaConsumer@1077] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions
2022-07-05 02:43:37,780 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:RecordCollectorImpl@268] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2] task [0_0] Closing record collector clean
2022-07-05 02:43:37,781 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:StreamTask@508] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2] task [0_0] Closed clean
2022-07-05 02:43:37,781 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:KafkaProducer@1204] - [Producer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2-producer] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2022-07-05 02:43:37,782 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:Metrics@659] - Metrics scheduler closed
2022-07-05 02:43:37,782 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:43:37,782 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:Metrics@669] - Metrics reporters closed
2022-07-05 02:43:37,783 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:AppInfoParser@83] - App info kafka.producer for appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2-producer unregistered
2022-07-05 02:43:37,783 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:KafkaConsumer@1077] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions
2022-07-05 02:43:37,783 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:Metrics@659] - Metrics scheduler closed
2022-07-05 02:43:37,783 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:43:37,783 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:Metrics@669] - Metrics reporters closed
2022-07-05 02:43:37,785 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:AppInfoParser@83] - App info kafka.consumer for appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2-consumer unregistered
2022-07-05 02:43:37,785 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:Metrics@659] - Metrics scheduler closed
2022-07-05 02:43:37,785 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:43:37,785 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:Metrics@669] - Metrics reporters closed
2022-07-05 02:43:37,787 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:AppInfoParser@83] - App info kafka.consumer for appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2-restore-consumer unregistered
2022-07-05 02:43:37,787 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:StreamThread@230] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2] State transition from PENDING_SHUTDOWN to DEAD
2022-07-05 02:43:37,787 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2:StreamThread@1100] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-StreamThread-2] Shutdown complete
2022-07-05 02:43:37,787 - ERROR [kafka-streams-close-thread:StateDirectory@414] - Some task directories still locked while closing state, this indicates unclean shutdown: {}
2022-07-05 02:43:37,788 - INFO  [kafka-admin-client-thread | appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-admin:AppInfoParser@83] - App info kafka.admin.client for appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-admin unregistered
2022-07-05 02:43:37,789 - INFO  [kafka-admin-client-thread | appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-admin:Metrics@659] - Metrics scheduler closed
2022-07-05 02:43:37,789 - INFO  [kafka-admin-client-thread | appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-admin:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:43:37,789 - INFO  [kafka-admin-client-thread | appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3-admin:Metrics@669] - Metrics reporters closed
2022-07-05 02:43:37,789 - INFO  [kafka-streams-close-thread:Metrics@659] - Metrics scheduler closed
2022-07-05 02:43:37,789 - INFO  [kafka-streams-close-thread:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:43:37,789 - INFO  [kafka-streams-close-thread:Metrics@669] - Metrics reporters closed
2022-07-05 02:43:37,789 - INFO  [kafka-streams-close-thread:KafkaStreams@321] - stream-client [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3] State transition from PENDING_SHUTDOWN to NOT_RUNNING
2022-07-05 02:43:37,789 - INFO  [main:KafkaStreams@1367] - stream-client [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-bc03ea36-31db-4df8-a3d8-3ba77abb2da3] Streams client stopped completely
2022-07-05 02:43:37,794 - INFO  [main:Logging@66] - [KafkaServer id=0] shutting down
2022-07-05 02:43:37,795 - INFO  [main:Logging@66] - [KafkaServer id=0] Starting controlled shutdown
2022-07-05 02:43:37,805 - INFO  [main:Logging@66] - [KafkaServer id=0] Controlled shutdown succeeded
2022-07-05 02:43:37,806 - INFO  [main:Logging@66] - [/config/changes-event-process-thread]: Shutting down
2022-07-05 02:43:37,807 - INFO  [main:Logging@66] - [/config/changes-event-process-thread]: Shutdown completed
2022-07-05 02:43:37,807 - INFO  [/config/changes-event-process-thread:Logging@66] - [/config/changes-event-process-thread]: Stopped
2022-07-05 02:43:37,807 - INFO  [main:Logging@66] - [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopping socket server request processors
2022-07-05 02:43:37,811 - INFO  [main:Logging@66] - [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopped socket server request processors
2022-07-05 02:43:37,812 - INFO  [main:Logging@66] - [data-plane Kafka Request Handler on Broker 0], shutting down
2022-07-05 02:43:37,813 - INFO  [main:Logging@66] - [data-plane Kafka Request Handler on Broker 0], shut down completely
2022-07-05 02:43:37,814 - INFO  [main:Logging@66] - [ExpirationReaper-0-AlterAcls]: Shutting down
2022-07-05 02:43:37,836 - INFO  [ExpirationReaper-0-AlterAcls:Logging@66] - [ExpirationReaper-0-AlterAcls]: Stopped
2022-07-05 02:43:37,836 - INFO  [main:Logging@66] - [ExpirationReaper-0-AlterAcls]: Shutdown completed
2022-07-05 02:43:37,836 - INFO  [main:Logging@66] - [KafkaApi-0] Shutdown complete.
2022-07-05 02:43:37,837 - INFO  [main:Logging@66] - [ExpirationReaper-0-topic]: Shutting down
2022-07-05 02:43:37,851 - INFO  [ExpirationReaper-0-topic:Logging@66] - [ExpirationReaper-0-topic]: Stopped
2022-07-05 02:43:37,851 - INFO  [main:Logging@66] - [ExpirationReaper-0-topic]: Shutdown completed
2022-07-05 02:43:37,852 - INFO  [main:Logging@66] - [TransactionCoordinator id=0] Shutting down.
2022-07-05 02:43:37,853 - INFO  [main:Logging@66] - [ProducerId Manager 0]: Shutdown complete: last producerId assigned 0
2022-07-05 02:43:37,853 - INFO  [main:Logging@66] - [Transaction State Manager 0]: Shutdown complete
2022-07-05 02:43:37,853 - INFO  [main:Logging@66] - [Transaction Marker Channel Manager 0]: Shutting down
2022-07-05 02:43:37,854 - INFO  [TxnMarkerSenderThread-0:Logging@66] - [Transaction Marker Channel Manager 0]: Stopped
2022-07-05 02:43:37,854 - INFO  [main:Logging@66] - [Transaction Marker Channel Manager 0]: Shutdown completed
2022-07-05 02:43:37,854 - INFO  [main:Logging@66] - [TransactionCoordinator id=0] Shutdown complete.
2022-07-05 02:43:37,855 - INFO  [main:Logging@66] - [GroupCoordinator 0]: Shutting down.
2022-07-05 02:43:37,855 - INFO  [main:Logging@66] - [ExpirationReaper-0-Heartbeat]: Shutting down
2022-07-05 02:43:38,030 - INFO  [ExpirationReaper-0-Heartbeat:Logging@66] - [ExpirationReaper-0-Heartbeat]: Stopped
2022-07-05 02:43:38,030 - INFO  [main:Logging@66] - [ExpirationReaper-0-Heartbeat]: Shutdown completed
2022-07-05 02:43:38,031 - INFO  [main:Logging@66] - [ExpirationReaper-0-Rebalance]: Shutting down
2022-07-05 02:43:38,038 - INFO  [ExpirationReaper-0-Rebalance:Logging@66] - [ExpirationReaper-0-Rebalance]: Stopped
2022-07-05 02:43:38,038 - INFO  [main:Logging@66] - [ExpirationReaper-0-Rebalance]: Shutdown completed
2022-07-05 02:43:38,039 - INFO  [main:Logging@66] - [GroupCoordinator 0]: Shutdown complete.
2022-07-05 02:43:38,039 - INFO  [main:Logging@66] - [ReplicaManager broker=0] Shutting down
2022-07-05 02:43:38,040 - INFO  [main:Logging@66] - [LogDirFailureHandler]: Shutting down
2022-07-05 02:43:38,040 - INFO  [main:Logging@66] - [LogDirFailureHandler]: Shutdown completed
2022-07-05 02:43:38,040 - INFO  [LogDirFailureHandler:Logging@66] - [LogDirFailureHandler]: Stopped
2022-07-05 02:43:38,040 - INFO  [main:Logging@66] - [ReplicaFetcherManager on broker 0] shutting down
2022-07-05 02:43:38,041 - INFO  [main:Logging@66] - [ReplicaFetcherManager on broker 0] shutdown completed
2022-07-05 02:43:38,041 - INFO  [main:Logging@66] - [ReplicaAlterLogDirsManager on broker 0] shutting down
2022-07-05 02:43:38,042 - INFO  [main:Logging@66] - [ReplicaAlterLogDirsManager on broker 0] shutdown completed
2022-07-05 02:43:38,042 - INFO  [main:Logging@66] - [ExpirationReaper-0-Fetch]: Shutting down
2022-07-05 02:43:38,226 - INFO  [ExpirationReaper-0-Fetch:Logging@66] - [ExpirationReaper-0-Fetch]: Stopped
2022-07-05 02:43:38,226 - INFO  [main:Logging@66] - [ExpirationReaper-0-Fetch]: Shutdown completed
2022-07-05 02:43:38,226 - INFO  [main:Logging@66] - [ExpirationReaper-0-Produce]: Shutting down
2022-07-05 02:43:38,236 - INFO  [ExpirationReaper-0-Produce:Logging@66] - [ExpirationReaper-0-Produce]: Stopped
2022-07-05 02:43:38,236 - INFO  [main:Logging@66] - [ExpirationReaper-0-Produce]: Shutdown completed
2022-07-05 02:43:38,236 - INFO  [main:Logging@66] - [ExpirationReaper-0-DeleteRecords]: Shutting down
2022-07-05 02:43:38,436 - INFO  [ExpirationReaper-0-DeleteRecords:Logging@66] - [ExpirationReaper-0-DeleteRecords]: Stopped
2022-07-05 02:43:38,436 - INFO  [main:Logging@66] - [ExpirationReaper-0-DeleteRecords]: Shutdown completed
2022-07-05 02:43:38,437 - INFO  [main:Logging@66] - [ExpirationReaper-0-ElectLeader]: Shutting down
2022-07-05 02:43:38,636 - INFO  [ExpirationReaper-0-ElectLeader:Logging@66] - [ExpirationReaper-0-ElectLeader]: Stopped
2022-07-05 02:43:38,636 - INFO  [main:Logging@66] - [ExpirationReaper-0-ElectLeader]: Shutdown completed
2022-07-05 02:43:38,638 - INFO  [main:Logging@66] - [ReplicaManager broker=0] Shut down completely
2022-07-05 02:43:38,638 - INFO  [main:Logging@66] - [BrokerToControllerChannelManager broker=0 name=alterIsr]: Shutting down
2022-07-05 02:43:38,639 - INFO  [main:Logging@66] - [BrokerToControllerChannelManager broker=0 name=alterIsr]: Shutdown completed
2022-07-05 02:43:38,639 - INFO  [BrokerToControllerChannelManager broker=0 name=alterIsr:Logging@66] - [BrokerToControllerChannelManager broker=0 name=alterIsr]: Stopped
2022-07-05 02:43:38,639 - INFO  [main:Logging@66] - Broker to controller channel manager for alterIsr shutdown
2022-07-05 02:43:38,640 - INFO  [main:Logging@66] - Shutting down.
2022-07-05 02:43:38,668 - INFO  [pool-8-thread-1:Logging@66] - [ProducerStateManager partition=__consumer_offsets-0] Writing producer snapshot at offset 18
2022-07-05 02:43:38,678 - INFO  [pool-8-thread-1:Logging@66] - [ProducerStateManager partition=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0] Writing producer snapshot at offset 2
2022-07-05 02:43:38,703 - INFO  [main:Logging@66] - Shutdown complete.
2022-07-05 02:43:38,710 - INFO  [main:Logging@66] - [feature-zk-node-event-process-thread]: Shutting down
2022-07-05 02:43:38,710 - INFO  [feature-zk-node-event-process-thread:Logging@66] - [feature-zk-node-event-process-thread]: Stopped
2022-07-05 02:43:38,710 - INFO  [main:Logging@66] - [feature-zk-node-event-process-thread]: Shutdown completed
2022-07-05 02:43:38,711 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Closing.
2022-07-05 02:43:38,814 - INFO  [main:ZooKeeper@1422] - Session: 0x100a45eb35f0000 closed
2022-07-05 02:43:38,815 - INFO  [main-EventThread:ClientCnxn$EventThread@524] - EventThread shut down for session: 0x100a45eb35f0000
2022-07-05 02:43:38,815 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Closed.
2022-07-05 02:43:38,816 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Fetch]: Shutting down
2022-07-05 02:43:39,431 - INFO  [ThrottledChannelReaper-Fetch:Logging@66] - [ThrottledChannelReaper-Fetch]: Stopped
2022-07-05 02:43:39,431 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Fetch]: Shutdown completed
2022-07-05 02:43:39,432 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Produce]: Shutting down
2022-07-05 02:43:40,431 - INFO  [ThrottledChannelReaper-Produce:Logging@66] - [ThrottledChannelReaper-Produce]: Stopped
2022-07-05 02:43:40,431 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Produce]: Shutdown completed
2022-07-05 02:43:40,432 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Request]: Shutting down
2022-07-05 02:43:41,431 - INFO  [ThrottledChannelReaper-Request:Logging@66] - [ThrottledChannelReaper-Request]: Stopped
2022-07-05 02:43:41,432 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Request]: Shutdown completed
2022-07-05 02:43:41,432 - INFO  [main:Logging@66] - [ThrottledChannelReaper-ControllerMutation]: Shutting down
2022-07-05 02:43:42,432 - INFO  [ThrottledChannelReaper-ControllerMutation:Logging@66] - [ThrottledChannelReaper-ControllerMutation]: Stopped
2022-07-05 02:43:42,432 - INFO  [main:Logging@66] - [ThrottledChannelReaper-ControllerMutation]: Shutdown completed
2022-07-05 02:43:42,433 - INFO  [main:Logging@66] - [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutting down socket server
2022-07-05 02:43:42,454 - INFO  [main:Logging@66] - [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutdown completed
2022-07-05 02:43:42,454 - INFO  [main:Metrics@659] - Metrics scheduler closed
2022-07-05 02:43:42,455 - INFO  [main:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:43:42,455 - INFO  [main:Metrics@669] - Metrics reporters closed
2022-07-05 02:43:42,456 - INFO  [main:Logging@66] - Broker and topic stats closed
2022-07-05 02:43:42,457 - INFO  [main:AppInfoParser@83] - App info kafka.server for 0 unregistered
2022-07-05 02:43:42,457 - INFO  [main:Logging@66] - [KafkaServer id=0] shut down completed
2022-07-05 02:43:42,461 - INFO  [ConnnectionExpirer:NIOServerCnxnFactory$ConnectionExpirerThread@583] - ConnnectionExpirerThread interrupted
2022-07-05 02:43:42,462 - INFO  [NIOServerCxnFactory.AcceptThread:/127.0.0.1:0:NIOServerCnxnFactory$AcceptThread@219] - accept thread exitted run method
2022-07-05 02:43:42,462 - INFO  [NIOServerCxnFactory.SelectorThread-1:NIOServerCnxnFactory$SelectorThread@420] - selector thread exitted run method
2022-07-05 02:43:42,462 - INFO  [NIOServerCxnFactory.SelectorThread-2:NIOServerCnxnFactory$SelectorThread@420] - selector thread exitted run method
2022-07-05 02:43:42,463 - INFO  [NIOServerCxnFactory.SelectorThread-0:NIOServerCnxnFactory$SelectorThread@420] - selector thread exitted run method
2022-07-05 02:43:42,463 - INFO  [main:ZooKeeperServer@573] - shutting down
2022-07-05 02:43:42,463 - INFO  [main:SessionTrackerImpl@237] - Shutting down
2022-07-05 02:43:42,463 - INFO  [main:PrepRequestProcessor@1008] - Shutting down
2022-07-05 02:43:42,463 - INFO  [main:SyncRequestProcessor@191] - Shutting down
2022-07-05 02:43:42,463 - INFO  [ProcessThread(sid:0 cport:33589)::PrepRequestProcessor@156] - PrepRequestProcessor exited loop!
2022-07-05 02:43:42,463 - INFO  [SyncThread:0:SyncRequestProcessor@169] - SyncRequestProcessor exited!
2022-07-05 02:43:42,464 - INFO  [main:FinalRequestProcessor@514] - shutdown of request processor complete
2022-07-05 02:47:14,039 - INFO  [main:Log4jControllerRegistration$@31] - Registered kafka:type=kafka.Log4jController MBean
2022-07-05 02:47:14,087 - INFO  [main:Environment@109] - Server environment:zookeeper.version=3.5.9-83df9301aa5c2a5d284a9940177808c01bc35cef, built on 01/06/2021 20:03 GMT
2022-07-05 02:47:14,087 - INFO  [main:Environment@109] - Server environment:host.name=razor15
2022-07-05 02:47:14,087 - INFO  [main:Environment@109] - Server environment:java.version=1.8.0_275
2022-07-05 02:47:14,088 - INFO  [main:Environment@109] - Server environment:java.vendor=Private Build
2022-07-05 02:47:14,088 - INFO  [main:Environment@109] - Server environment:java.home=/usr/lib/jvm/java-8-openjdk-amd64/jre
2022-07-05 02:47:14,088 - INFO  [main:Environment@109] - Server environment:java.class.path=/home/tonypan/flaky-reproduction/experiment/kafka-12508/junit-platform-console-standalone-1.7.0.jar
2022-07-05 02:47:14,088 - INFO  [main:Environment@109] - Server environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib
2022-07-05 02:47:14,088 - INFO  [main:Environment@109] - Server environment:java.io.tmpdir=/tmp
2022-07-05 02:47:14,088 - INFO  [main:Environment@109] - Server environment:java.compiler=<NA>
2022-07-05 02:47:14,088 - INFO  [main:Environment@109] - Server environment:os.name=Linux
2022-07-05 02:47:14,088 - INFO  [main:Environment@109] - Server environment:os.arch=amd64
2022-07-05 02:47:14,088 - INFO  [main:Environment@109] - Server environment:os.version=4.15.0-128-generic
2022-07-05 02:47:14,089 - INFO  [main:Environment@109] - Server environment:user.name=tonypan
2022-07-05 02:47:14,089 - INFO  [main:Environment@109] - Server environment:user.home=/home/tonypan
2022-07-05 02:47:14,089 - INFO  [main:Environment@109] - Server environment:user.dir=/home/tonypan/flaky-reproduction/experiment/kafka-12508
2022-07-05 02:47:14,090 - INFO  [main:Environment@109] - Server environment:os.memory.free=400MB
2022-07-05 02:47:14,090 - INFO  [main:Environment@109] - Server environment:os.memory.max=7051MB
2022-07-05 02:47:14,090 - INFO  [main:Environment@109] - Server environment:os.memory.total=475MB
2022-07-05 02:47:14,095 - INFO  [main:FileTxnSnapLog@115] - zookeeper.snapshot.trust.empty : false
2022-07-05 02:47:14,121 - INFO  [main:ZKDatabase@117] - zookeeper.snapshotSizeFactor = 0.33
2022-07-05 02:47:14,128 - INFO  [main:ZooKeeperServer@953] - minSessionTimeout set to 1600
2022-07-05 02:47:14,128 - INFO  [main:ZooKeeperServer@962] - maxSessionTimeout set to 16000
2022-07-05 02:47:14,129 - INFO  [main:ZooKeeperServer@181] - Created server with tickTime 800 minSessionTimeout 1600 maxSessionTimeout 16000 datadir /tmp/kafka-615264495762727543/version-2 snapdir /tmp/kafka-7316225976286460174/version-2
2022-07-05 02:47:14,141 - INFO  [main:NIOServerCnxnFactory@673] - Configuring NIO connection handler with 10s sessionless connection timeout, 3 selector thread(s), 40 worker threads, and 64 kB direct buffers.
2022-07-05 02:47:14,147 - INFO  [main:NIOServerCnxnFactory@686] - binding to port /127.0.0.1:0
2022-07-05 02:47:14,157 - INFO  [main:FileTxnSnapLog@404] - Snapshotting: 0x0 to /tmp/kafka-7316225976286460174/version-2/snapshot.0
2022-07-05 02:47:14,161 - INFO  [main:FileTxnSnapLog@404] - Snapshotting: 0x0 to /tmp/kafka-7316225976286460174/version-2/snapshot.0
2022-07-05 02:47:14,187 - INFO  [ProcessThread(sid:0 cport:37995)::PrepRequestProcessor@132] - PrepRequestProcessor (sid:0) started, reconfigEnabled=false
2022-07-05 02:47:14,521 - INFO  [main:AbstractConfig@372] - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 0
	host.name = localhost
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.8-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/junit2884833929294850451/junit3642864754825928179
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.8-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000000
	metadata.log.dir = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = -1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 5
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:37995
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 10000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

2022-07-05 02:47:14,542 - INFO  [main:X509Util@79] - Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation
2022-07-05 02:47:14,632 - INFO  [main:Logging@66] - starting
2022-07-05 02:47:14,633 - INFO  [main:Logging@66] - Connecting to zookeeper on 127.0.0.1:37995
2022-07-05 02:47:14,655 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:37995.
2022-07-05 02:47:14,662 - INFO  [main:Environment@109] - Client environment:zookeeper.version=3.5.9-83df9301aa5c2a5d284a9940177808c01bc35cef, built on 01/06/2021 20:03 GMT
2022-07-05 02:47:14,662 - INFO  [main:Environment@109] - Client environment:host.name=razor15
2022-07-05 02:47:14,662 - INFO  [main:Environment@109] - Client environment:java.version=1.8.0_275
2022-07-05 02:47:14,662 - INFO  [main:Environment@109] - Client environment:java.vendor=Private Build
2022-07-05 02:47:14,662 - INFO  [main:Environment@109] - Client environment:java.home=/usr/lib/jvm/java-8-openjdk-amd64/jre
2022-07-05 02:47:14,663 - INFO  [main:Environment@109] - Client environment:java.class.path=/home/tonypan/flaky-reproduction/experiment/kafka-12508/junit-platform-console-standalone-1.7.0.jar
2022-07-05 02:47:14,663 - INFO  [main:Environment@109] - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib
2022-07-05 02:47:14,663 - INFO  [main:Environment@109] - Client environment:java.io.tmpdir=/tmp
2022-07-05 02:47:14,663 - INFO  [main:Environment@109] - Client environment:java.compiler=<NA>
2022-07-05 02:47:14,663 - INFO  [main:Environment@109] - Client environment:os.name=Linux
2022-07-05 02:47:14,663 - INFO  [main:Environment@109] - Client environment:os.arch=amd64
2022-07-05 02:47:14,663 - INFO  [main:Environment@109] - Client environment:os.version=4.15.0-128-generic
2022-07-05 02:47:14,663 - INFO  [main:Environment@109] - Client environment:user.name=tonypan
2022-07-05 02:47:14,663 - INFO  [main:Environment@109] - Client environment:user.home=/home/tonypan
2022-07-05 02:47:14,663 - INFO  [main:Environment@109] - Client environment:user.dir=/home/tonypan/flaky-reproduction/experiment/kafka-12508
2022-07-05 02:47:14,664 - INFO  [main:Environment@109] - Client environment:os.memory.free=334MB
2022-07-05 02:47:14,664 - INFO  [main:Environment@109] - Client environment:os.memory.max=7051MB
2022-07-05 02:47:14,664 - INFO  [main:Environment@109] - Client environment:os.memory.total=441MB
2022-07-05 02:47:14,669 - INFO  [main:ZooKeeper@868] - Initiating client connection, connectString=127.0.0.1:37995 sessionTimeout=10000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@31add175
2022-07-05 02:47:14,673 - INFO  [main:ClientCnxnSocket@237] - jute.maxbuffer value is 4194304 Bytes
2022-07-05 02:47:14,681 - INFO  [main:ClientCnxn@1653] - zookeeper.request.timeout value is 0. feature enabled=
2022-07-05 02:47:14,682 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Waiting until connected.
2022-07-05 02:47:14,688 - INFO  [main-SendThread(127.0.0.1:37995):ClientCnxn$SendThread@1112] - Opening socket connection to server localhost/127.0.0.1:37995. Will not attempt to authenticate using SASL (unknown error)
2022-07-05 02:47:14,690 - INFO  [main-SendThread(127.0.0.1:37995):ClientCnxn$SendThread@959] - Socket connection established, initiating session, client: /127.0.0.1:37370, server: localhost/127.0.0.1:37995
2022-07-05 02:47:14,700 - INFO  [SyncThread:0:FileTxnLog@218] - Creating new log file: log.1
2022-07-05 02:47:14,710 - INFO  [main-SendThread(127.0.0.1:37995):ClientCnxn$SendThread@1394] - Session establishment complete on server localhost/127.0.0.1:37995, sessionid = 0x100a462f8620000, negotiated timeout = 10000
2022-07-05 02:47:14,716 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Connected.
2022-07-05 02:47:14,830 - INFO  [feature-zk-node-event-process-thread:Logging@66] - [feature-zk-node-event-process-thread]: Starting
2022-07-05 02:47:14,843 - INFO  [feature-zk-node-event-process-thread:Logging@66] - Feature ZK node at path: /feature does not exist
2022-07-05 02:47:14,843 - INFO  [feature-zk-node-event-process-thread:Logging@66] - Cleared cache
2022-07-05 02:47:15,117 - INFO  [main:Logging@66] - Cluster ID = NnPz9UrjRi2EtgdIfOXTaw
2022-07-05 02:47:15,120 - WARN  [main:Logging@70] - No meta.properties file under dir /tmp/junit2884833929294850451/junit3642864754825928179/meta.properties
2022-07-05 02:47:15,181 - INFO  [main:AbstractConfig@372] - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 0
	host.name = localhost
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.8-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/junit2884833929294850451/junit3642864754825928179
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.8-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000000
	metadata.log.dir = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = -1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 5
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:37995
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 10000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

2022-07-05 02:47:15,193 - INFO  [main:AbstractConfig@372] - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 0
	host.name = localhost
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.8-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/junit2884833929294850451/junit3642864754825928179
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.8-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000000
	metadata.log.dir = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = -1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 5
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:37995
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 10000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

2022-07-05 02:47:15,226 - INFO  [ThrottledChannelReaper-Fetch:Logging@66] - [ThrottledChannelReaper-Fetch]: Starting
2022-07-05 02:47:15,226 - INFO  [ThrottledChannelReaper-Produce:Logging@66] - [ThrottledChannelReaper-Produce]: Starting
2022-07-05 02:47:15,227 - INFO  [ThrottledChannelReaper-Request:Logging@66] - [ThrottledChannelReaper-Request]: Starting
2022-07-05 02:47:15,229 - INFO  [ThrottledChannelReaper-ControllerMutation:Logging@66] - [ThrottledChannelReaper-ControllerMutation]: Starting
2022-07-05 02:47:15,260 - INFO  [main:Logging@66] - Loading logs from log dirs ArraySeq(/tmp/junit2884833929294850451/junit3642864754825928179)
2022-07-05 02:47:15,263 - INFO  [main:Logging@66] - Attempting recovery for all logs in /tmp/junit2884833929294850451/junit3642864754825928179 since no clean shutdown file was found
2022-07-05 02:47:15,268 - INFO  [main:Logging@66] - Loaded 0 logs in 0ms.
2022-07-05 02:47:15,269 - INFO  [main:Logging@66] - Starting log cleanup with a period of 300000 ms.
2022-07-05 02:47:15,276 - INFO  [main:Logging@66] - Starting log flusher with a default period of 9223372036854775807 ms.
2022-07-05 02:47:15,652 - INFO  [main:Logging@66] - Updated connection-accept-rate max connection creation rate to 2147483647
2022-07-05 02:47:15,656 - INFO  [main:Logging@66] - Awaiting socket connections on localhost:39997.
2022-07-05 02:47:15,694 - INFO  [main:Logging@66] - [SocketServer listenerType=ZK_BROKER, nodeId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT)
2022-07-05 02:47:15,722 - INFO  [BrokerToControllerChannelManager broker=0 name=alterIsr:Logging@66] - [BrokerToControllerChannelManager broker=0 name=alterIsr]: Starting
2022-07-05 02:47:15,745 - INFO  [ExpirationReaper-0-Produce:Logging@66] - [ExpirationReaper-0-Produce]: Starting
2022-07-05 02:47:15,745 - INFO  [ExpirationReaper-0-Fetch:Logging@66] - [ExpirationReaper-0-Fetch]: Starting
2022-07-05 02:47:15,746 - INFO  [ExpirationReaper-0-DeleteRecords:Logging@66] - [ExpirationReaper-0-DeleteRecords]: Starting
2022-07-05 02:47:15,746 - INFO  [ExpirationReaper-0-ElectLeader:Logging@66] - [ExpirationReaper-0-ElectLeader]: Starting
2022-07-05 02:47:15,759 - INFO  [LogDirFailureHandler:Logging@66] - [LogDirFailureHandler]: Starting
2022-07-05 02:47:15,799 - INFO  [main:Logging@66] - Creating /brokers/ids/0 (is it secure? false)
2022-07-05 02:47:15,825 - INFO  [main:Logging@66] - Stat of the created znode at /brokers/ids/0 is: 25,25,1657003635818,1657003635818,1,0,0,72238339018850304,204,0,25

2022-07-05 02:47:15,826 - INFO  [main:Logging@66] - Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://localhost:39997, czxid (broker epoch): 25
2022-07-05 02:47:15,890 - INFO  [ExpirationReaper-0-topic:Logging@66] - [ExpirationReaper-0-topic]: Starting
2022-07-05 02:47:15,896 - INFO  [ExpirationReaper-0-Heartbeat:Logging@66] - [ExpirationReaper-0-Heartbeat]: Starting
2022-07-05 02:47:15,896 - INFO  [ExpirationReaper-0-Rebalance:Logging@66] - [ExpirationReaper-0-Rebalance]: Starting
2022-07-05 02:47:15,898 - INFO  [controller-event-thread:Logging@66] - Successfully created /controller_epoch with initial epoch 0
2022-07-05 02:47:15,910 - INFO  [main-EventThread:Logging@66] - Feature ZK node created at path: /feature
2022-07-05 02:47:15,914 - INFO  [main:Logging@66] - [GroupCoordinator 0]: Starting up.
2022-07-05 02:47:15,919 - INFO  [main:Logging@66] - [GroupCoordinator 0]: Startup complete.
2022-07-05 02:47:15,950 - INFO  [main:Logging@66] - [ProducerId Manager 0]: Acquired new producerId block (brokerId:0,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1
2022-07-05 02:47:15,951 - INFO  [main:Logging@66] - [TransactionCoordinator id=0] Starting up.
2022-07-05 02:47:15,954 - INFO  [TxnMarkerSenderThread-0:Logging@66] - [Transaction Marker Channel Manager 0]: Starting
2022-07-05 02:47:15,955 - INFO  [main:Logging@66] - [TransactionCoordinator id=0] Startup complete.
2022-07-05 02:47:15,955 - INFO  [feature-zk-node-event-process-thread:Logging@66] - Updated cache from existing <empty> to latest FinalizedFeaturesAndEpoch(features=Features{}, epoch=0).
2022-07-05 02:47:15,979 - INFO  [ExpirationReaper-0-AlterAcls:Logging@66] - [ExpirationReaper-0-AlterAcls]: Starting
2022-07-05 02:47:16,002 - INFO  [/config/changes-event-process-thread:Logging@66] - [/config/changes-event-process-thread]: Starting
2022-07-05 02:47:16,008 - INFO  [main:Logging@66] - [SocketServer listenerType=ZK_BROKER, nodeId=0] Starting socket server acceptors and processors
2022-07-05 02:47:16,014 - INFO  [main:Logging@66] - [SocketServer listenerType=ZK_BROKER, nodeId=0] Started data-plane acceptor and processor(s) for endpoint : ListenerName(PLAINTEXT)
2022-07-05 02:47:16,014 - INFO  [main:Logging@66] - [SocketServer listenerType=ZK_BROKER, nodeId=0] Started socket server acceptors and processors
2022-07-05 02:47:16,015 - WARN  [main:AppInfoParser@46] - Error while loading kafka-version.properties: null
2022-07-05 02:47:16,016 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2022-07-05 02:47:16,016 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2022-07-05 02:47:16,016 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1657003633854
2022-07-05 02:47:16,017 - INFO  [main:Logging@66] - [KafkaServer id=0] started
2022-07-05 02:47:16,034 - INFO  [main:AbstractConfig@372] - AdminClientConfig values: 
	bootstrap.servers = [localhost:39997]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2022-07-05 02:47:16,056 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2022-07-05 02:47:16,057 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2022-07-05 02:47:16,057 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1657003636056
2022-07-05 02:47:16,112 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Creating topic inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0))
2022-07-05 02:47:16,132 - INFO  [BrokerToControllerChannelManager broker=0 name=alterIsr:Logging@66] - [BrokerToControllerChannelManager broker=0 name=alterIsr]: Recorded new controller, from now on will use broker localhost:39997 (id: 0 rack: null)
2022-07-05 02:47:16,182 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0)
2022-07-05 02:47:16,251 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0, dir=/tmp/junit2884833929294850451/junit3642864754825928179] Loading producer state till offset 0 with message format version 2
2022-07-05 02:47:16,255 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 in /tmp/junit2884833929294850451/junit3642864754825928179/inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.8-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2022-07-05 02:47:16,256 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 broker=0] No checkpointed highwatermark is found for partition inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0
2022-07-05 02:47:16,257 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 broker=0] Log loaded for partition inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 with initial high watermark 0
2022-07-05 02:47:16,284 - INFO  [kafka-admin-client-thread | adminclient-1:AppInfoParser@83] - App info kafka.admin.client for adminclient-1 unregistered
2022-07-05 02:47:16,286 - INFO  [kafka-admin-client-thread | adminclient-1:Metrics@659] - Metrics scheduler closed
2022-07-05 02:47:16,287 - INFO  [kafka-admin-client-thread | adminclient-1:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:47:16,287 - INFO  [kafka-admin-client-thread | adminclient-1:Metrics@669] - Metrics reporters closed
2022-07-05 02:47:16,289 - INFO  [main:AbstractConfig@372] - AdminClientConfig values: 
	bootstrap.servers = [localhost:39997]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2022-07-05 02:47:16,291 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2022-07-05 02:47:16,291 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2022-07-05 02:47:16,291 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1657003636291
2022-07-05 02:47:16,303 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Creating topic outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0))
2022-07-05 02:47:16,314 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0)
2022-07-05 02:47:16,316 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Log partition=outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0, dir=/tmp/junit2884833929294850451/junit3642864754825928179] Loading producer state till offset 0 with message format version 2
2022-07-05 02:47:16,317 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Created log for partition outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 in /tmp/junit2884833929294850451/junit3642864754825928179/outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.8-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2022-07-05 02:47:16,318 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 broker=0] No checkpointed highwatermark is found for partition outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0
2022-07-05 02:47:16,318 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 broker=0] Log loaded for partition outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 with initial high watermark 0
2022-07-05 02:47:16,322 - INFO  [kafka-admin-client-thread | adminclient-2:AppInfoParser@83] - App info kafka.admin.client for adminclient-2 unregistered
2022-07-05 02:47:16,323 - INFO  [kafka-admin-client-thread | adminclient-2:Metrics@659] - Metrics scheduler closed
2022-07-05 02:47:16,323 - INFO  [kafka-admin-client-thread | adminclient-2:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:47:16,323 - INFO  [kafka-admin-client-thread | adminclient-2:Metrics@669] - Metrics reporters closed
2022-07-05 02:47:16,377 - INFO  [main:AbstractConfig@372] - StreamsConfig values: 
	acceptable.recovery.lag = 10000
	application.id = appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover
	application.server = 
	bootstrap.servers = [localhost:39997]
	buffered.records.per.partition = 1000
	built.in.metrics.version = latest
	cache.max.bytes.buffering = 0
	client.id = 
	commit.interval.ms = 300000
	connections.max.idle.ms = 540000
	default.deserialization.exception.handler = class org.apache.kafka.streams.errors.LogAndFailExceptionHandler
	default.key.serde = class org.apache.kafka.common.serialization.Serdes$IntegerSerde
	default.production.exception.handler = class org.apache.kafka.streams.errors.DefaultProductionExceptionHandler
	default.timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	default.value.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	default.windowed.key.serde.inner = null
	default.windowed.value.serde.inner = null
	max.task.idle.ms = 0
	max.warmup.replicas = 2
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	probing.rebalance.interval.ms = 600000
	processing.guarantee = at_least_once
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retries = 0
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 600000
	state.dir = /tmp/kafka-2180998660974683200
	task.timeout.ms = 300000
	topology.optimization = none
	upgrade.from = null
	window.size.ms = null
	windowstore.changelog.additional.retention.ms = 86400000

2022-07-05 02:47:16,407 - WARN  [main:StateDirectory@138] - Using an OS temp directory in the state.dir property can cause failures with writing the checkpoint file due to the fact that this directory can be cleared by the OS. Resolved state.dir: [/tmp/kafka-2180998660974683200]
2022-07-05 02:47:16,408 - INFO  [main:StateDirectory@212] - No process id found on disk, got fresh process id 8225f260-9829-4e8d-bbba-eaa26babd6d9
2022-07-05 02:47:16,441 - INFO  [main:AbstractConfig@372] - AdminClientConfig values: 
	bootstrap.servers = [localhost:39997]
	client.dns.lookup = use_all_dns_ips
	client.id = appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-admin
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2022-07-05 02:47:16,443 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2022-07-05 02:47:16,443 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2022-07-05 02:47:16,443 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1657003636443
2022-07-05 02:47:16,447 - WARN  [main:ClientMetrics@55] - Error while loading kafka-streams-version.properties
java.lang.NullPointerException
	at java.util.Properties$LineReader.readLine(Properties.java:434)
	at java.util.Properties.load0(Properties.java:353)
	at java.util.Properties.load(Properties.java:341)
	at org.apache.kafka.streams.internals.metrics.ClientMetrics.<clinit>(ClientMetrics.java:53)
	at org.apache.kafka.streams.KafkaStreams.<init>(KafkaStreams.java:825)
	at org.apache.kafka.streams.KafkaStreams.<init>(KafkaStreams.java:781)
	at org.apache.kafka.streams.KafkaStreams.<init>(KafkaStreams.java:691)
	at org.apache.kafka.streams.integration.EmitOnChangeIntegrationTest.shouldEmitSameRecordAfterFailover(EmitOnChangeIntegrationTest.java:101)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
2022-07-05 02:47:16,450 - INFO  [main:KafkaStreams@825] - stream-client [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9] Kafka Streams version: unknown
2022-07-05 02:47:16,450 - INFO  [main:KafkaStreams@826] - stream-client [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9] Kafka Streams commit ID: unknown
2022-07-05 02:47:16,459 - INFO  [main:StreamThread@337] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1] Creating restore consumer client
2022-07-05 02:47:16,464 - INFO  [main:AbstractConfig@372] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [localhost:39997]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1-restore-consumer
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = false
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2022-07-05 02:47:16,484 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2022-07-05 02:47:16,484 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2022-07-05 02:47:16,484 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1657003636484
2022-07-05 02:47:16,490 - INFO  [main:ActiveTaskCreator@96] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1] Creating thread producer client
2022-07-05 02:47:16,495 - INFO  [main:AbstractConfig@372] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:39997]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-07-05 02:47:16,508 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2022-07-05 02:47:16,508 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2022-07-05 02:47:16,508 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1657003636507
2022-07-05 02:47:16,515 - INFO  [main:StreamThread@389] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1] Creating consumer client
2022-07-05 02:47:16,516 - INFO  [kafka-producer-network-thread | appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1-producer:Metadata@279] - [Producer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1-producer] Cluster ID: NnPz9UrjRi2EtgdIfOXTaw
2022-07-05 02:47:16,518 - INFO  [main:AbstractConfig@372] - ConsumerConfig values: 
	allow.auto.create.topics = false
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:39997]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1-consumer
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = false
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2022-07-05 02:47:16,528 - INFO  [main:AssignorConfiguration@114] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1-consumer] Cooperative rebalancing enabled now
2022-07-05 02:47:16,540 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2022-07-05 02:47:16,541 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2022-07-05 02:47:16,541 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1657003636540
2022-07-05 02:47:16,548 - INFO  [main:KafkaStreams@321] - stream-client [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9] State transition from CREATED to REBALANCING
2022-07-05 02:47:16,548 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:StreamThread@550] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1] Starting
2022-07-05 02:47:16,548 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:StreamThread@230] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1] State transition from CREATED to STARTING
2022-07-05 02:47:16,549 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:KafkaConsumer@968] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Subscribed to topic(s): inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover
2022-07-05 02:47:16,561 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:Metadata@279] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Cluster ID: NnPz9UrjRi2EtgdIfOXTaw
2022-07-05 02:47:16,563 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Creating topic __consumer_offsets with configuration {segment.bytes=104857600, cleanup.policy=compact, compression.type=producer} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0))
2022-07-05 02:47:16,582 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(__consumer_offsets-4, __consumer_offsets-3, __consumer_offsets-2, __consumer_offsets-0, __consumer_offsets-1)
2022-07-05 02:47:16,585 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-3, dir=/tmp/junit2884833929294850451/junit3642864754825928179] Loading producer state till offset 0 with message format version 2
2022-07-05 02:47:16,585 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-3 in /tmp/junit2884833929294850451/junit3642864754825928179/__consumer_offsets-3 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.8-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2022-07-05 02:47:16,586 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3
2022-07-05 02:47:16,586 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-3 broker=0] Log loaded for partition __consumer_offsets-3 with initial high watermark 0
2022-07-05 02:47:16,589 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-2, dir=/tmp/junit2884833929294850451/junit3642864754825928179] Loading producer state till offset 0 with message format version 2
2022-07-05 02:47:16,590 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-2 in /tmp/junit2884833929294850451/junit3642864754825928179/__consumer_offsets-2 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.8-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2022-07-05 02:47:16,590 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2
2022-07-05 02:47:16,590 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-2 broker=0] Log loaded for partition __consumer_offsets-2 with initial high watermark 0
2022-07-05 02:47:16,597 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-4, dir=/tmp/junit2884833929294850451/junit3642864754825928179] Loading producer state till offset 0 with message format version 2
2022-07-05 02:47:16,598 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-4 in /tmp/junit2884833929294850451/junit3642864754825928179/__consumer_offsets-4 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.8-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2022-07-05 02:47:16,598 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4
2022-07-05 02:47:16,598 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-4 broker=0] Log loaded for partition __consumer_offsets-4 with initial high watermark 0
2022-07-05 02:47:16,606 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-1, dir=/tmp/junit2884833929294850451/junit3642864754825928179] Loading producer state till offset 0 with message format version 2
2022-07-05 02:47:16,606 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-1 in /tmp/junit2884833929294850451/junit3642864754825928179/__consumer_offsets-1 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.8-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2022-07-05 02:47:16,606 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1
2022-07-05 02:47:16,606 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-1 broker=0] Log loaded for partition __consumer_offsets-1 with initial high watermark 0
2022-07-05 02:47:16,614 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-0, dir=/tmp/junit2884833929294850451/junit3642864754825928179] Loading producer state till offset 0 with message format version 2
2022-07-05 02:47:16,615 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-0 in /tmp/junit2884833929294850451/junit3642864754825928179/__consumer_offsets-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.8-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2022-07-05 02:47:16,615 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0
2022-07-05 02:47:16,615 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-0 broker=0] Log loaded for partition __consumer_offsets-0 with initial high watermark 0
2022-07-05 02:47:16,621 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Elected as the group coordinator for partition 3
2022-07-05 02:47:16,622 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3
2022-07-05 02:47:16,623 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Elected as the group coordinator for partition 2
2022-07-05 02:47:16,623 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2
2022-07-05 02:47:16,623 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Elected as the group coordinator for partition 4
2022-07-05 02:47:16,623 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4
2022-07-05 02:47:16,623 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Elected as the group coordinator for partition 1
2022-07-05 02:47:16,623 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1
2022-07-05 02:47:16,623 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Elected as the group coordinator for partition 0
2022-07-05 02:47:16,624 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0
2022-07-05 02:47:16,627 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 4 milliseconds, of which 1 milliseconds was spent in the scheduler.
2022-07-05 02:47:16,627 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 4 milliseconds, of which 4 milliseconds was spent in the scheduler.
2022-07-05 02:47:16,627 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 4 milliseconds, of which 4 milliseconds was spent in the scheduler.
2022-07-05 02:47:16,627 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 4 milliseconds, of which 4 milliseconds was spent in the scheduler.
2022-07-05 02:47:16,627 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 3 milliseconds, of which 3 milliseconds was spent in the scheduler.
2022-07-05 02:47:16,652 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:StreamThread@829] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update
2022-07-05 02:47:16,657 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:AbstractCoordinator$FindCoordinatorResponseHandler@844] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Discovered group coordinator localhost:39997 (id: 2147483647 rack: null)
2022-07-05 02:47:16,658 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:AbstractCoordinator@534] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] (Re-)joining group
2022-07-05 02:47:16,677 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [GroupCoordinator 0]: Dynamic member with unknown member id joins group appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover in Empty state. Created a new member id appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1-consumer-93d157f4-3eb9-4515-ab82-242dc6da29f8 and request the member to rejoin with this id.
2022-07-05 02:47:16,682 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:AbstractCoordinator@982] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Request joining group due to: need to re-join with the given member-id
2022-07-05 02:47:16,682 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:AbstractCoordinator@534] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] (Re-)joining group
2022-07-05 02:47:16,687 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover in state PreparingRebalance with old generation 0 (__consumer_offsets-0) (reason: Adding new member appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1-consumer-93d157f4-3eb9-4515-ab82-242dc6da29f8 with group instance id None)
2022-07-05 02:47:16,692 - INFO  [executor-Rebalance:Logging@66] - [GroupCoordinator 0]: Stabilized group appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover generation 1 (__consumer_offsets-0) with 1 members
2022-07-05 02:47:16,693 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Successfully joined group with generation Generation{generationId=1, memberId='appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1-consumer-93d157f4-3eb9-4515-ab82-242dc6da29f8', protocol='stream'}
2022-07-05 02:47:16,709 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Creating topic appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog with configuration {message.timestamp.type=CreateTime, cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0))
2022-07-05 02:47:16,719 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0)
2022-07-05 02:47:16,722 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Log partition=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0, dir=/tmp/junit2884833929294850451/junit3642864754825928179] Loading producer state till offset 0 with message format version 2
2022-07-05 02:47:16,723 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Created log for partition appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0 in /tmp/junit2884833929294850451/junit3642864754825928179/appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.8-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2022-07-05 02:47:16,723 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0 broker=0] No checkpointed highwatermark is found for partition appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0
2022-07-05 02:47:16,723 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0 broker=0] Log loaded for partition appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0 with initial high watermark 0
2022-07-05 02:47:16,736 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:HighAvailabilityTaskAssignor@95] - Decided on assignment: {8225f260-9829-4e8d-bbba-eaa26babd6d9=[activeTasks: ([0_0]) standbyTasks: ([]) prevActiveTasks: ([]) prevStandbyTasks: ([]) changelogOffsetTotalsByTask: ([]) taskLagTotals: ([0_0=0]) capacity: 1 assigned: 1]} with no followup probing rebalance.
2022-07-05 02:47:16,736 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:StreamsPartitionAssignor@595] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1-consumer] Assigned tasks [0_0] including stateful [0_0] to clients as: 
8225f260-9829-4e8d-bbba-eaa26babd6d9=[activeTasks: ([0_0]) standbyTasks: ([])].
2022-07-05 02:47:16,740 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:StreamsPartitionAssignor@803] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1-consumer] Client 8225f260-9829-4e8d-bbba-eaa26babd6d9 per-consumer assignment:
	prev owned active {}
	prev owned standby {appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1-consumer-93d157f4-3eb9-4515-ab82-242dc6da29f8=[]}
	assigned active {appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1-consumer-93d157f4-3eb9-4515-ab82-242dc6da29f8=[0_0]}
	revoking active {}
	assigned standby {}

2022-07-05 02:47:16,740 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:StreamsPartitionAssignor@822] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1-consumer] Finished stable assignment of tasks, no followup rebalances required.
2022-07-05 02:47:16,740 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:ConsumerCoordinator@626] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Finished assignment for group at generation 1: {appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1-consumer-93d157f4-3eb9-4515-ab82-242dc6da29f8=Assignment(partitions=[inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0], userDataSize=48)}
2022-07-05 02:47:16,747 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover for generation 1. The group has 1 members, 0 of which are static.
2022-07-05 02:47:16,790 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:AbstractCoordinator$SyncGroupResponseHandler@756] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Successfully synced group in generation Generation{generationId=1, memberId='appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1-consumer-93d157f4-3eb9-4515-ab82-242dc6da29f8', protocol='stream'}
2022-07-05 02:47:16,790 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:ConsumerCoordinator@392] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Updating assignment with
	Assigned partitions:                       [inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0]
	Current owned partitions:                  []
	Added partitions (assigned - owned):       [inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0]
	Revoked partitions (owned - assigned):     []

2022-07-05 02:47:16,791 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:ConsumerCoordinator@276] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Notifying assignor about the new Assignment(partitions=[inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0], userDataSize=48)
2022-07-05 02:47:16,791 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:StreamsPartitionAssignor@1306] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1-consumer] No followup rebalance was requested, resetting the rebalance schedule.
2022-07-05 02:47:16,793 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:TaskManager@254] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1] Handle new assignment with:
	New active tasks: [0_0]
	New standby tasks: []
	Existing active tasks: []
	Existing standby tasks: []
2022-07-05 02:47:16,808 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:ConsumerCoordinator@288] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Adding newly assigned partitions: inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0
2022-07-05 02:47:16,808 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:StreamThread@230] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1] State transition from STARTING to PARTITIONS_ASSIGNED
2022-07-05 02:47:16,817 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:ConsumerCoordinator$OffsetFetchResponseHandler@1352] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Found no committed offset for partition inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0
2022-07-05 02:47:16,830 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:SubscriptionState@398] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Resetting offset for partition inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:39997 (id: 0 rack: null)], epoch=0}}.
2022-07-05 02:47:17,003 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:RocksDBTimestampedStore@100] - Opening store test-store in regular mode
2022-07-05 02:47:17,007 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:ProcessorStateManager@256] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1] task [0_0] State store test-store did not find checkpoint offset, hence would default to the starting offset at changelog appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0
2022-07-05 02:47:17,007 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:StreamTask@235] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1] task [0_0] Initialized
2022-07-05 02:47:17,013 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:KafkaConsumer@1123] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1-restore-consumer, groupId=null] Subscribed to partition(s): appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0
2022-07-05 02:47:17,014 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:SubscriptionState@619] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1-restore-consumer, groupId=null] Seeking to EARLIEST offset of partition appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0
2022-07-05 02:47:17,017 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:Metadata@279] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1-restore-consumer, groupId=null] Cluster ID: NnPz9UrjRi2EtgdIfOXTaw
2022-07-05 02:47:17,020 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:SubscriptionState@398] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1-restore-consumer, groupId=null] Resetting offset for partition appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:39997 (id: 0 rack: null)], epoch=0}}.
2022-07-05 02:47:17,121 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:StoreChangelogReader@609] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1] Finished restoring changelog appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0 to store test-store with a total number of 0 records
2022-07-05 02:47:17,123 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:ConsumerCoordinator$OffsetFetchResponseHandler@1352] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Found no committed offset for partition inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0
2022-07-05 02:47:17,127 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:StreamTask@255] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1] task [0_0] Restored and ready to run
2022-07-05 02:47:17,127 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:StreamThread@853] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1] Restoration took 319 ms for all tasks [0_0]
2022-07-05 02:47:17,127 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:StreamThread@230] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1] State transition from PARTITIONS_ASSIGNED to RUNNING
2022-07-05 02:47:17,128 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:KafkaStreams@321] - stream-client [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9] State transition from REBALANCING to RUNNING
2022-07-05 02:47:17,131 - INFO  [main:AbstractConfig@372] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:39997]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.IntegerSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2022-07-05 02:47:17,133 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2022-07-05 02:47:17,133 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2022-07-05 02:47:17,133 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1657003637133
2022-07-05 02:47:17,136 - INFO  [kafka-producer-network-thread | producer-1:Metadata@279] - [Producer clientId=producer-1] Cluster ID: NnPz9UrjRi2EtgdIfOXTaw
2022-07-05 02:47:17,139 - INFO  [main:KafkaProducer@1204] - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2022-07-05 02:47:17,154 - INFO  [main:Metrics@659] - Metrics scheduler closed
2022-07-05 02:47:17,154 - INFO  [main:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:47:17,154 - INFO  [main:Metrics@669] - Metrics reporters closed
2022-07-05 02:47:17,155 - INFO  [main:AppInfoParser@83] - App info kafka.producer for producer-1 unregistered
2022-07-05 02:47:17,156 - INFO  [main:AbstractConfig@372] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:39997]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-af26b49f-7aba-4a72-bdf8-fa92f99879d4-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = af26b49f-7aba-4a72-bdf8-fa92f99879d4
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.IntegerDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2022-07-05 02:47:17,158 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2022-07-05 02:47:17,158 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2022-07-05 02:47:17,158 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1657003637158
2022-07-05 02:47:17,159 - INFO  [main:KafkaConsumer@968] - [Consumer clientId=consumer-af26b49f-7aba-4a72-bdf8-fa92f99879d4-1, groupId=af26b49f-7aba-4a72-bdf8-fa92f99879d4] Subscribed to topic(s): outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover
2022-07-05 02:47:17,162 - INFO  [main:Metadata@279] - [Consumer clientId=consumer-af26b49f-7aba-4a72-bdf8-fa92f99879d4-1, groupId=af26b49f-7aba-4a72-bdf8-fa92f99879d4] Cluster ID: NnPz9UrjRi2EtgdIfOXTaw
2022-07-05 02:47:17,163 - INFO  [main:AbstractCoordinator$FindCoordinatorResponseHandler@844] - [Consumer clientId=consumer-af26b49f-7aba-4a72-bdf8-fa92f99879d4-1, groupId=af26b49f-7aba-4a72-bdf8-fa92f99879d4] Discovered group coordinator localhost:39997 (id: 2147483647 rack: null)
2022-07-05 02:47:17,163 - INFO  [main:AbstractCoordinator@534] - [Consumer clientId=consumer-af26b49f-7aba-4a72-bdf8-fa92f99879d4-1, groupId=af26b49f-7aba-4a72-bdf8-fa92f99879d4] (Re-)joining group
2022-07-05 02:47:17,165 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [GroupCoordinator 0]: Dynamic member with unknown member id joins group af26b49f-7aba-4a72-bdf8-fa92f99879d4 in Empty state. Created a new member id consumer-af26b49f-7aba-4a72-bdf8-fa92f99879d4-1-743e7cea-6f6e-44b4-ad48-3b71b2e3fbe6 and request the member to rejoin with this id.
2022-07-05 02:47:17,166 - INFO  [main:AbstractCoordinator@982] - [Consumer clientId=consumer-af26b49f-7aba-4a72-bdf8-fa92f99879d4-1, groupId=af26b49f-7aba-4a72-bdf8-fa92f99879d4] Request joining group due to: need to re-join with the given member-id
2022-07-05 02:47:17,166 - INFO  [main:AbstractCoordinator@534] - [Consumer clientId=consumer-af26b49f-7aba-4a72-bdf8-fa92f99879d4-1, groupId=af26b49f-7aba-4a72-bdf8-fa92f99879d4] (Re-)joining group
2022-07-05 02:47:17,167 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group af26b49f-7aba-4a72-bdf8-fa92f99879d4 in state PreparingRebalance with old generation 0 (__consumer_offsets-4) (reason: Adding new member consumer-af26b49f-7aba-4a72-bdf8-fa92f99879d4-1-743e7cea-6f6e-44b4-ad48-3b71b2e3fbe6 with group instance id None)
2022-07-05 02:47:17,167 - INFO  [executor-Rebalance:Logging@66] - [GroupCoordinator 0]: Stabilized group af26b49f-7aba-4a72-bdf8-fa92f99879d4 generation 1 (__consumer_offsets-4) with 1 members
2022-07-05 02:47:17,168 - INFO  [main:AbstractCoordinator$JoinGroupResponseHandler@590] - [Consumer clientId=consumer-af26b49f-7aba-4a72-bdf8-fa92f99879d4-1, groupId=af26b49f-7aba-4a72-bdf8-fa92f99879d4] Successfully joined group with generation Generation{generationId=1, memberId='consumer-af26b49f-7aba-4a72-bdf8-fa92f99879d4-1-743e7cea-6f6e-44b4-ad48-3b71b2e3fbe6', protocol='range'}
2022-07-05 02:47:17,169 - INFO  [main:ConsumerCoordinator@626] - [Consumer clientId=consumer-af26b49f-7aba-4a72-bdf8-fa92f99879d4-1, groupId=af26b49f-7aba-4a72-bdf8-fa92f99879d4] Finished assignment for group at generation 1: {consumer-af26b49f-7aba-4a72-bdf8-fa92f99879d4-1-743e7cea-6f6e-44b4-ad48-3b71b2e3fbe6=Assignment(partitions=[outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0])}
2022-07-05 02:47:17,170 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group af26b49f-7aba-4a72-bdf8-fa92f99879d4 for generation 1. The group has 1 members, 0 of which are static.
2022-07-05 02:47:17,171 - INFO  [main:AbstractCoordinator$SyncGroupResponseHandler@756] - [Consumer clientId=consumer-af26b49f-7aba-4a72-bdf8-fa92f99879d4-1, groupId=af26b49f-7aba-4a72-bdf8-fa92f99879d4] Successfully synced group in generation Generation{generationId=1, memberId='consumer-af26b49f-7aba-4a72-bdf8-fa92f99879d4-1-743e7cea-6f6e-44b4-ad48-3b71b2e3fbe6', protocol='range'}
2022-07-05 02:47:17,172 - INFO  [main:ConsumerCoordinator@276] - [Consumer clientId=consumer-af26b49f-7aba-4a72-bdf8-fa92f99879d4-1, groupId=af26b49f-7aba-4a72-bdf8-fa92f99879d4] Notifying assignor about the new Assignment(partitions=[outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0])
2022-07-05 02:47:17,172 - INFO  [main:ConsumerCoordinator@288] - [Consumer clientId=consumer-af26b49f-7aba-4a72-bdf8-fa92f99879d4-1, groupId=af26b49f-7aba-4a72-bdf8-fa92f99879d4] Adding newly assigned partitions: outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0
2022-07-05 02:47:17,173 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1352] - [Consumer clientId=consumer-af26b49f-7aba-4a72-bdf8-fa92f99879d4-1, groupId=af26b49f-7aba-4a72-bdf8-fa92f99879d4] Found no committed offset for partition outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0
2022-07-05 02:47:17,177 - INFO  [main:SubscriptionState@398] - [Consumer clientId=consumer-af26b49f-7aba-4a72-bdf8-fa92f99879d4-1, groupId=af26b49f-7aba-4a72-bdf8-fa92f99879d4] Resetting offset for partition outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:39997 (id: 0 rack: null)], epoch=0}}.
2022-07-05 02:47:17,182 - ERROR [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:TaskManager@1153] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1] Failed to process stream task 0_0 due to the following error:
org.apache.kafka.streams.errors.StreamsException: Exception caught in process. taskId=0_0, processor=KSTREAM-SOURCE-0000000000, topic=inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover, partition=0, offset=0, stacktrace=java.lang.IllegalStateException: Kaboom
	at org.apache.kafka.streams.integration.EmitOnChangeIntegrationTest.lambda$shouldEmitSameRecordAfterFailover$0(EmitOnChangeIntegrationTest.java:94)
	at org.apache.kafka.streams.kstream.internals.KStreamMap$KStreamMapProcessor.process(KStreamMap.java:41)
	at org.apache.kafka.streams.processor.internals.ProcessorAdapter.process(ProcessorAdapter.java:71)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.lambda$process$2(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:884)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forwardInternal(ProcessorContextImpl.java:281)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:260)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:219)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:172)
	at org.apache.kafka.streams.kstream.internals.KStreamMapValues$KStreamMapProcessor.process(KStreamMapValues.java:41)
	at org.apache.kafka.streams.processor.internals.ProcessorAdapter.process(ProcessorAdapter.java:71)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.lambda$process$2(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:884)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forwardInternal(ProcessorContextImpl.java:281)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:260)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:219)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:172)
	at org.apache.kafka.streams.kstream.internals.TimestampedTupleForwarder.maybeForward(TimestampedTupleForwarder.java:51)
	at org.apache.kafka.streams.kstream.internals.KTableSource$KTableSourceProcessor.process(KTableSource.java:145)
	at org.apache.kafka.streams.processor.internals.ProcessorAdapter.process(ProcessorAdapter.java:71)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.lambda$process$2(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:884)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forwardInternal(ProcessorContextImpl.java:281)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:260)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:219)
	at org.apache.kafka.streams.processor.internals.SourceNode.process(SourceNode.java:86)
	at org.apache.kafka.streams.processor.internals.StreamTask.lambda$process$1(StreamTask.java:706)
	at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:884)
	at org.apache.kafka.streams.processor.internals.StreamTask.process(StreamTask.java:706)
	at org.apache.kafka.streams.processor.internals.TaskManager.process(TaskManager.java:1136)
	at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:755)
	at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:585)
	at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:557)

	at org.apache.kafka.streams.processor.internals.StreamTask.process(StreamTask.java:733)
	at org.apache.kafka.streams.processor.internals.TaskManager.process(TaskManager.java:1136)
	at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:755)
	at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:585)
	at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:557)
Caused by: java.lang.IllegalStateException: Kaboom
	at org.apache.kafka.streams.integration.EmitOnChangeIntegrationTest.lambda$shouldEmitSameRecordAfterFailover$0(EmitOnChangeIntegrationTest.java:94)
	at org.apache.kafka.streams.kstream.internals.KStreamMap$KStreamMapProcessor.process(KStreamMap.java:41)
	at org.apache.kafka.streams.processor.internals.ProcessorAdapter.process(ProcessorAdapter.java:71)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.lambda$process$2(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:884)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forwardInternal(ProcessorContextImpl.java:281)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:260)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:219)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:172)
	at org.apache.kafka.streams.kstream.internals.KStreamMapValues$KStreamMapProcessor.process(KStreamMapValues.java:41)
	at org.apache.kafka.streams.processor.internals.ProcessorAdapter.process(ProcessorAdapter.java:71)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.lambda$process$2(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:884)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forwardInternal(ProcessorContextImpl.java:281)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:260)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:219)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:172)
	at org.apache.kafka.streams.kstream.internals.TimestampedTupleForwarder.maybeForward(TimestampedTupleForwarder.java:51)
	at org.apache.kafka.streams.kstream.internals.KTableSource$KTableSourceProcessor.process(KTableSource.java:145)
	at org.apache.kafka.streams.processor.internals.ProcessorAdapter.process(ProcessorAdapter.java:71)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.lambda$process$2(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:884)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forwardInternal(ProcessorContextImpl.java:281)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:260)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:219)
	at org.apache.kafka.streams.processor.internals.SourceNode.process(SourceNode.java:86)
	at org.apache.kafka.streams.processor.internals.StreamTask.lambda$process$1(StreamTask.java:706)
	at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:884)
	at org.apache.kafka.streams.processor.internals.StreamTask.process(StreamTask.java:706)
	... 4 more
2022-07-05 02:47:17,183 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:StreamThread@1057] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1] Informed to shut down
2022-07-05 02:47:17,183 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:StreamThread@230] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1] State transition from RUNNING to PENDING_SHUTDOWN
2022-07-05 02:47:17,184 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:StreamThread@337] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2] Creating restore consumer client
2022-07-05 02:47:17,185 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:AbstractConfig@372] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [localhost:39997]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2-restore-consumer
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = false
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2022-07-05 02:47:17,187 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2022-07-05 02:47:17,187 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2022-07-05 02:47:17,187 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1657003637187
2022-07-05 02:47:17,187 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:ActiveTaskCreator@96] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2] Creating thread producer client
2022-07-05 02:47:17,187 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:AbstractConfig@372] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:39997]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2-producer
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-07-05 02:47:17,189 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2022-07-05 02:47:17,189 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2022-07-05 02:47:17,189 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1657003637189
2022-07-05 02:47:17,190 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:StreamThread@389] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2] Creating consumer client
2022-07-05 02:47:17,190 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:AbstractConfig@372] - ConsumerConfig values: 
	allow.auto.create.topics = false
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:39997]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2-consumer
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = false
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2022-07-05 02:47:17,192 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:AssignorConfiguration@114] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2-consumer] Cooperative rebalancing enabled now
2022-07-05 02:47:17,192 - INFO  [kafka-producer-network-thread | appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2-producer:Metadata@279] - [Producer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2-producer] Cluster ID: NnPz9UrjRi2EtgdIfOXTaw
2022-07-05 02:47:17,193 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2022-07-05 02:47:17,193 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2022-07-05 02:47:17,193 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1657003637193
2022-07-05 02:47:17,194 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:StreamThread@1071] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1] Shutting down
2022-07-05 02:47:17,194 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:StreamThread@550] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2] Starting
2022-07-05 02:47:17,194 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:StreamThread@230] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2] State transition from CREATED to STARTING
2022-07-05 02:47:17,194 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:KafkaConsumer@968] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Subscribed to topic(s): inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover
2022-07-05 02:47:17,195 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:StreamTask@1144] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1] task [0_0] Suspended RUNNING
2022-07-05 02:47:17,195 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:StreamTask@290] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1] task [0_0] Suspended running
2022-07-05 02:47:17,197 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:KafkaConsumer@1077] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions
2022-07-05 02:47:17,197 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:Metadata@279] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Cluster ID: NnPz9UrjRi2EtgdIfOXTaw
2022-07-05 02:47:17,197 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:AbstractCoordinator$FindCoordinatorResponseHandler@844] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Discovered group coordinator localhost:39997 (id: 2147483647 rack: null)
2022-07-05 02:47:17,198 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:AbstractCoordinator@534] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] (Re-)joining group
2022-07-05 02:47:17,200 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupCoordinator 0]: Dynamic member with unknown member id joins group appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover in Stable state. Created a new member id appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2-consumer-477036ab-ae5a-44ed-863b-6b31b29d618f and request the member to rejoin with this id.
2022-07-05 02:47:17,201 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:AbstractCoordinator@982] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Request joining group due to: need to re-join with the given member-id
2022-07-05 02:47:17,201 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:AbstractCoordinator@534] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] (Re-)joining group
2022-07-05 02:47:17,202 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover in state PreparingRebalance with old generation 1 (__consumer_offsets-0) (reason: Adding new member appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2-consumer-477036ab-ae5a-44ed-863b-6b31b29d618f with group instance id None)
2022-07-05 02:47:17,209 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:RecordCollectorImpl@283] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1] task [0_0] Closing record collector dirty
2022-07-05 02:47:17,209 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:StreamTask@515] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1] task [0_0] Closed dirty
2022-07-05 02:47:17,210 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:KafkaProducer@1204] - [Producer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1-producer] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2022-07-05 02:47:17,214 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:Metrics@659] - Metrics scheduler closed
2022-07-05 02:47:17,214 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:47:17,215 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:Metrics@669] - Metrics reporters closed
2022-07-05 02:47:17,215 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:AppInfoParser@83] - App info kafka.producer for appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1-producer unregistered
2022-07-05 02:47:17,215 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:KafkaConsumer@1077] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions
2022-07-05 02:47:17,216 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:Metrics@659] - Metrics scheduler closed
2022-07-05 02:47:17,216 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:47:17,216 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:Metrics@669] - Metrics reporters closed
2022-07-05 02:47:17,217 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:AppInfoParser@83] - App info kafka.consumer for appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1-consumer unregistered
2022-07-05 02:47:17,217 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:Metrics@659] - Metrics scheduler closed
2022-07-05 02:47:17,217 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:47:17,217 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:Metrics@669] - Metrics reporters closed
2022-07-05 02:47:17,218 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:AppInfoParser@83] - App info kafka.consumer for appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1-restore-consumer unregistered
2022-07-05 02:47:17,219 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:StreamThread@230] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1] State transition from PENDING_SHUTDOWN to DEAD
2022-07-05 02:47:17,219 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:StreamThread@1100] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1] Shutdown complete
2022-07-05 02:47:17,219 - ERROR [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1:NIOServerCnxnFactory$1@92] - Thread 	StreamsThread threadId: appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1
TaskManager
	MetadataState:
	Tasks:
 died
org.apache.kafka.streams.errors.StreamsException: Exception caught in process. taskId=0_0, processor=KSTREAM-SOURCE-0000000000, topic=inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover, partition=0, offset=0, stacktrace=java.lang.IllegalStateException: Kaboom
	at org.apache.kafka.streams.integration.EmitOnChangeIntegrationTest.lambda$shouldEmitSameRecordAfterFailover$0(EmitOnChangeIntegrationTest.java:94)
	at org.apache.kafka.streams.kstream.internals.KStreamMap$KStreamMapProcessor.process(KStreamMap.java:41)
	at org.apache.kafka.streams.processor.internals.ProcessorAdapter.process(ProcessorAdapter.java:71)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.lambda$process$2(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:884)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forwardInternal(ProcessorContextImpl.java:281)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:260)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:219)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:172)
	at org.apache.kafka.streams.kstream.internals.KStreamMapValues$KStreamMapProcessor.process(KStreamMapValues.java:41)
	at org.apache.kafka.streams.processor.internals.ProcessorAdapter.process(ProcessorAdapter.java:71)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.lambda$process$2(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:884)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forwardInternal(ProcessorContextImpl.java:281)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:260)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:219)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:172)
	at org.apache.kafka.streams.kstream.internals.TimestampedTupleForwarder.maybeForward(TimestampedTupleForwarder.java:51)
	at org.apache.kafka.streams.kstream.internals.KTableSource$KTableSourceProcessor.process(KTableSource.java:145)
	at org.apache.kafka.streams.processor.internals.ProcessorAdapter.process(ProcessorAdapter.java:71)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.lambda$process$2(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:884)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forwardInternal(ProcessorContextImpl.java:281)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:260)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:219)
	at org.apache.kafka.streams.processor.internals.SourceNode.process(SourceNode.java:86)
	at org.apache.kafka.streams.processor.internals.StreamTask.lambda$process$1(StreamTask.java:706)
	at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:884)
	at org.apache.kafka.streams.processor.internals.StreamTask.process(StreamTask.java:706)
	at org.apache.kafka.streams.processor.internals.TaskManager.process(TaskManager.java:1136)
	at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:755)
	at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:585)
	at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:557)

	at org.apache.kafka.streams.processor.internals.StreamTask.process(StreamTask.java:733)
	at org.apache.kafka.streams.processor.internals.TaskManager.process(TaskManager.java:1136)
	at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:755)
	at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:585)
	at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:557)
Caused by: java.lang.IllegalStateException: Kaboom
	at org.apache.kafka.streams.integration.EmitOnChangeIntegrationTest.lambda$shouldEmitSameRecordAfterFailover$0(EmitOnChangeIntegrationTest.java:94)
	at org.apache.kafka.streams.kstream.internals.KStreamMap$KStreamMapProcessor.process(KStreamMap.java:41)
	at org.apache.kafka.streams.processor.internals.ProcessorAdapter.process(ProcessorAdapter.java:71)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.lambda$process$2(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:884)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forwardInternal(ProcessorContextImpl.java:281)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:260)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:219)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:172)
	at org.apache.kafka.streams.kstream.internals.KStreamMapValues$KStreamMapProcessor.process(KStreamMapValues.java:41)
	at org.apache.kafka.streams.processor.internals.ProcessorAdapter.process(ProcessorAdapter.java:71)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.lambda$process$2(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:884)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forwardInternal(ProcessorContextImpl.java:281)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:260)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:219)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:172)
	at org.apache.kafka.streams.kstream.internals.TimestampedTupleForwarder.maybeForward(TimestampedTupleForwarder.java:51)
	at org.apache.kafka.streams.kstream.internals.KTableSource$KTableSourceProcessor.process(KTableSource.java:145)
	at org.apache.kafka.streams.processor.internals.ProcessorAdapter.process(ProcessorAdapter.java:71)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.lambda$process$2(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:884)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:181)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forwardInternal(ProcessorContextImpl.java:281)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:260)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:219)
	at org.apache.kafka.streams.processor.internals.SourceNode.process(SourceNode.java:86)
	at org.apache.kafka.streams.processor.internals.StreamTask.lambda$process$1(StreamTask.java:706)
	at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:884)
	at org.apache.kafka.streams.processor.internals.StreamTask.process(StreamTask.java:706)
	... 4 more
2022-07-05 02:47:17,294 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:StreamThread@829] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update
2022-07-05 02:47:26,791 - INFO  [executor-Heartbeat:Logging@66] - [GroupCoordinator 0]: Member appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1-consumer-93d157f4-3eb9-4515-ab82-242dc6da29f8 in group appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover has failed, removing it from the group
2022-07-05 02:47:26,792 - INFO  [executor-Heartbeat:Logging@66] - [GroupCoordinator 0]: Stabilized group appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover generation 2 (__consumer_offsets-0) with 1 members
2022-07-05 02:47:26,792 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:AbstractCoordinator$JoinGroupResponseHandler@590] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Successfully joined group with generation Generation{generationId=2, memberId='appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2-consumer-477036ab-ae5a-44ed-863b-6b31b29d618f', protocol='stream'}
2022-07-05 02:47:26,798 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:HighAvailabilityTaskAssignor@95] - Decided on assignment: {8225f260-9829-4e8d-bbba-eaa26babd6d9=[activeTasks: ([0_0]) standbyTasks: ([]) prevActiveTasks: ([]) prevStandbyTasks: ([]) changelogOffsetTotalsByTask: ([]) taskLagTotals: ([0_0=1]) capacity: 1 assigned: 1]} with no followup probing rebalance.
2022-07-05 02:47:26,798 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:StreamsPartitionAssignor@595] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2-consumer] Assigned tasks [0_0] including stateful [0_0] to clients as: 
8225f260-9829-4e8d-bbba-eaa26babd6d9=[activeTasks: ([0_0]) standbyTasks: ([])].
2022-07-05 02:47:26,799 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:StreamsPartitionAssignor@803] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2-consumer] Client 8225f260-9829-4e8d-bbba-eaa26babd6d9 per-consumer assignment:
	prev owned active {}
	prev owned standby {appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2-consumer-477036ab-ae5a-44ed-863b-6b31b29d618f=[]}
	assigned active {appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2-consumer-477036ab-ae5a-44ed-863b-6b31b29d618f=[0_0]}
	revoking active {}
	assigned standby {}

2022-07-05 02:47:26,799 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:StreamsPartitionAssignor@822] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2-consumer] Finished stable assignment of tasks, no followup rebalances required.
2022-07-05 02:47:26,799 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:ConsumerCoordinator@626] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Finished assignment for group at generation 2: {appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2-consumer-477036ab-ae5a-44ed-863b-6b31b29d618f=Assignment(partitions=[inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0], userDataSize=48)}
2022-07-05 02:47:26,799 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover for generation 2. The group has 1 members, 0 of which are static.
2022-07-05 02:47:26,801 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:AbstractCoordinator$SyncGroupResponseHandler@756] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Successfully synced group in generation Generation{generationId=2, memberId='appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2-consumer-477036ab-ae5a-44ed-863b-6b31b29d618f', protocol='stream'}
2022-07-05 02:47:26,802 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:ConsumerCoordinator@392] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Updating assignment with
	Assigned partitions:                       [inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0]
	Current owned partitions:                  []
	Added partitions (assigned - owned):       [inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0]
	Revoked partitions (owned - assigned):     []

2022-07-05 02:47:26,802 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:ConsumerCoordinator@276] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Notifying assignor about the new Assignment(partitions=[inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0], userDataSize=48)
2022-07-05 02:47:26,802 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:StreamsPartitionAssignor@1306] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2-consumer] No followup rebalance was requested, resetting the rebalance schedule.
2022-07-05 02:47:26,802 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:TaskManager@254] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2] Handle new assignment with:
	New active tasks: [0_0]
	New standby tasks: []
	Existing active tasks: []
	Existing standby tasks: []
2022-07-05 02:47:26,803 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:ConsumerCoordinator@288] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Adding newly assigned partitions: inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0
2022-07-05 02:47:26,803 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:StreamThread@230] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2] State transition from STARTING to PARTITIONS_ASSIGNED
2022-07-05 02:47:26,803 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:KafkaStreams@321] - stream-client [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9] State transition from RUNNING to REBALANCING
2022-07-05 02:47:26,804 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:ConsumerCoordinator$OffsetFetchResponseHandler@1352] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Found no committed offset for partition inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0
2022-07-05 02:47:26,807 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:SubscriptionState@398] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Resetting offset for partition inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:39997 (id: 0 rack: null)], epoch=0}}.
2022-07-05 02:47:26,857 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:RocksDBTimestampedStore@100] - Opening store test-store in regular mode
2022-07-05 02:47:26,857 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:ProcessorStateManager@256] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2] task [0_0] State store test-store did not find checkpoint offset, hence would default to the starting offset at changelog appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0
2022-07-05 02:47:26,857 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:StreamTask@235] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2] task [0_0] Initialized
2022-07-05 02:47:26,860 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:KafkaConsumer@1123] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2-restore-consumer, groupId=null] Subscribed to partition(s): appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0
2022-07-05 02:47:26,860 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:SubscriptionState@619] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2-restore-consumer, groupId=null] Seeking to EARLIEST offset of partition appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0
2022-07-05 02:47:26,863 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:Metadata@279] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2-restore-consumer, groupId=null] Cluster ID: NnPz9UrjRi2EtgdIfOXTaw
2022-07-05 02:47:26,865 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:SubscriptionState@398] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2-restore-consumer, groupId=null] Resetting offset for partition appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:39997 (id: 0 rack: null)], epoch=0}}.
2022-07-05 02:47:26,872 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:StoreChangelogReader@609] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2] Finished restoring changelog appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0 to store test-store with a total number of 1 records
2022-07-05 02:47:26,873 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:ConsumerCoordinator$OffsetFetchResponseHandler@1352] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Found no committed offset for partition inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0
2022-07-05 02:47:26,874 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:StreamTask@255] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2] task [0_0] Restored and ready to run
2022-07-05 02:47:26,874 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:StreamThread@853] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2] Restoration took 71 ms for all tasks [0_0]
2022-07-05 02:47:26,874 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:StreamThread@230] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2] State transition from PARTITIONS_ASSIGNED to RUNNING
2022-07-05 02:47:26,874 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:KafkaStreams@321] - stream-client [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9] State transition from REBALANCING to RUNNING
2022-07-05 02:47:45,276 - INFO  [kafka-scheduler-4:Logging@66] - [ProducerStateManager partition=outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0] Writing producer snapshot at offset 1
2022-07-05 02:47:45,279 - INFO  [kafka-scheduler-4:Logging@66] - [Log partition=outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0, dir=/tmp/junit2884833929294850451/junit3642864754825928179] Rolled new log segment at offset 1 in 0 ms.
2022-07-05 02:47:45,280 - INFO  [kafka-scheduler-4:Logging@66] - [Log partition=outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0, dir=/tmp/junit2884833929294850451/junit3642864754825928179] Deleting segment LogSegment(baseOffset=0, size=73, lastModifiedTime=1657003646000, largestRecordTimestamp=Some(0)) due to retention time 604800000ms breach based on the largest record timestamp in the segment
2022-07-05 02:47:45,282 - INFO  [kafka-scheduler-4:Logging@66] - [Log partition=outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0, dir=/tmp/junit2884833929294850451/junit3642864754825928179] Incremented log start offset to 1 due to segment deletion
2022-07-05 02:47:45,287 - INFO  [kafka-scheduler-4:Logging@66] - [ProducerStateManager partition=inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0] Writing producer snapshot at offset 2
2022-07-05 02:47:45,295 - INFO  [kafka-scheduler-4:Logging@66] - [Log partition=inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0, dir=/tmp/junit2884833929294850451/junit3642864754825928179] Rolled new log segment at offset 2 in 0 ms.
2022-07-05 02:47:45,295 - INFO  [kafka-scheduler-4:Logging@66] - [Log partition=inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0, dir=/tmp/junit2884833929294850451/junit3642864754825928179] Deleting segment LogSegment(baseOffset=0, size=85, lastModifiedTime=1657003637000, largestRecordTimestamp=Some(0)) due to retention time 604800000ms breach based on the largest record timestamp in the segment
2022-07-05 02:47:45,295 - INFO  [kafka-scheduler-4:Logging@66] - [Log partition=inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0, dir=/tmp/junit2884833929294850451/junit3642864754825928179] Incremented log start offset to 2 due to segment deletion
2022-07-05 02:48:17,410 - INFO  [main:ConsumerCoordinator@307] - [Consumer clientId=consumer-af26b49f-7aba-4a72-bdf8-fa92f99879d4-1, groupId=af26b49f-7aba-4a72-bdf8-fa92f99879d4] Revoke previously assigned partitions outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0
2022-07-05 02:48:17,410 - INFO  [main:AbstractCoordinator@1038] - [Consumer clientId=consumer-af26b49f-7aba-4a72-bdf8-fa92f99879d4-1, groupId=af26b49f-7aba-4a72-bdf8-fa92f99879d4] Member consumer-af26b49f-7aba-4a72-bdf8-fa92f99879d4-1-743e7cea-6f6e-44b4-ad48-3b71b2e3fbe6 sending LeaveGroup request to coordinator localhost:39997 (id: 2147483647 rack: null) due to the consumer is being closed
2022-07-05 02:48:17,411 - INFO  [main:AbstractCoordinator@961] - [Consumer clientId=consumer-af26b49f-7aba-4a72-bdf8-fa92f99879d4-1, groupId=af26b49f-7aba-4a72-bdf8-fa92f99879d4] Resetting generation due to: consumer pro-actively leaving the group
2022-07-05 02:48:17,411 - INFO  [main:AbstractCoordinator@982] - [Consumer clientId=consumer-af26b49f-7aba-4a72-bdf8-fa92f99879d4-1, groupId=af26b49f-7aba-4a72-bdf8-fa92f99879d4] Request joining group due to: consumer pro-actively leaving the group
2022-07-05 02:48:17,413 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group af26b49f-7aba-4a72-bdf8-fa92f99879d4 in state PreparingRebalance with old generation 1 (__consumer_offsets-4) (reason: removing member consumer-af26b49f-7aba-4a72-bdf8-fa92f99879d4-1-743e7cea-6f6e-44b4-ad48-3b71b2e3fbe6 on LeaveGroup)
2022-07-05 02:48:17,413 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupCoordinator 0]: Group af26b49f-7aba-4a72-bdf8-fa92f99879d4 with generation 2 is now empty (__consumer_offsets-4)
2022-07-05 02:48:17,415 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupCoordinator 0]: Member MemberMetadata(memberId=consumer-af26b49f-7aba-4a72-bdf8-fa92f99879d4-1-743e7cea-6f6e-44b4-ad48-3b71b2e3fbe6, groupInstanceId=None, clientId=consumer-af26b49f-7aba-4a72-bdf8-fa92f99879d4-1, clientHost=/127.0.0.1, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(range)) has left group af26b49f-7aba-4a72-bdf8-fa92f99879d4 through explicit `LeaveGroup` request
2022-07-05 02:48:17,417 - INFO  [main:Metrics@659] - Metrics scheduler closed
2022-07-05 02:48:17,417 - INFO  [main:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:48:17,417 - INFO  [main:Metrics@669] - Metrics reporters closed
2022-07-05 02:48:17,418 - INFO  [main:AppInfoParser@83] - App info kafka.consumer for consumer-af26b49f-7aba-4a72-bdf8-fa92f99879d4-1 unregistered
2022-07-05 02:48:17,419 - INFO  [main:KafkaStreams@321] - stream-client [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9] State transition from RUNNING to PENDING_SHUTDOWN
2022-07-05 02:48:17,419 - INFO  [kafka-streams-close-thread:StreamThread@1057] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-1] Informed to shut down
2022-07-05 02:48:17,419 - INFO  [kafka-streams-close-thread:StreamThread@1057] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2] Informed to shut down
2022-07-05 02:48:17,420 - INFO  [kafka-streams-close-thread:StreamThread@230] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2] State transition from RUNNING to PENDING_SHUTDOWN
2022-07-05 02:48:17,508 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:StreamThread@729] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2] Thread state is already PENDING_SHUTDOWN, skipping the run once call after poll request
2022-07-05 02:48:17,508 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:StreamThread@1071] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2] Shutting down
2022-07-05 02:48:17,530 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:StreamTask@1144] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2] task [0_0] Suspended RUNNING
2022-07-05 02:48:17,530 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:StreamTask@290] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2] task [0_0] Suspended running
2022-07-05 02:48:17,531 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:KafkaConsumer@1077] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions
2022-07-05 02:48:17,532 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:RecordCollectorImpl@268] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2] task [0_0] Closing record collector clean
2022-07-05 02:48:17,532 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:StreamTask@508] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2] task [0_0] Closed clean
2022-07-05 02:48:17,532 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:KafkaProducer@1204] - [Producer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2-producer] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2022-07-05 02:48:17,534 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:Metrics@659] - Metrics scheduler closed
2022-07-05 02:48:17,534 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:48:17,534 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:Metrics@669] - Metrics reporters closed
2022-07-05 02:48:17,534 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:AppInfoParser@83] - App info kafka.producer for appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2-producer unregistered
2022-07-05 02:48:17,534 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:KafkaConsumer@1077] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions
2022-07-05 02:48:17,534 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:Metrics@659] - Metrics scheduler closed
2022-07-05 02:48:17,534 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:48:17,534 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:Metrics@669] - Metrics reporters closed
2022-07-05 02:48:17,536 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:AppInfoParser@83] - App info kafka.consumer for appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2-consumer unregistered
2022-07-05 02:48:17,536 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:Metrics@659] - Metrics scheduler closed
2022-07-05 02:48:17,536 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:48:17,536 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:Metrics@669] - Metrics reporters closed
2022-07-05 02:48:17,537 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:AppInfoParser@83] - App info kafka.consumer for appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2-restore-consumer unregistered
2022-07-05 02:48:17,537 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:StreamThread@230] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2] State transition from PENDING_SHUTDOWN to DEAD
2022-07-05 02:48:17,537 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2:StreamThread@1100] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-StreamThread-2] Shutdown complete
2022-07-05 02:48:17,538 - ERROR [kafka-streams-close-thread:StateDirectory@414] - Some task directories still locked while closing state, this indicates unclean shutdown: {}
2022-07-05 02:48:17,538 - INFO  [kafka-admin-client-thread | appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-admin:AppInfoParser@83] - App info kafka.admin.client for appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-admin unregistered
2022-07-05 02:48:17,539 - INFO  [kafka-admin-client-thread | appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-admin:Metrics@659] - Metrics scheduler closed
2022-07-05 02:48:17,539 - INFO  [kafka-admin-client-thread | appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-admin:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:48:17,539 - INFO  [kafka-admin-client-thread | appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9-admin:Metrics@669] - Metrics reporters closed
2022-07-05 02:48:17,539 - INFO  [kafka-streams-close-thread:Metrics@659] - Metrics scheduler closed
2022-07-05 02:48:17,539 - INFO  [kafka-streams-close-thread:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:48:17,539 - INFO  [kafka-streams-close-thread:Metrics@669] - Metrics reporters closed
2022-07-05 02:48:17,539 - INFO  [kafka-streams-close-thread:KafkaStreams@321] - stream-client [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9] State transition from PENDING_SHUTDOWN to NOT_RUNNING
2022-07-05 02:48:17,540 - INFO  [main:KafkaStreams@1367] - stream-client [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-8225f260-9829-4e8d-bbba-eaa26babd6d9] Streams client stopped completely
2022-07-05 02:48:17,544 - INFO  [main:Logging@66] - [KafkaServer id=0] shutting down
2022-07-05 02:48:17,545 - INFO  [main:Logging@66] - [KafkaServer id=0] Starting controlled shutdown
2022-07-05 02:48:17,555 - INFO  [main:Logging@66] - [KafkaServer id=0] Controlled shutdown succeeded
2022-07-05 02:48:17,556 - INFO  [main:Logging@66] - [/config/changes-event-process-thread]: Shutting down
2022-07-05 02:48:17,556 - INFO  [main:Logging@66] - [/config/changes-event-process-thread]: Shutdown completed
2022-07-05 02:48:17,557 - INFO  [/config/changes-event-process-thread:Logging@66] - [/config/changes-event-process-thread]: Stopped
2022-07-05 02:48:17,557 - INFO  [main:Logging@66] - [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopping socket server request processors
2022-07-05 02:48:17,561 - INFO  [main:Logging@66] - [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopped socket server request processors
2022-07-05 02:48:17,561 - INFO  [main:Logging@66] - [data-plane Kafka Request Handler on Broker 0], shutting down
2022-07-05 02:48:17,562 - INFO  [main:Logging@66] - [data-plane Kafka Request Handler on Broker 0], shut down completely
2022-07-05 02:48:17,564 - INFO  [main:Logging@66] - [ExpirationReaper-0-AlterAcls]: Shutting down
2022-07-05 02:48:17,692 - INFO  [ExpirationReaper-0-AlterAcls:Logging@66] - [ExpirationReaper-0-AlterAcls]: Stopped
2022-07-05 02:48:17,692 - INFO  [main:Logging@66] - [ExpirationReaper-0-AlterAcls]: Shutdown completed
2022-07-05 02:48:17,692 - INFO  [main:Logging@66] - [KafkaApi-0] Shutdown complete.
2022-07-05 02:48:17,693 - INFO  [main:Logging@66] - [ExpirationReaper-0-topic]: Shutting down
2022-07-05 02:48:17,892 - INFO  [ExpirationReaper-0-topic:Logging@66] - [ExpirationReaper-0-topic]: Stopped
2022-07-05 02:48:17,892 - INFO  [main:Logging@66] - [ExpirationReaper-0-topic]: Shutdown completed
2022-07-05 02:48:17,893 - INFO  [main:Logging@66] - [TransactionCoordinator id=0] Shutting down.
2022-07-05 02:48:17,894 - INFO  [main:Logging@66] - [ProducerId Manager 0]: Shutdown complete: last producerId assigned 0
2022-07-05 02:48:17,894 - INFO  [main:Logging@66] - [Transaction State Manager 0]: Shutdown complete
2022-07-05 02:48:17,894 - INFO  [main:Logging@66] - [Transaction Marker Channel Manager 0]: Shutting down
2022-07-05 02:48:17,894 - INFO  [TxnMarkerSenderThread-0:Logging@66] - [Transaction Marker Channel Manager 0]: Stopped
2022-07-05 02:48:17,894 - INFO  [main:Logging@66] - [Transaction Marker Channel Manager 0]: Shutdown completed
2022-07-05 02:48:17,895 - INFO  [main:Logging@66] - [TransactionCoordinator id=0] Shutdown complete.
2022-07-05 02:48:17,895 - INFO  [main:Logging@66] - [GroupCoordinator 0]: Shutting down.
2022-07-05 02:48:17,896 - INFO  [main:Logging@66] - [ExpirationReaper-0-Heartbeat]: Shutting down
2022-07-05 02:48:18,030 - INFO  [ExpirationReaper-0-Heartbeat:Logging@66] - [ExpirationReaper-0-Heartbeat]: Stopped
2022-07-05 02:48:18,030 - INFO  [main:Logging@66] - [ExpirationReaper-0-Heartbeat]: Shutdown completed
2022-07-05 02:48:18,031 - INFO  [main:Logging@66] - [ExpirationReaper-0-Rebalance]: Shutting down
2022-07-05 02:48:18,092 - INFO  [ExpirationReaper-0-Rebalance:Logging@66] - [ExpirationReaper-0-Rebalance]: Stopped
2022-07-05 02:48:18,092 - INFO  [main:Logging@66] - [ExpirationReaper-0-Rebalance]: Shutdown completed
2022-07-05 02:48:18,093 - INFO  [main:Logging@66] - [GroupCoordinator 0]: Shutdown complete.
2022-07-05 02:48:18,093 - INFO  [main:Logging@66] - [ReplicaManager broker=0] Shutting down
2022-07-05 02:48:18,093 - INFO  [main:Logging@66] - [LogDirFailureHandler]: Shutting down
2022-07-05 02:48:18,094 - INFO  [LogDirFailureHandler:Logging@66] - [LogDirFailureHandler]: Stopped
2022-07-05 02:48:18,094 - INFO  [main:Logging@66] - [LogDirFailureHandler]: Shutdown completed
2022-07-05 02:48:18,094 - INFO  [main:Logging@66] - [ReplicaFetcherManager on broker 0] shutting down
2022-07-05 02:48:18,095 - INFO  [main:Logging@66] - [ReplicaFetcherManager on broker 0] shutdown completed
2022-07-05 02:48:18,095 - INFO  [main:Logging@66] - [ReplicaAlterLogDirsManager on broker 0] shutting down
2022-07-05 02:48:18,095 - INFO  [main:Logging@66] - [ReplicaAlterLogDirsManager on broker 0] shutdown completed
2022-07-05 02:48:18,095 - INFO  [main:Logging@66] - [ExpirationReaper-0-Fetch]: Shutting down
2022-07-05 02:48:18,198 - INFO  [ExpirationReaper-0-Fetch:Logging@66] - [ExpirationReaper-0-Fetch]: Stopped
2022-07-05 02:48:18,198 - INFO  [main:Logging@66] - [ExpirationReaper-0-Fetch]: Shutdown completed
2022-07-05 02:48:18,198 - INFO  [main:Logging@66] - [ExpirationReaper-0-Produce]: Shutting down
2022-07-05 02:48:18,388 - INFO  [main:Logging@66] - [ExpirationReaper-0-Produce]: Shutdown completed
2022-07-05 02:48:18,388 - INFO  [ExpirationReaper-0-Produce:Logging@66] - [ExpirationReaper-0-Produce]: Stopped
2022-07-05 02:48:18,388 - INFO  [main:Logging@66] - [ExpirationReaper-0-DeleteRecords]: Shutting down
2022-07-05 02:48:18,588 - INFO  [main:Logging@66] - [ExpirationReaper-0-DeleteRecords]: Shutdown completed
2022-07-05 02:48:18,588 - INFO  [ExpirationReaper-0-DeleteRecords:Logging@66] - [ExpirationReaper-0-DeleteRecords]: Stopped
2022-07-05 02:48:18,588 - INFO  [main:Logging@66] - [ExpirationReaper-0-ElectLeader]: Shutting down
2022-07-05 02:48:18,788 - INFO  [ExpirationReaper-0-ElectLeader:Logging@66] - [ExpirationReaper-0-ElectLeader]: Stopped
2022-07-05 02:48:18,788 - INFO  [main:Logging@66] - [ExpirationReaper-0-ElectLeader]: Shutdown completed
2022-07-05 02:48:18,790 - INFO  [main:Logging@66] - [ReplicaManager broker=0] Shut down completely
2022-07-05 02:48:18,790 - INFO  [main:Logging@66] - [BrokerToControllerChannelManager broker=0 name=alterIsr]: Shutting down
2022-07-05 02:48:18,791 - INFO  [BrokerToControllerChannelManager broker=0 name=alterIsr:Logging@66] - [BrokerToControllerChannelManager broker=0 name=alterIsr]: Stopped
2022-07-05 02:48:18,791 - INFO  [main:Logging@66] - [BrokerToControllerChannelManager broker=0 name=alterIsr]: Shutdown completed
2022-07-05 02:48:18,791 - INFO  [main:Logging@66] - Broker to controller channel manager for alterIsr shutdown
2022-07-05 02:48:18,792 - INFO  [main:Logging@66] - Shutting down.
2022-07-05 02:48:18,803 - INFO  [pool-8-thread-1:Logging@66] - [ProducerStateManager partition=__consumer_offsets-0] Writing producer snapshot at offset 3
2022-07-05 02:48:18,813 - INFO  [pool-8-thread-1:Logging@66] - [ProducerStateManager partition=__consumer_offsets-4] Writing producer snapshot at offset 15
2022-07-05 02:48:18,814 - INFO  [pool-8-thread-1:Logging@66] - [ProducerStateManager partition=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0] Writing producer snapshot at offset 2
2022-07-05 02:48:18,840 - INFO  [main:Logging@66] - Shutdown complete.
2022-07-05 02:48:18,845 - INFO  [main:Logging@66] - [feature-zk-node-event-process-thread]: Shutting down
2022-07-05 02:48:18,846 - INFO  [feature-zk-node-event-process-thread:Logging@66] - [feature-zk-node-event-process-thread]: Stopped
2022-07-05 02:48:18,846 - INFO  [main:Logging@66] - [feature-zk-node-event-process-thread]: Shutdown completed
2022-07-05 02:48:18,846 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Closing.
2022-07-05 02:48:18,949 - INFO  [main:ZooKeeper@1422] - Session: 0x100a462f8620000 closed
2022-07-05 02:48:18,949 - INFO  [main-EventThread:ClientCnxn$EventThread@524] - EventThread shut down for session: 0x100a462f8620000
2022-07-05 02:48:18,950 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Closed.
2022-07-05 02:48:18,950 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Fetch]: Shutting down
2022-07-05 02:48:19,234 - INFO  [ThrottledChannelReaper-Fetch:Logging@66] - [ThrottledChannelReaper-Fetch]: Stopped
2022-07-05 02:48:19,234 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Fetch]: Shutdown completed
2022-07-05 02:48:19,235 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Produce]: Shutting down
2022-07-05 02:48:19,235 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Produce]: Shutdown completed
2022-07-05 02:48:19,235 - INFO  [ThrottledChannelReaper-Produce:Logging@66] - [ThrottledChannelReaper-Produce]: Stopped
2022-07-05 02:48:19,235 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Request]: Shutting down
2022-07-05 02:48:19,236 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Request]: Shutdown completed
2022-07-05 02:48:19,236 - INFO  [ThrottledChannelReaper-Request:Logging@66] - [ThrottledChannelReaper-Request]: Stopped
2022-07-05 02:48:19,236 - INFO  [main:Logging@66] - [ThrottledChannelReaper-ControllerMutation]: Shutting down
2022-07-05 02:48:19,237 - INFO  [ThrottledChannelReaper-ControllerMutation:Logging@66] - [ThrottledChannelReaper-ControllerMutation]: Stopped
2022-07-05 02:48:19,237 - INFO  [main:Logging@66] - [ThrottledChannelReaper-ControllerMutation]: Shutdown completed
2022-07-05 02:48:19,238 - INFO  [main:Logging@66] - [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutting down socket server
2022-07-05 02:48:19,257 - INFO  [main:Logging@66] - [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutdown completed
2022-07-05 02:48:19,258 - INFO  [main:Metrics@659] - Metrics scheduler closed
2022-07-05 02:48:19,258 - INFO  [main:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:48:19,258 - INFO  [main:Metrics@669] - Metrics reporters closed
2022-07-05 02:48:19,260 - INFO  [main:Logging@66] - Broker and topic stats closed
2022-07-05 02:48:19,260 - INFO  [main:AppInfoParser@83] - App info kafka.server for 0 unregistered
2022-07-05 02:48:19,261 - INFO  [main:Logging@66] - [KafkaServer id=0] shut down completed
2022-07-05 02:48:19,265 - INFO  [ConnnectionExpirer:NIOServerCnxnFactory$ConnectionExpirerThread@583] - ConnnectionExpirerThread interrupted
2022-07-05 02:48:19,266 - INFO  [NIOServerCxnFactory.AcceptThread:/127.0.0.1:0:NIOServerCnxnFactory$AcceptThread@219] - accept thread exitted run method
2022-07-05 02:48:19,266 - INFO  [NIOServerCxnFactory.SelectorThread-1:NIOServerCnxnFactory$SelectorThread@420] - selector thread exitted run method
2022-07-05 02:48:19,266 - INFO  [NIOServerCxnFactory.SelectorThread-0:NIOServerCnxnFactory$SelectorThread@420] - selector thread exitted run method
2022-07-05 02:48:19,266 - INFO  [NIOServerCxnFactory.SelectorThread-2:NIOServerCnxnFactory$SelectorThread@420] - selector thread exitted run method
2022-07-05 02:48:19,266 - INFO  [main:ZooKeeperServer@573] - shutting down
2022-07-05 02:48:19,266 - INFO  [main:SessionTrackerImpl@237] - Shutting down
2022-07-05 02:48:19,267 - INFO  [main:PrepRequestProcessor@1008] - Shutting down
2022-07-05 02:48:19,267 - INFO  [main:SyncRequestProcessor@191] - Shutting down
2022-07-05 02:48:19,267 - INFO  [ProcessThread(sid:0 cport:37995)::PrepRequestProcessor@156] - PrepRequestProcessor exited loop!
2022-07-05 02:48:19,267 - INFO  [SyncThread:0:SyncRequestProcessor@169] - SyncRequestProcessor exited!
2022-07-05 02:48:19,267 - INFO  [main:FinalRequestProcessor@514] - shutdown of request processor complete
2022-07-05 02:55:49,959 - INFO  [main:Log4jControllerRegistration$@31] - Registered kafka:type=kafka.Log4jController MBean
2022-07-05 02:55:50,008 - INFO  [main:Environment@109] - Server environment:zookeeper.version=3.5.9-83df9301aa5c2a5d284a9940177808c01bc35cef, built on 01/06/2021 20:03 GMT
2022-07-05 02:55:50,008 - INFO  [main:Environment@109] - Server environment:host.name=razor15
2022-07-05 02:55:50,008 - INFO  [main:Environment@109] - Server environment:java.version=1.8.0_275
2022-07-05 02:55:50,008 - INFO  [main:Environment@109] - Server environment:java.vendor=Private Build
2022-07-05 02:55:50,008 - INFO  [main:Environment@109] - Server environment:java.home=/usr/lib/jvm/java-8-openjdk-amd64/jre
2022-07-05 02:55:50,009 - INFO  [main:Environment@109] - Server environment:java.class.path=/home/tonypan/flaky-reproduction/experiment/kafka-12508/junit-platform-console-standalone-1.7.0.jar
2022-07-05 02:55:50,009 - INFO  [main:Environment@109] - Server environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib
2022-07-05 02:55:50,009 - INFO  [main:Environment@109] - Server environment:java.io.tmpdir=/tmp
2022-07-05 02:55:50,009 - INFO  [main:Environment@109] - Server environment:java.compiler=<NA>
2022-07-05 02:55:50,009 - INFO  [main:Environment@109] - Server environment:os.name=Linux
2022-07-05 02:55:50,009 - INFO  [main:Environment@109] - Server environment:os.arch=amd64
2022-07-05 02:55:50,009 - INFO  [main:Environment@109] - Server environment:os.version=4.15.0-128-generic
2022-07-05 02:55:50,009 - INFO  [main:Environment@109] - Server environment:user.name=tonypan
2022-07-05 02:55:50,009 - INFO  [main:Environment@109] - Server environment:user.home=/home/tonypan
2022-07-05 02:55:50,010 - INFO  [main:Environment@109] - Server environment:user.dir=/home/tonypan/flaky-reproduction/experiment/kafka-12508
2022-07-05 02:55:50,010 - INFO  [main:Environment@109] - Server environment:os.memory.free=402MB
2022-07-05 02:55:50,011 - INFO  [main:Environment@109] - Server environment:os.memory.max=7051MB
2022-07-05 02:55:50,011 - INFO  [main:Environment@109] - Server environment:os.memory.total=475MB
2022-07-05 02:55:50,016 - INFO  [main:FileTxnSnapLog@115] - zookeeper.snapshot.trust.empty : false
2022-07-05 02:55:50,041 - INFO  [main:ZKDatabase@117] - zookeeper.snapshotSizeFactor = 0.33
2022-07-05 02:55:50,048 - INFO  [main:ZooKeeperServer@953] - minSessionTimeout set to 1600
2022-07-05 02:55:50,048 - INFO  [main:ZooKeeperServer@962] - maxSessionTimeout set to 16000
2022-07-05 02:55:50,049 - INFO  [main:ZooKeeperServer@181] - Created server with tickTime 800 minSessionTimeout 1600 maxSessionTimeout 16000 datadir /tmp/kafka-9144782362241573084/version-2 snapdir /tmp/kafka-4591034836704425464/version-2
2022-07-05 02:55:50,061 - INFO  [main:NIOServerCnxnFactory@673] - Configuring NIO connection handler with 10s sessionless connection timeout, 3 selector thread(s), 40 worker threads, and 64 kB direct buffers.
2022-07-05 02:55:50,068 - INFO  [main:NIOServerCnxnFactory@686] - binding to port /127.0.0.1:0
2022-07-05 02:55:50,077 - INFO  [main:FileTxnSnapLog@404] - Snapshotting: 0x0 to /tmp/kafka-4591034836704425464/version-2/snapshot.0
2022-07-05 02:55:50,081 - INFO  [main:FileTxnSnapLog@404] - Snapshotting: 0x0 to /tmp/kafka-4591034836704425464/version-2/snapshot.0
2022-07-05 02:55:50,107 - INFO  [ProcessThread(sid:0 cport:41833)::PrepRequestProcessor@132] - PrepRequestProcessor (sid:0) started, reconfigEnabled=false
2022-07-05 02:55:50,441 - INFO  [main:AbstractConfig@372] - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 0
	host.name = localhost
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.8-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/junit8105028921469205202/junit4780392996165565763
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.8-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000000
	metadata.log.dir = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = -1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 5
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:41833
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 10000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

2022-07-05 02:55:50,462 - INFO  [main:X509Util@79] - Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation
2022-07-05 02:55:50,554 - INFO  [main:Logging@66] - starting
2022-07-05 02:55:50,555 - INFO  [main:Logging@66] - Connecting to zookeeper on 127.0.0.1:41833
2022-07-05 02:55:50,578 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:41833.
2022-07-05 02:55:50,585 - INFO  [main:Environment@109] - Client environment:zookeeper.version=3.5.9-83df9301aa5c2a5d284a9940177808c01bc35cef, built on 01/06/2021 20:03 GMT
2022-07-05 02:55:50,585 - INFO  [main:Environment@109] - Client environment:host.name=razor15
2022-07-05 02:55:50,585 - INFO  [main:Environment@109] - Client environment:java.version=1.8.0_275
2022-07-05 02:55:50,585 - INFO  [main:Environment@109] - Client environment:java.vendor=Private Build
2022-07-05 02:55:50,585 - INFO  [main:Environment@109] - Client environment:java.home=/usr/lib/jvm/java-8-openjdk-amd64/jre
2022-07-05 02:55:50,585 - INFO  [main:Environment@109] - Client environment:java.class.path=/home/tonypan/flaky-reproduction/experiment/kafka-12508/junit-platform-console-standalone-1.7.0.jar
2022-07-05 02:55:50,585 - INFO  [main:Environment@109] - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib
2022-07-05 02:55:50,585 - INFO  [main:Environment@109] - Client environment:java.io.tmpdir=/tmp
2022-07-05 02:55:50,586 - INFO  [main:Environment@109] - Client environment:java.compiler=<NA>
2022-07-05 02:55:50,586 - INFO  [main:Environment@109] - Client environment:os.name=Linux
2022-07-05 02:55:50,586 - INFO  [main:Environment@109] - Client environment:os.arch=amd64
2022-07-05 02:55:50,586 - INFO  [main:Environment@109] - Client environment:os.version=4.15.0-128-generic
2022-07-05 02:55:50,586 - INFO  [main:Environment@109] - Client environment:user.name=tonypan
2022-07-05 02:55:50,586 - INFO  [main:Environment@109] - Client environment:user.home=/home/tonypan
2022-07-05 02:55:50,586 - INFO  [main:Environment@109] - Client environment:user.dir=/home/tonypan/flaky-reproduction/experiment/kafka-12508
2022-07-05 02:55:50,586 - INFO  [main:Environment@109] - Client environment:os.memory.free=336MB
2022-07-05 02:55:50,586 - INFO  [main:Environment@109] - Client environment:os.memory.max=7051MB
2022-07-05 02:55:50,586 - INFO  [main:Environment@109] - Client environment:os.memory.total=442MB
2022-07-05 02:55:50,591 - INFO  [main:ZooKeeper@868] - Initiating client connection, connectString=127.0.0.1:41833 sessionTimeout=10000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@31add175
2022-07-05 02:55:50,596 - INFO  [main:ClientCnxnSocket@237] - jute.maxbuffer value is 4194304 Bytes
2022-07-05 02:55:50,603 - INFO  [main:ClientCnxn@1653] - zookeeper.request.timeout value is 0. feature enabled=
2022-07-05 02:55:50,605 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Waiting until connected.
2022-07-05 02:55:50,611 - INFO  [main-SendThread(127.0.0.1:41833):ClientCnxn$SendThread@1112] - Opening socket connection to server localhost/127.0.0.1:41833. Will not attempt to authenticate using SASL (unknown error)
2022-07-05 02:55:50,613 - INFO  [main-SendThread(127.0.0.1:41833):ClientCnxn$SendThread@959] - Socket connection established, initiating session, client: /127.0.0.1:58688, server: localhost/127.0.0.1:41833
2022-07-05 02:55:50,623 - INFO  [SyncThread:0:FileTxnLog@218] - Creating new log file: log.1
2022-07-05 02:55:50,632 - INFO  [main-SendThread(127.0.0.1:41833):ClientCnxn$SendThread@1394] - Session establishment complete on server localhost/127.0.0.1:41833, sessionid = 0x100a46ad7b20000, negotiated timeout = 10000
2022-07-05 02:55:50,638 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Connected.
2022-07-05 02:55:50,749 - INFO  [feature-zk-node-event-process-thread:Logging@66] - [feature-zk-node-event-process-thread]: Starting
2022-07-05 02:55:50,762 - INFO  [feature-zk-node-event-process-thread:Logging@66] - Feature ZK node at path: /feature does not exist
2022-07-05 02:55:50,763 - INFO  [feature-zk-node-event-process-thread:Logging@66] - Cleared cache
2022-07-05 02:55:51,043 - INFO  [main:Logging@66] - Cluster ID = -AaV8s_dTk699eQPq24CCQ
2022-07-05 02:55:51,047 - WARN  [main:Logging@70] - No meta.properties file under dir /tmp/junit8105028921469205202/junit4780392996165565763/meta.properties
2022-07-05 02:55:51,109 - INFO  [main:AbstractConfig@372] - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 0
	host.name = localhost
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.8-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/junit8105028921469205202/junit4780392996165565763
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.8-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000000
	metadata.log.dir = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = -1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 5
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:41833
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 10000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

2022-07-05 02:55:51,121 - INFO  [main:AbstractConfig@372] - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 0
	host.name = localhost
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.8-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/junit8105028921469205202/junit4780392996165565763
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.8-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000000
	metadata.log.dir = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = -1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 5
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:41833
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 10000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

2022-07-05 02:55:51,158 - INFO  [ThrottledChannelReaper-Fetch:Logging@66] - [ThrottledChannelReaper-Fetch]: Starting
2022-07-05 02:55:51,159 - INFO  [ThrottledChannelReaper-Produce:Logging@66] - [ThrottledChannelReaper-Produce]: Starting
2022-07-05 02:55:51,160 - INFO  [ThrottledChannelReaper-Request:Logging@66] - [ThrottledChannelReaper-Request]: Starting
2022-07-05 02:55:51,162 - INFO  [ThrottledChannelReaper-ControllerMutation:Logging@66] - [ThrottledChannelReaper-ControllerMutation]: Starting
2022-07-05 02:55:51,193 - INFO  [main:Logging@66] - Loading logs from log dirs ArraySeq(/tmp/junit8105028921469205202/junit4780392996165565763)
2022-07-05 02:55:51,197 - INFO  [main:Logging@66] - Attempting recovery for all logs in /tmp/junit8105028921469205202/junit4780392996165565763 since no clean shutdown file was found
2022-07-05 02:55:51,202 - INFO  [main:Logging@66] - Loaded 0 logs in 0ms.
2022-07-05 02:55:51,202 - INFO  [main:Logging@66] - Starting log cleanup with a period of 300000 ms.
2022-07-05 02:55:51,205 - INFO  [main:Logging@66] - Starting log flusher with a default period of 9223372036854775807 ms.
2022-07-05 02:55:51,598 - INFO  [main:Logging@66] - Updated connection-accept-rate max connection creation rate to 2147483647
2022-07-05 02:55:51,601 - INFO  [main:Logging@66] - Awaiting socket connections on localhost:34343.
2022-07-05 02:55:51,640 - INFO  [main:Logging@66] - [SocketServer listenerType=ZK_BROKER, nodeId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT)
2022-07-05 02:55:51,668 - INFO  [BrokerToControllerChannelManager broker=0 name=alterIsr:Logging@66] - [BrokerToControllerChannelManager broker=0 name=alterIsr]: Starting
2022-07-05 02:55:51,690 - INFO  [ExpirationReaper-0-Produce:Logging@66] - [ExpirationReaper-0-Produce]: Starting
2022-07-05 02:55:51,691 - INFO  [ExpirationReaper-0-Fetch:Logging@66] - [ExpirationReaper-0-Fetch]: Starting
2022-07-05 02:55:51,691 - INFO  [ExpirationReaper-0-DeleteRecords:Logging@66] - [ExpirationReaper-0-DeleteRecords]: Starting
2022-07-05 02:55:51,691 - INFO  [ExpirationReaper-0-ElectLeader:Logging@66] - [ExpirationReaper-0-ElectLeader]: Starting
2022-07-05 02:55:51,705 - INFO  [LogDirFailureHandler:Logging@66] - [LogDirFailureHandler]: Starting
2022-07-05 02:55:51,746 - INFO  [main:Logging@66] - Creating /brokers/ids/0 (is it secure? false)
2022-07-05 02:55:51,773 - INFO  [main:Logging@66] - Stat of the created znode at /brokers/ids/0 is: 25,25,1657004151766,1657004151766,1,0,0,72238372830183424,204,0,25

2022-07-05 02:55:51,774 - INFO  [main:Logging@66] - Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://localhost:34343, czxid (broker epoch): 25
2022-07-05 02:55:51,837 - INFO  [ExpirationReaper-0-topic:Logging@66] - [ExpirationReaper-0-topic]: Starting
2022-07-05 02:55:51,842 - INFO  [ExpirationReaper-0-Heartbeat:Logging@66] - [ExpirationReaper-0-Heartbeat]: Starting
2022-07-05 02:55:51,843 - INFO  [ExpirationReaper-0-Rebalance:Logging@66] - [ExpirationReaper-0-Rebalance]: Starting
2022-07-05 02:55:51,845 - INFO  [controller-event-thread:Logging@66] - Successfully created /controller_epoch with initial epoch 0
2022-07-05 02:55:51,856 - INFO  [main-EventThread:Logging@66] - Feature ZK node created at path: /feature
2022-07-05 02:55:51,859 - INFO  [main:Logging@66] - [GroupCoordinator 0]: Starting up.
2022-07-05 02:55:51,863 - INFO  [main:Logging@66] - [GroupCoordinator 0]: Startup complete.
2022-07-05 02:55:51,886 - INFO  [main:Logging@66] - [ProducerId Manager 0]: Acquired new producerId block (brokerId:0,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1
2022-07-05 02:55:51,887 - INFO  [main:Logging@66] - [TransactionCoordinator id=0] Starting up.
2022-07-05 02:55:51,889 - INFO  [TxnMarkerSenderThread-0:Logging@66] - [Transaction Marker Channel Manager 0]: Starting
2022-07-05 02:55:51,889 - INFO  [main:Logging@66] - [TransactionCoordinator id=0] Startup complete.
2022-07-05 02:55:51,891 - INFO  [feature-zk-node-event-process-thread:Logging@66] - Updated cache from existing <empty> to latest FinalizedFeaturesAndEpoch(features=Features{}, epoch=0).
2022-07-05 02:55:51,914 - INFO  [ExpirationReaper-0-AlterAcls:Logging@66] - [ExpirationReaper-0-AlterAcls]: Starting
2022-07-05 02:55:51,941 - INFO  [/config/changes-event-process-thread:Logging@66] - [/config/changes-event-process-thread]: Starting
2022-07-05 02:55:51,947 - INFO  [main:Logging@66] - [SocketServer listenerType=ZK_BROKER, nodeId=0] Starting socket server acceptors and processors
2022-07-05 02:55:51,951 - INFO  [main:Logging@66] - [SocketServer listenerType=ZK_BROKER, nodeId=0] Started data-plane acceptor and processor(s) for endpoint : ListenerName(PLAINTEXT)
2022-07-05 02:55:51,952 - INFO  [main:Logging@66] - [SocketServer listenerType=ZK_BROKER, nodeId=0] Started socket server acceptors and processors
2022-07-05 02:55:51,953 - WARN  [main:AppInfoParser@46] - Error while loading kafka-version.properties: null
2022-07-05 02:55:51,954 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2022-07-05 02:55:51,954 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2022-07-05 02:55:51,954 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1657004149771
2022-07-05 02:55:51,955 - INFO  [main:Logging@66] - [KafkaServer id=0] started
2022-07-05 02:55:51,972 - INFO  [main:AbstractConfig@372] - AdminClientConfig values: 
	bootstrap.servers = [localhost:34343]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2022-07-05 02:55:51,994 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2022-07-05 02:55:51,994 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2022-07-05 02:55:51,994 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1657004151994
2022-07-05 02:55:52,053 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Creating topic inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0))
2022-07-05 02:55:52,078 - INFO  [BrokerToControllerChannelManager broker=0 name=alterIsr:Logging@66] - [BrokerToControllerChannelManager broker=0 name=alterIsr]: Recorded new controller, from now on will use broker localhost:34343 (id: 0 rack: null)
2022-07-05 02:55:52,125 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0)
2022-07-05 02:55:52,193 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0, dir=/tmp/junit8105028921469205202/junit4780392996165565763] Loading producer state till offset 0 with message format version 2
2022-07-05 02:55:52,197 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 in /tmp/junit8105028921469205202/junit4780392996165565763/inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.8-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2022-07-05 02:55:52,199 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 broker=0] No checkpointed highwatermark is found for partition inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0
2022-07-05 02:55:52,201 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 broker=0] Log loaded for partition inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 with initial high watermark 0
2022-07-05 02:55:52,228 - INFO  [kafka-admin-client-thread | adminclient-1:AppInfoParser@83] - App info kafka.admin.client for adminclient-1 unregistered
2022-07-05 02:55:52,230 - INFO  [kafka-admin-client-thread | adminclient-1:Metrics@659] - Metrics scheduler closed
2022-07-05 02:55:52,230 - INFO  [kafka-admin-client-thread | adminclient-1:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:55:52,231 - INFO  [kafka-admin-client-thread | adminclient-1:Metrics@669] - Metrics reporters closed
2022-07-05 02:55:52,233 - INFO  [main:AbstractConfig@372] - AdminClientConfig values: 
	bootstrap.servers = [localhost:34343]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2022-07-05 02:55:52,235 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2022-07-05 02:55:52,236 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2022-07-05 02:55:52,236 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1657004152235
2022-07-05 02:55:52,247 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Creating topic outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0))
2022-07-05 02:55:52,257 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0)
2022-07-05 02:55:52,260 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Log partition=outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0, dir=/tmp/junit8105028921469205202/junit4780392996165565763] Loading producer state till offset 0 with message format version 2
2022-07-05 02:55:52,261 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Created log for partition outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 in /tmp/junit8105028921469205202/junit4780392996165565763/outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.8-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2022-07-05 02:55:52,261 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 broker=0] No checkpointed highwatermark is found for partition outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0
2022-07-05 02:55:52,261 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 broker=0] Log loaded for partition outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 with initial high watermark 0
2022-07-05 02:55:52,265 - INFO  [kafka-admin-client-thread | adminclient-2:AppInfoParser@83] - App info kafka.admin.client for adminclient-2 unregistered
2022-07-05 02:55:52,266 - INFO  [kafka-admin-client-thread | adminclient-2:Metrics@659] - Metrics scheduler closed
2022-07-05 02:55:52,266 - INFO  [kafka-admin-client-thread | adminclient-2:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:55:52,266 - INFO  [kafka-admin-client-thread | adminclient-2:Metrics@669] - Metrics reporters closed
2022-07-05 02:55:52,321 - INFO  [main:AbstractConfig@372] - StreamsConfig values: 
	acceptable.recovery.lag = 10000
	application.id = appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover
	application.server = 
	bootstrap.servers = [localhost:34343]
	buffered.records.per.partition = 1000
	built.in.metrics.version = latest
	cache.max.bytes.buffering = 0
	client.id = 
	commit.interval.ms = 300000
	connections.max.idle.ms = 540000
	default.deserialization.exception.handler = class org.apache.kafka.streams.errors.LogAndFailExceptionHandler
	default.key.serde = class org.apache.kafka.common.serialization.Serdes$IntegerSerde
	default.production.exception.handler = class org.apache.kafka.streams.errors.DefaultProductionExceptionHandler
	default.timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	default.value.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	default.windowed.key.serde.inner = null
	default.windowed.value.serde.inner = null
	max.task.idle.ms = 0
	max.warmup.replicas = 2
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	probing.rebalance.interval.ms = 600000
	processing.guarantee = at_least_once
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retries = 0
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 600000
	state.dir = /tmp/kafka-5409862009541855416
	task.timeout.ms = 300000
	topology.optimization = none
	upgrade.from = null
	window.size.ms = null
	windowstore.changelog.additional.retention.ms = 86400000

2022-07-05 02:55:52,351 - WARN  [main:StateDirectory@138] - Using an OS temp directory in the state.dir property can cause failures with writing the checkpoint file due to the fact that this directory can be cleared by the OS. Resolved state.dir: [/tmp/kafka-5409862009541855416]
2022-07-05 02:55:52,353 - INFO  [main:StateDirectory@212] - No process id found on disk, got fresh process id 2738b082-65ee-4915-a848-e3e22f48498c
2022-07-05 02:55:52,386 - INFO  [main:AbstractConfig@372] - AdminClientConfig values: 
	bootstrap.servers = [localhost:34343]
	client.dns.lookup = use_all_dns_ips
	client.id = appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-admin
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2022-07-05 02:55:52,389 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2022-07-05 02:55:52,389 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2022-07-05 02:55:52,389 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1657004152389
2022-07-05 02:55:52,392 - WARN  [main:ClientMetrics@55] - Error while loading kafka-streams-version.properties
java.lang.NullPointerException
	at java.util.Properties$LineReader.readLine(Properties.java:434)
	at java.util.Properties.load0(Properties.java:353)
	at java.util.Properties.load(Properties.java:341)
	at org.apache.kafka.streams.internals.metrics.ClientMetrics.<clinit>(ClientMetrics.java:53)
	at org.apache.kafka.streams.KafkaStreams.<init>(KafkaStreams.java:825)
	at org.apache.kafka.streams.KafkaStreams.<init>(KafkaStreams.java:781)
	at org.apache.kafka.streams.KafkaStreams.<init>(KafkaStreams.java:691)
	at org.apache.kafka.streams.integration.EmitOnChangeIntegrationTest.shouldEmitSameRecordAfterFailover(EmitOnChangeIntegrationTest.java:106)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
2022-07-05 02:55:52,395 - INFO  [main:KafkaStreams@825] - stream-client [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c] Kafka Streams version: unknown
2022-07-05 02:55:52,395 - INFO  [main:KafkaStreams@826] - stream-client [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c] Kafka Streams commit ID: unknown
2022-07-05 02:55:52,404 - INFO  [main:StreamThread@337] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1] Creating restore consumer client
2022-07-05 02:55:52,409 - INFO  [main:AbstractConfig@372] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [localhost:34343]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1-restore-consumer
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = false
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2022-07-05 02:55:52,429 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2022-07-05 02:55:52,430 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2022-07-05 02:55:52,430 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1657004152429
2022-07-05 02:55:52,436 - INFO  [main:ActiveTaskCreator@96] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1] Creating thread producer client
2022-07-05 02:55:52,440 - INFO  [main:AbstractConfig@372] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:34343]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-07-05 02:55:52,453 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2022-07-05 02:55:52,453 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2022-07-05 02:55:52,453 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1657004152453
2022-07-05 02:55:52,459 - INFO  [main:StreamThread@389] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1] Creating consumer client
2022-07-05 02:55:52,461 - INFO  [kafka-producer-network-thread | appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1-producer:Metadata@279] - [Producer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1-producer] Cluster ID: -AaV8s_dTk699eQPq24CCQ
2022-07-05 02:55:52,461 - INFO  [main:AbstractConfig@372] - ConsumerConfig values: 
	allow.auto.create.topics = false
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:34343]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1-consumer
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = false
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2022-07-05 02:55:52,472 - INFO  [main:AssignorConfiguration@114] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1-consumer] Cooperative rebalancing enabled now
2022-07-05 02:55:52,484 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2022-07-05 02:55:52,485 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2022-07-05 02:55:52,485 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1657004152484
2022-07-05 02:55:52,491 - INFO  [main:KafkaStreams@321] - stream-client [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c] State transition from CREATED to REBALANCING
2022-07-05 02:55:52,492 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:StreamThread@550] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1] Starting
2022-07-05 02:55:52,492 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:StreamThread@230] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1] State transition from CREATED to STARTING
2022-07-05 02:55:52,492 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:KafkaConsumer@968] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Subscribed to topic(s): inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover
2022-07-05 02:55:52,506 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:Metadata@279] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Cluster ID: -AaV8s_dTk699eQPq24CCQ
2022-07-05 02:55:52,507 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Creating topic __consumer_offsets with configuration {segment.bytes=104857600, cleanup.policy=compact, compression.type=producer} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0))
2022-07-05 02:55:52,527 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(__consumer_offsets-4, __consumer_offsets-3, __consumer_offsets-2, __consumer_offsets-0, __consumer_offsets-1)
2022-07-05 02:55:52,530 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-3, dir=/tmp/junit8105028921469205202/junit4780392996165565763] Loading producer state till offset 0 with message format version 2
2022-07-05 02:55:52,531 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-3 in /tmp/junit8105028921469205202/junit4780392996165565763/__consumer_offsets-3 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.8-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2022-07-05 02:55:52,531 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3
2022-07-05 02:55:52,531 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-3 broker=0] Log loaded for partition __consumer_offsets-3 with initial high watermark 0
2022-07-05 02:55:52,535 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-2, dir=/tmp/junit8105028921469205202/junit4780392996165565763] Loading producer state till offset 0 with message format version 2
2022-07-05 02:55:52,535 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-2 in /tmp/junit8105028921469205202/junit4780392996165565763/__consumer_offsets-2 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.8-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2022-07-05 02:55:52,535 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2
2022-07-05 02:55:52,536 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-2 broker=0] Log loaded for partition __consumer_offsets-2 with initial high watermark 0
2022-07-05 02:55:52,543 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-4, dir=/tmp/junit8105028921469205202/junit4780392996165565763] Loading producer state till offset 0 with message format version 2
2022-07-05 02:55:52,544 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-4 in /tmp/junit8105028921469205202/junit4780392996165565763/__consumer_offsets-4 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.8-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2022-07-05 02:55:52,544 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4
2022-07-05 02:55:52,544 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-4 broker=0] Log loaded for partition __consumer_offsets-4 with initial high watermark 0
2022-07-05 02:55:52,551 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-1, dir=/tmp/junit8105028921469205202/junit4780392996165565763] Loading producer state till offset 0 with message format version 2
2022-07-05 02:55:52,552 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-1 in /tmp/junit8105028921469205202/junit4780392996165565763/__consumer_offsets-1 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.8-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2022-07-05 02:55:52,552 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1
2022-07-05 02:55:52,552 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-1 broker=0] Log loaded for partition __consumer_offsets-1 with initial high watermark 0
2022-07-05 02:55:52,560 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-0, dir=/tmp/junit8105028921469205202/junit4780392996165565763] Loading producer state till offset 0 with message format version 2
2022-07-05 02:55:52,560 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-0 in /tmp/junit8105028921469205202/junit4780392996165565763/__consumer_offsets-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.8-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2022-07-05 02:55:52,560 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0
2022-07-05 02:55:52,560 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-0 broker=0] Log loaded for partition __consumer_offsets-0 with initial high watermark 0
2022-07-05 02:55:52,567 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Elected as the group coordinator for partition 3
2022-07-05 02:55:52,567 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3
2022-07-05 02:55:52,568 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Elected as the group coordinator for partition 2
2022-07-05 02:55:52,568 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2
2022-07-05 02:55:52,568 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Elected as the group coordinator for partition 4
2022-07-05 02:55:52,568 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4
2022-07-05 02:55:52,569 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Elected as the group coordinator for partition 1
2022-07-05 02:55:52,569 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1
2022-07-05 02:55:52,569 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Elected as the group coordinator for partition 0
2022-07-05 02:55:52,569 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0
2022-07-05 02:55:52,572 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 5 milliseconds, of which 1 milliseconds was spent in the scheduler.
2022-07-05 02:55:52,572 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 4 milliseconds, of which 4 milliseconds was spent in the scheduler.
2022-07-05 02:55:52,572 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 4 milliseconds, of which 4 milliseconds was spent in the scheduler.
2022-07-05 02:55:52,573 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 4 milliseconds, of which 3 milliseconds was spent in the scheduler.
2022-07-05 02:55:52,573 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 4 milliseconds, of which 4 milliseconds was spent in the scheduler.
2022-07-05 02:55:52,596 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:StreamThread@829] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update
2022-07-05 02:55:52,601 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:AbstractCoordinator$FindCoordinatorResponseHandler@844] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Discovered group coordinator localhost:34343 (id: 2147483647 rack: null)
2022-07-05 02:55:52,603 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:AbstractCoordinator@534] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] (Re-)joining group
2022-07-05 02:55:52,622 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [GroupCoordinator 0]: Dynamic member with unknown member id joins group appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover in Empty state. Created a new member id appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1-consumer-a6b89ab6-3b34-43d7-91c3-fd8a57735ff8 and request the member to rejoin with this id.
2022-07-05 02:55:52,626 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:AbstractCoordinator@982] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Request joining group due to: need to re-join with the given member-id
2022-07-05 02:55:52,627 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:AbstractCoordinator@534] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] (Re-)joining group
2022-07-05 02:55:52,631 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover in state PreparingRebalance with old generation 0 (__consumer_offsets-0) (reason: Adding new member appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1-consumer-a6b89ab6-3b34-43d7-91c3-fd8a57735ff8 with group instance id None)
2022-07-05 02:55:52,636 - INFO  [executor-Rebalance:Logging@66] - [GroupCoordinator 0]: Stabilized group appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover generation 1 (__consumer_offsets-0) with 1 members
2022-07-05 02:55:52,638 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Successfully joined group with generation Generation{generationId=1, memberId='appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1-consumer-a6b89ab6-3b34-43d7-91c3-fd8a57735ff8', protocol='stream'}
2022-07-05 02:55:52,655 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Creating topic appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog with configuration {message.timestamp.type=CreateTime, cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0))
2022-07-05 02:55:52,665 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0)
2022-07-05 02:55:52,668 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0, dir=/tmp/junit8105028921469205202/junit4780392996165565763] Loading producer state till offset 0 with message format version 2
2022-07-05 02:55:52,669 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0 in /tmp/junit8105028921469205202/junit4780392996165565763/appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.8-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2022-07-05 02:55:52,669 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0 broker=0] No checkpointed highwatermark is found for partition appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0
2022-07-05 02:55:52,669 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0 broker=0] Log loaded for partition appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0 with initial high watermark 0
2022-07-05 02:55:52,681 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:HighAvailabilityTaskAssignor@95] - Decided on assignment: {2738b082-65ee-4915-a848-e3e22f48498c=[activeTasks: ([0_0]) standbyTasks: ([]) prevActiveTasks: ([]) prevStandbyTasks: ([]) changelogOffsetTotalsByTask: ([]) taskLagTotals: ([0_0=0]) capacity: 1 assigned: 1]} with no followup probing rebalance.
2022-07-05 02:55:52,682 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:StreamsPartitionAssignor@595] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1-consumer] Assigned tasks [0_0] including stateful [0_0] to clients as: 
2738b082-65ee-4915-a848-e3e22f48498c=[activeTasks: ([0_0]) standbyTasks: ([])].
2022-07-05 02:55:52,686 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:StreamsPartitionAssignor@803] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1-consumer] Client 2738b082-65ee-4915-a848-e3e22f48498c per-consumer assignment:
	prev owned active {}
	prev owned standby {appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1-consumer-a6b89ab6-3b34-43d7-91c3-fd8a57735ff8=[]}
	assigned active {appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1-consumer-a6b89ab6-3b34-43d7-91c3-fd8a57735ff8=[0_0]}
	revoking active {}
	assigned standby {}

2022-07-05 02:55:52,686 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:StreamsPartitionAssignor@822] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1-consumer] Finished stable assignment of tasks, no followup rebalances required.
2022-07-05 02:55:52,687 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:ConsumerCoordinator@626] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Finished assignment for group at generation 1: {appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1-consumer-a6b89ab6-3b34-43d7-91c3-fd8a57735ff8=Assignment(partitions=[inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0], userDataSize=48)}
2022-07-05 02:55:52,695 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover for generation 1. The group has 1 members, 0 of which are static.
2022-07-05 02:55:52,745 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:AbstractCoordinator$SyncGroupResponseHandler@756] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Successfully synced group in generation Generation{generationId=1, memberId='appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1-consumer-a6b89ab6-3b34-43d7-91c3-fd8a57735ff8', protocol='stream'}
2022-07-05 02:55:52,746 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:ConsumerCoordinator@392] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Updating assignment with
	Assigned partitions:                       [inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0]
	Current owned partitions:                  []
	Added partitions (assigned - owned):       [inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0]
	Revoked partitions (owned - assigned):     []

2022-07-05 02:55:52,746 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:ConsumerCoordinator@276] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Notifying assignor about the new Assignment(partitions=[inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0], userDataSize=48)
2022-07-05 02:55:52,746 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:StreamsPartitionAssignor@1306] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1-consumer] No followup rebalance was requested, resetting the rebalance schedule.
2022-07-05 02:55:52,748 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:TaskManager@254] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1] Handle new assignment with:
	New active tasks: [0_0]
	New standby tasks: []
	Existing active tasks: []
	Existing standby tasks: []
2022-07-05 02:55:52,762 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:ConsumerCoordinator@288] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Adding newly assigned partitions: inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0
2022-07-05 02:55:52,762 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:StreamThread@230] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1] State transition from STARTING to PARTITIONS_ASSIGNED
2022-07-05 02:55:52,772 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:ConsumerCoordinator$OffsetFetchResponseHandler@1352] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Found no committed offset for partition inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0
2022-07-05 02:55:52,785 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:SubscriptionState@398] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Resetting offset for partition inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34343 (id: 0 rack: null)], epoch=0}}.
2022-07-05 02:55:52,942 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:RocksDBTimestampedStore@100] - Opening store test-store in regular mode
2022-07-05 02:55:52,945 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:ProcessorStateManager@256] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1] task [0_0] State store test-store did not find checkpoint offset, hence would default to the starting offset at changelog appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0
2022-07-05 02:55:52,945 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:StreamTask@235] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1] task [0_0] Initialized
2022-07-05 02:55:52,951 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:KafkaConsumer@1123] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1-restore-consumer, groupId=null] Subscribed to partition(s): appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0
2022-07-05 02:55:52,952 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:SubscriptionState@619] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1-restore-consumer, groupId=null] Seeking to EARLIEST offset of partition appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0
2022-07-05 02:55:52,955 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:Metadata@279] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1-restore-consumer, groupId=null] Cluster ID: -AaV8s_dTk699eQPq24CCQ
2022-07-05 02:55:52,957 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:SubscriptionState@398] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1-restore-consumer, groupId=null] Resetting offset for partition appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34343 (id: 0 rack: null)], epoch=0}}.
2022-07-05 02:55:53,058 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:StoreChangelogReader@609] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1] Finished restoring changelog appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0 to store test-store with a total number of 0 records
2022-07-05 02:55:53,060 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:ConsumerCoordinator$OffsetFetchResponseHandler@1352] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Found no committed offset for partition inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0
2022-07-05 02:55:53,063 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:StreamTask@255] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1] task [0_0] Restored and ready to run
2022-07-05 02:55:53,064 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:StreamThread@853] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1] Restoration took 302 ms for all tasks [0_0]
2022-07-05 02:55:53,064 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:StreamThread@230] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1] State transition from PARTITIONS_ASSIGNED to RUNNING
2022-07-05 02:55:53,065 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:KafkaStreams@321] - stream-client [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c] State transition from REBALANCING to RUNNING
2022-07-05 02:55:53,067 - INFO  [main:AbstractConfig@372] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:34343]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.IntegerSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2022-07-05 02:55:53,069 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2022-07-05 02:55:53,069 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2022-07-05 02:55:53,069 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1657004153069
2022-07-05 02:55:53,072 - INFO  [kafka-producer-network-thread | producer-1:Metadata@279] - [Producer clientId=producer-1] Cluster ID: -AaV8s_dTk699eQPq24CCQ
2022-07-05 02:55:53,075 - INFO  [main:KafkaProducer@1204] - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2022-07-05 02:55:53,089 - INFO  [main:Metrics@659] - Metrics scheduler closed
2022-07-05 02:55:53,089 - INFO  [main:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:55:53,090 - INFO  [main:Metrics@669] - Metrics reporters closed
2022-07-05 02:55:53,090 - INFO  [main:AppInfoParser@83] - App info kafka.producer for producer-1 unregistered
2022-07-05 02:55:53,091 - INFO  [main:AbstractConfig@372] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:34343]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-19b5593b-6cca-42a7-93b9-003e83ac41f3-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 19b5593b-6cca-42a7-93b9-003e83ac41f3
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.IntegerDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2022-07-05 02:55:53,093 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2022-07-05 02:55:53,093 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2022-07-05 02:55:53,093 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1657004153093
2022-07-05 02:55:53,094 - INFO  [main:KafkaConsumer@968] - [Consumer clientId=consumer-19b5593b-6cca-42a7-93b9-003e83ac41f3-1, groupId=19b5593b-6cca-42a7-93b9-003e83ac41f3] Subscribed to topic(s): outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover
2022-07-05 02:55:53,097 - INFO  [main:Metadata@279] - [Consumer clientId=consumer-19b5593b-6cca-42a7-93b9-003e83ac41f3-1, groupId=19b5593b-6cca-42a7-93b9-003e83ac41f3] Cluster ID: -AaV8s_dTk699eQPq24CCQ
2022-07-05 02:55:53,098 - INFO  [main:AbstractCoordinator$FindCoordinatorResponseHandler@844] - [Consumer clientId=consumer-19b5593b-6cca-42a7-93b9-003e83ac41f3-1, groupId=19b5593b-6cca-42a7-93b9-003e83ac41f3] Discovered group coordinator localhost:34343 (id: 2147483647 rack: null)
2022-07-05 02:55:53,099 - INFO  [main:AbstractCoordinator@534] - [Consumer clientId=consumer-19b5593b-6cca-42a7-93b9-003e83ac41f3-1, groupId=19b5593b-6cca-42a7-93b9-003e83ac41f3] (Re-)joining group
2022-07-05 02:55:53,101 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupCoordinator 0]: Dynamic member with unknown member id joins group 19b5593b-6cca-42a7-93b9-003e83ac41f3 in Empty state. Created a new member id consumer-19b5593b-6cca-42a7-93b9-003e83ac41f3-1-f6c9156d-9495-4e2f-ac11-bdf298ab71ef and request the member to rejoin with this id.
2022-07-05 02:55:53,101 - INFO  [main:AbstractCoordinator@982] - [Consumer clientId=consumer-19b5593b-6cca-42a7-93b9-003e83ac41f3-1, groupId=19b5593b-6cca-42a7-93b9-003e83ac41f3] Request joining group due to: need to re-join with the given member-id
2022-07-05 02:55:53,102 - INFO  [main:AbstractCoordinator@534] - [Consumer clientId=consumer-19b5593b-6cca-42a7-93b9-003e83ac41f3-1, groupId=19b5593b-6cca-42a7-93b9-003e83ac41f3] (Re-)joining group
2022-07-05 02:55:53,102 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group 19b5593b-6cca-42a7-93b9-003e83ac41f3 in state PreparingRebalance with old generation 0 (__consumer_offsets-1) (reason: Adding new member consumer-19b5593b-6cca-42a7-93b9-003e83ac41f3-1-f6c9156d-9495-4e2f-ac11-bdf298ab71ef with group instance id None)
2022-07-05 02:55:53,103 - INFO  [executor-Rebalance:Logging@66] - [GroupCoordinator 0]: Stabilized group 19b5593b-6cca-42a7-93b9-003e83ac41f3 generation 1 (__consumer_offsets-1) with 1 members
2022-07-05 02:55:53,103 - INFO  [main:AbstractCoordinator$JoinGroupResponseHandler@590] - [Consumer clientId=consumer-19b5593b-6cca-42a7-93b9-003e83ac41f3-1, groupId=19b5593b-6cca-42a7-93b9-003e83ac41f3] Successfully joined group with generation Generation{generationId=1, memberId='consumer-19b5593b-6cca-42a7-93b9-003e83ac41f3-1-f6c9156d-9495-4e2f-ac11-bdf298ab71ef', protocol='range'}
2022-07-05 02:55:53,104 - INFO  [main:ConsumerCoordinator@626] - [Consumer clientId=consumer-19b5593b-6cca-42a7-93b9-003e83ac41f3-1, groupId=19b5593b-6cca-42a7-93b9-003e83ac41f3] Finished assignment for group at generation 1: {consumer-19b5593b-6cca-42a7-93b9-003e83ac41f3-1-f6c9156d-9495-4e2f-ac11-bdf298ab71ef=Assignment(partitions=[outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0])}
2022-07-05 02:55:53,105 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group 19b5593b-6cca-42a7-93b9-003e83ac41f3 for generation 1. The group has 1 members, 0 of which are static.
2022-07-05 02:55:53,106 - INFO  [main:AbstractCoordinator$SyncGroupResponseHandler@756] - [Consumer clientId=consumer-19b5593b-6cca-42a7-93b9-003e83ac41f3-1, groupId=19b5593b-6cca-42a7-93b9-003e83ac41f3] Successfully synced group in generation Generation{generationId=1, memberId='consumer-19b5593b-6cca-42a7-93b9-003e83ac41f3-1-f6c9156d-9495-4e2f-ac11-bdf298ab71ef', protocol='range'}
2022-07-05 02:55:53,107 - INFO  [main:ConsumerCoordinator@276] - [Consumer clientId=consumer-19b5593b-6cca-42a7-93b9-003e83ac41f3-1, groupId=19b5593b-6cca-42a7-93b9-003e83ac41f3] Notifying assignor about the new Assignment(partitions=[outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0])
2022-07-05 02:55:53,107 - INFO  [main:ConsumerCoordinator@288] - [Consumer clientId=consumer-19b5593b-6cca-42a7-93b9-003e83ac41f3-1, groupId=19b5593b-6cca-42a7-93b9-003e83ac41f3] Adding newly assigned partitions: outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0
2022-07-05 02:55:53,108 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1352] - [Consumer clientId=consumer-19b5593b-6cca-42a7-93b9-003e83ac41f3-1, groupId=19b5593b-6cca-42a7-93b9-003e83ac41f3] Found no committed offset for partition outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0
2022-07-05 02:55:53,111 - INFO  [main:SubscriptionState@398] - [Consumer clientId=consumer-19b5593b-6cca-42a7-93b9-003e83ac41f3-1, groupId=19b5593b-6cca-42a7-93b9-003e83ac41f3] Resetting offset for partition outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:34343 (id: 0 rack: null)], epoch=0}}.
2022-07-05 02:55:53,235 - INFO  [main:ConsumerCoordinator@307] - [Consumer clientId=consumer-19b5593b-6cca-42a7-93b9-003e83ac41f3-1, groupId=19b5593b-6cca-42a7-93b9-003e83ac41f3] Revoke previously assigned partitions outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0
2022-07-05 02:55:53,236 - INFO  [main:AbstractCoordinator@1038] - [Consumer clientId=consumer-19b5593b-6cca-42a7-93b9-003e83ac41f3-1, groupId=19b5593b-6cca-42a7-93b9-003e83ac41f3] Member consumer-19b5593b-6cca-42a7-93b9-003e83ac41f3-1-f6c9156d-9495-4e2f-ac11-bdf298ab71ef sending LeaveGroup request to coordinator localhost:34343 (id: 2147483647 rack: null) due to the consumer is being closed
2022-07-05 02:55:53,236 - INFO  [main:AbstractCoordinator@961] - [Consumer clientId=consumer-19b5593b-6cca-42a7-93b9-003e83ac41f3-1, groupId=19b5593b-6cca-42a7-93b9-003e83ac41f3] Resetting generation due to: consumer pro-actively leaving the group
2022-07-05 02:55:53,237 - INFO  [main:AbstractCoordinator@982] - [Consumer clientId=consumer-19b5593b-6cca-42a7-93b9-003e83ac41f3-1, groupId=19b5593b-6cca-42a7-93b9-003e83ac41f3] Request joining group due to: consumer pro-actively leaving the group
2022-07-05 02:55:53,239 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group 19b5593b-6cca-42a7-93b9-003e83ac41f3 in state PreparingRebalance with old generation 1 (__consumer_offsets-1) (reason: removing member consumer-19b5593b-6cca-42a7-93b9-003e83ac41f3-1-f6c9156d-9495-4e2f-ac11-bdf298ab71ef on LeaveGroup)
2022-07-05 02:55:53,240 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [GroupCoordinator 0]: Group 19b5593b-6cca-42a7-93b9-003e83ac41f3 with generation 2 is now empty (__consumer_offsets-1)
2022-07-05 02:55:53,242 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [GroupCoordinator 0]: Member MemberMetadata(memberId=consumer-19b5593b-6cca-42a7-93b9-003e83ac41f3-1-f6c9156d-9495-4e2f-ac11-bdf298ab71ef, groupInstanceId=None, clientId=consumer-19b5593b-6cca-42a7-93b9-003e83ac41f3-1, clientHost=/127.0.0.1, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(range)) has left group 19b5593b-6cca-42a7-93b9-003e83ac41f3 through explicit `LeaveGroup` request
2022-07-05 02:55:53,244 - INFO  [main:Metrics@659] - Metrics scheduler closed
2022-07-05 02:55:53,244 - INFO  [main:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:55:53,244 - INFO  [main:Metrics@669] - Metrics reporters closed
2022-07-05 02:55:53,245 - INFO  [main:AppInfoParser@83] - App info kafka.consumer for consumer-19b5593b-6cca-42a7-93b9-003e83ac41f3-1 unregistered
2022-07-05 02:55:53,246 - INFO  [main:KafkaStreams@321] - stream-client [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c] State transition from RUNNING to PENDING_SHUTDOWN
2022-07-05 02:55:53,247 - INFO  [kafka-streams-close-thread:StreamThread@1057] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1] Informed to shut down
2022-07-05 02:55:53,247 - INFO  [kafka-streams-close-thread:StreamThread@230] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1] State transition from RUNNING to PENDING_SHUTDOWN
2022-07-05 02:55:53,319 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:StreamThread@729] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1] Thread state is already PENDING_SHUTDOWN, skipping the run once call after poll request
2022-07-05 02:55:53,319 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:StreamThread@1071] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1] Shutting down
2022-07-05 02:55:53,342 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:StreamTask@1144] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1] task [0_0] Suspended RUNNING
2022-07-05 02:55:53,342 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:StreamTask@290] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1] task [0_0] Suspended running
2022-07-05 02:55:53,343 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:KafkaConsumer@1077] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions
2022-07-05 02:55:53,345 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:RecordCollectorImpl@268] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1] task [0_0] Closing record collector clean
2022-07-05 02:55:53,345 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:StreamTask@508] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1] task [0_0] Closed clean
2022-07-05 02:55:53,346 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:KafkaProducer@1204] - [Producer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1-producer] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2022-07-05 02:55:53,347 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:Metrics@659] - Metrics scheduler closed
2022-07-05 02:55:53,347 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:55:53,347 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:Metrics@669] - Metrics reporters closed
2022-07-05 02:55:53,347 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:AppInfoParser@83] - App info kafka.producer for appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1-producer unregistered
2022-07-05 02:55:53,348 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:KafkaConsumer@1077] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions
2022-07-05 02:55:53,348 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:Metrics@659] - Metrics scheduler closed
2022-07-05 02:55:53,348 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:55:53,348 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:Metrics@669] - Metrics reporters closed
2022-07-05 02:55:53,350 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:AppInfoParser@83] - App info kafka.consumer for appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1-consumer unregistered
2022-07-05 02:55:53,350 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:Metrics@659] - Metrics scheduler closed
2022-07-05 02:55:53,350 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:55:53,350 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:Metrics@669] - Metrics reporters closed
2022-07-05 02:55:53,351 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:AppInfoParser@83] - App info kafka.consumer for appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1-restore-consumer unregistered
2022-07-05 02:55:53,351 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:StreamThread@230] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1] State transition from PENDING_SHUTDOWN to DEAD
2022-07-05 02:55:53,352 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1:StreamThread@1100] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-StreamThread-1] Shutdown complete
2022-07-05 02:55:53,352 - ERROR [kafka-streams-close-thread:StateDirectory@414] - Some task directories still locked while closing state, this indicates unclean shutdown: {}
2022-07-05 02:55:53,352 - INFO  [kafka-admin-client-thread | appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-admin:AppInfoParser@83] - App info kafka.admin.client for appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-admin unregistered
2022-07-05 02:55:53,353 - INFO  [kafka-admin-client-thread | appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-admin:Metrics@659] - Metrics scheduler closed
2022-07-05 02:55:53,353 - INFO  [kafka-admin-client-thread | appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-admin:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:55:53,353 - INFO  [kafka-admin-client-thread | appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c-admin:Metrics@669] - Metrics reporters closed
2022-07-05 02:55:53,353 - INFO  [kafka-streams-close-thread:Metrics@659] - Metrics scheduler closed
2022-07-05 02:55:53,353 - INFO  [kafka-streams-close-thread:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:55:53,353 - INFO  [kafka-streams-close-thread:Metrics@669] - Metrics reporters closed
2022-07-05 02:55:53,353 - INFO  [kafka-streams-close-thread:KafkaStreams@321] - stream-client [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c] State transition from PENDING_SHUTDOWN to NOT_RUNNING
2022-07-05 02:55:53,353 - INFO  [main:KafkaStreams@1367] - stream-client [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-2738b082-65ee-4915-a848-e3e22f48498c] Streams client stopped completely
2022-07-05 02:55:53,356 - INFO  [main:Logging@66] - [KafkaServer id=0] shutting down
2022-07-05 02:55:53,357 - INFO  [main:Logging@66] - [KafkaServer id=0] Starting controlled shutdown
2022-07-05 02:55:53,367 - INFO  [main:Logging@66] - [KafkaServer id=0] Controlled shutdown succeeded
2022-07-05 02:55:53,368 - INFO  [main:Logging@66] - [/config/changes-event-process-thread]: Shutting down
2022-07-05 02:55:53,369 - INFO  [main:Logging@66] - [/config/changes-event-process-thread]: Shutdown completed
2022-07-05 02:55:53,369 - INFO  [/config/changes-event-process-thread:Logging@66] - [/config/changes-event-process-thread]: Stopped
2022-07-05 02:55:53,369 - INFO  [main:Logging@66] - [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopping socket server request processors
2022-07-05 02:55:53,373 - INFO  [main:Logging@66] - [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopped socket server request processors
2022-07-05 02:55:53,374 - INFO  [main:Logging@66] - [data-plane Kafka Request Handler on Broker 0], shutting down
2022-07-05 02:55:53,375 - INFO  [main:Logging@66] - [data-plane Kafka Request Handler on Broker 0], shut down completely
2022-07-05 02:55:53,376 - INFO  [main:Logging@66] - [ExpirationReaper-0-AlterAcls]: Shutting down
2022-07-05 02:55:53,522 - INFO  [ExpirationReaper-0-AlterAcls:Logging@66] - [ExpirationReaper-0-AlterAcls]: Stopped
2022-07-05 02:55:53,522 - INFO  [main:Logging@66] - [ExpirationReaper-0-AlterAcls]: Shutdown completed
2022-07-05 02:55:53,523 - INFO  [main:Logging@66] - [KafkaApi-0] Shutdown complete.
2022-07-05 02:55:53,524 - INFO  [main:Logging@66] - [ExpirationReaper-0-topic]: Shutting down
2022-07-05 02:55:53,638 - INFO  [ExpirationReaper-0-topic:Logging@66] - [ExpirationReaper-0-topic]: Stopped
2022-07-05 02:55:53,638 - INFO  [main:Logging@66] - [ExpirationReaper-0-topic]: Shutdown completed
2022-07-05 02:55:53,640 - INFO  [main:Logging@66] - [TransactionCoordinator id=0] Shutting down.
2022-07-05 02:55:53,640 - INFO  [main:Logging@66] - [ProducerId Manager 0]: Shutdown complete: last producerId assigned 0
2022-07-05 02:55:53,641 - INFO  [main:Logging@66] - [Transaction State Manager 0]: Shutdown complete
2022-07-05 02:55:53,641 - INFO  [main:Logging@66] - [Transaction Marker Channel Manager 0]: Shutting down
2022-07-05 02:55:53,641 - INFO  [TxnMarkerSenderThread-0:Logging@66] - [Transaction Marker Channel Manager 0]: Stopped
2022-07-05 02:55:53,641 - INFO  [main:Logging@66] - [Transaction Marker Channel Manager 0]: Shutdown completed
2022-07-05 02:55:53,642 - INFO  [main:Logging@66] - [TransactionCoordinator id=0] Shutdown complete.
2022-07-05 02:55:53,642 - INFO  [main:Logging@66] - [GroupCoordinator 0]: Shutting down.
2022-07-05 02:55:53,642 - INFO  [main:Logging@66] - [ExpirationReaper-0-Heartbeat]: Shutting down
2022-07-05 02:55:53,644 - INFO  [ExpirationReaper-0-Heartbeat:Logging@66] - [ExpirationReaper-0-Heartbeat]: Stopped
2022-07-05 02:55:53,644 - INFO  [main:Logging@66] - [ExpirationReaper-0-Heartbeat]: Shutdown completed
2022-07-05 02:55:53,644 - INFO  [main:Logging@66] - [ExpirationReaper-0-Rebalance]: Shutting down
2022-07-05 02:55:53,703 - INFO  [ExpirationReaper-0-Rebalance:Logging@66] - [ExpirationReaper-0-Rebalance]: Stopped
2022-07-05 02:55:53,703 - INFO  [main:Logging@66] - [ExpirationReaper-0-Rebalance]: Shutdown completed
2022-07-05 02:55:53,704 - INFO  [main:Logging@66] - [GroupCoordinator 0]: Shutdown complete.
2022-07-05 02:55:53,704 - INFO  [main:Logging@66] - [ReplicaManager broker=0] Shutting down
2022-07-05 02:55:53,704 - INFO  [main:Logging@66] - [LogDirFailureHandler]: Shutting down
2022-07-05 02:55:53,705 - INFO  [main:Logging@66] - [LogDirFailureHandler]: Shutdown completed
2022-07-05 02:55:53,705 - INFO  [LogDirFailureHandler:Logging@66] - [LogDirFailureHandler]: Stopped
2022-07-05 02:55:53,705 - INFO  [main:Logging@66] - [ReplicaFetcherManager on broker 0] shutting down
2022-07-05 02:55:53,706 - INFO  [main:Logging@66] - [ReplicaFetcherManager on broker 0] shutdown completed
2022-07-05 02:55:53,706 - INFO  [main:Logging@66] - [ReplicaAlterLogDirsManager on broker 0] shutting down
2022-07-05 02:55:53,707 - INFO  [main:Logging@66] - [ReplicaAlterLogDirsManager on broker 0] shutdown completed
2022-07-05 02:55:53,707 - INFO  [main:Logging@66] - [ExpirationReaper-0-Fetch]: Shutting down
2022-07-05 02:55:53,722 - INFO  [ExpirationReaper-0-Fetch:Logging@66] - [ExpirationReaper-0-Fetch]: Stopped
2022-07-05 02:55:53,723 - INFO  [main:Logging@66] - [ExpirationReaper-0-Fetch]: Shutdown completed
2022-07-05 02:55:53,723 - INFO  [main:Logging@66] - [ExpirationReaper-0-Produce]: Shutting down
2022-07-05 02:55:53,892 - INFO  [ExpirationReaper-0-Produce:Logging@66] - [ExpirationReaper-0-Produce]: Stopped
2022-07-05 02:55:53,892 - INFO  [main:Logging@66] - [ExpirationReaper-0-Produce]: Shutdown completed
2022-07-05 02:55:53,893 - INFO  [main:Logging@66] - [ExpirationReaper-0-DeleteRecords]: Shutting down
2022-07-05 02:55:54,093 - INFO  [ExpirationReaper-0-DeleteRecords:Logging@66] - [ExpirationReaper-0-DeleteRecords]: Stopped
2022-07-05 02:55:54,093 - INFO  [main:Logging@66] - [ExpirationReaper-0-DeleteRecords]: Shutdown completed
2022-07-05 02:55:54,093 - INFO  [main:Logging@66] - [ExpirationReaper-0-ElectLeader]: Shutting down
2022-07-05 02:55:54,293 - INFO  [ExpirationReaper-0-ElectLeader:Logging@66] - [ExpirationReaper-0-ElectLeader]: Stopped
2022-07-05 02:55:54,293 - INFO  [main:Logging@66] - [ExpirationReaper-0-ElectLeader]: Shutdown completed
2022-07-05 02:55:54,295 - INFO  [main:Logging@66] - [ReplicaManager broker=0] Shut down completely
2022-07-05 02:55:54,295 - INFO  [main:Logging@66] - [BrokerToControllerChannelManager broker=0 name=alterIsr]: Shutting down
2022-07-05 02:55:54,295 - INFO  [BrokerToControllerChannelManager broker=0 name=alterIsr:Logging@66] - [BrokerToControllerChannelManager broker=0 name=alterIsr]: Stopped
2022-07-05 02:55:54,295 - INFO  [main:Logging@66] - [BrokerToControllerChannelManager broker=0 name=alterIsr]: Shutdown completed
2022-07-05 02:55:54,296 - INFO  [main:Logging@66] - Broker to controller channel manager for alterIsr shutdown
2022-07-05 02:55:54,297 - INFO  [main:Logging@66] - Shutting down.
2022-07-05 02:55:54,312 - INFO  [pool-8-thread-1:Logging@66] - [ProducerStateManager partition=__consumer_offsets-0] Writing producer snapshot at offset 2
2022-07-05 02:55:54,328 - INFO  [pool-8-thread-1:Logging@66] - [ProducerStateManager partition=__consumer_offsets-1] Writing producer snapshot at offset 3
2022-07-05 02:55:54,344 - INFO  [pool-8-thread-1:Logging@66] - [ProducerStateManager partition=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0] Writing producer snapshot at offset 2
2022-07-05 02:55:54,361 - INFO  [pool-8-thread-1:Logging@66] - [ProducerStateManager partition=outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0] Writing producer snapshot at offset 2
2022-07-05 02:55:54,378 - INFO  [pool-8-thread-1:Logging@66] - [ProducerStateManager partition=inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0] Writing producer snapshot at offset 2
2022-07-05 02:55:54,404 - INFO  [main:Logging@66] - Shutdown complete.
2022-07-05 02:55:54,410 - INFO  [main:Logging@66] - [feature-zk-node-event-process-thread]: Shutting down
2022-07-05 02:55:54,410 - INFO  [feature-zk-node-event-process-thread:Logging@66] - [feature-zk-node-event-process-thread]: Stopped
2022-07-05 02:55:54,410 - INFO  [main:Logging@66] - [feature-zk-node-event-process-thread]: Shutdown completed
2022-07-05 02:55:54,411 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Closing.
2022-07-05 02:55:54,514 - INFO  [main:ZooKeeper@1422] - Session: 0x100a46ad7b20000 closed
2022-07-05 02:55:54,514 - INFO  [main-EventThread:ClientCnxn$EventThread@524] - EventThread shut down for session: 0x100a46ad7b20000
2022-07-05 02:55:54,515 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Closed.
2022-07-05 02:55:54,515 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Fetch]: Shutting down
2022-07-05 02:55:55,159 - INFO  [ThrottledChannelReaper-Fetch:Logging@66] - [ThrottledChannelReaper-Fetch]: Stopped
2022-07-05 02:55:55,159 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Fetch]: Shutdown completed
2022-07-05 02:55:55,160 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Produce]: Shutting down
2022-07-05 02:55:55,160 - INFO  [ThrottledChannelReaper-Produce:Logging@66] - [ThrottledChannelReaper-Produce]: Stopped
2022-07-05 02:55:55,160 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Produce]: Shutdown completed
2022-07-05 02:55:55,160 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Request]: Shutting down
2022-07-05 02:55:55,161 - INFO  [ThrottledChannelReaper-Request:Logging@66] - [ThrottledChannelReaper-Request]: Stopped
2022-07-05 02:55:55,161 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Request]: Shutdown completed
2022-07-05 02:55:55,161 - INFO  [main:Logging@66] - [ThrottledChannelReaper-ControllerMutation]: Shutting down
2022-07-05 02:55:55,162 - INFO  [ThrottledChannelReaper-ControllerMutation:Logging@66] - [ThrottledChannelReaper-ControllerMutation]: Stopped
2022-07-05 02:55:55,162 - INFO  [main:Logging@66] - [ThrottledChannelReaper-ControllerMutation]: Shutdown completed
2022-07-05 02:55:55,163 - INFO  [main:Logging@66] - [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutting down socket server
2022-07-05 02:55:55,181 - INFO  [main:Logging@66] - [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutdown completed
2022-07-05 02:55:55,181 - INFO  [main:Metrics@659] - Metrics scheduler closed
2022-07-05 02:55:55,182 - INFO  [main:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:55:55,182 - INFO  [main:Metrics@669] - Metrics reporters closed
2022-07-05 02:55:55,184 - INFO  [main:Logging@66] - Broker and topic stats closed
2022-07-05 02:55:55,184 - INFO  [main:AppInfoParser@83] - App info kafka.server for 0 unregistered
2022-07-05 02:55:55,185 - INFO  [main:Logging@66] - [KafkaServer id=0] shut down completed
2022-07-05 02:55:55,189 - INFO  [ConnnectionExpirer:NIOServerCnxnFactory$ConnectionExpirerThread@583] - ConnnectionExpirerThread interrupted
2022-07-05 02:55:55,190 - INFO  [NIOServerCxnFactory.AcceptThread:/127.0.0.1:0:NIOServerCnxnFactory$AcceptThread@219] - accept thread exitted run method
2022-07-05 02:55:55,190 - INFO  [NIOServerCxnFactory.SelectorThread-1:NIOServerCnxnFactory$SelectorThread@420] - selector thread exitted run method
2022-07-05 02:55:55,190 - INFO  [NIOServerCxnFactory.SelectorThread-0:NIOServerCnxnFactory$SelectorThread@420] - selector thread exitted run method
2022-07-05 02:55:55,190 - INFO  [NIOServerCxnFactory.SelectorThread-2:NIOServerCnxnFactory$SelectorThread@420] - selector thread exitted run method
2022-07-05 02:55:55,190 - INFO  [main:ZooKeeperServer@573] - shutting down
2022-07-05 02:55:55,191 - INFO  [main:SessionTrackerImpl@237] - Shutting down
2022-07-05 02:55:55,191 - INFO  [main:PrepRequestProcessor@1008] - Shutting down
2022-07-05 02:55:55,191 - INFO  [ProcessThread(sid:0 cport:41833)::PrepRequestProcessor@156] - PrepRequestProcessor exited loop!
2022-07-05 02:55:55,191 - INFO  [main:SyncRequestProcessor@191] - Shutting down
2022-07-05 02:55:55,191 - INFO  [SyncThread:0:SyncRequestProcessor@169] - SyncRequestProcessor exited!
2022-07-05 02:55:55,191 - INFO  [main:FinalRequestProcessor@514] - shutdown of request processor complete
2022-07-05 02:56:05,035 - INFO  [main:Log4jControllerRegistration$@31] - Registered kafka:type=kafka.Log4jController MBean
2022-07-05 02:56:05,082 - INFO  [main:Environment@109] - Server environment:zookeeper.version=3.5.9-83df9301aa5c2a5d284a9940177808c01bc35cef, built on 01/06/2021 20:03 GMT
2022-07-05 02:56:05,082 - INFO  [main:Environment@109] - Server environment:host.name=razor15
2022-07-05 02:56:05,083 - INFO  [main:Environment@109] - Server environment:java.version=1.8.0_275
2022-07-05 02:56:05,083 - INFO  [main:Environment@109] - Server environment:java.vendor=Private Build
2022-07-05 02:56:05,083 - INFO  [main:Environment@109] - Server environment:java.home=/usr/lib/jvm/java-8-openjdk-amd64/jre
2022-07-05 02:56:05,083 - INFO  [main:Environment@109] - Server environment:java.class.path=/home/tonypan/flaky-reproduction/experiment/kafka-12508/junit-platform-console-standalone-1.7.0.jar
2022-07-05 02:56:05,083 - INFO  [main:Environment@109] - Server environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib
2022-07-05 02:56:05,083 - INFO  [main:Environment@109] - Server environment:java.io.tmpdir=/tmp
2022-07-05 02:56:05,083 - INFO  [main:Environment@109] - Server environment:java.compiler=<NA>
2022-07-05 02:56:05,083 - INFO  [main:Environment@109] - Server environment:os.name=Linux
2022-07-05 02:56:05,084 - INFO  [main:Environment@109] - Server environment:os.arch=amd64
2022-07-05 02:56:05,084 - INFO  [main:Environment@109] - Server environment:os.version=4.15.0-128-generic
2022-07-05 02:56:05,084 - INFO  [main:Environment@109] - Server environment:user.name=tonypan
2022-07-05 02:56:05,084 - INFO  [main:Environment@109] - Server environment:user.home=/home/tonypan
2022-07-05 02:56:05,084 - INFO  [main:Environment@109] - Server environment:user.dir=/home/tonypan/flaky-reproduction/experiment/kafka-12508
2022-07-05 02:56:05,085 - INFO  [main:Environment@109] - Server environment:os.memory.free=401MB
2022-07-05 02:56:05,085 - INFO  [main:Environment@109] - Server environment:os.memory.max=7051MB
2022-07-05 02:56:05,085 - INFO  [main:Environment@109] - Server environment:os.memory.total=475MB
2022-07-05 02:56:05,090 - INFO  [main:FileTxnSnapLog@115] - zookeeper.snapshot.trust.empty : false
2022-07-05 02:56:05,114 - INFO  [main:ZKDatabase@117] - zookeeper.snapshotSizeFactor = 0.33
2022-07-05 02:56:05,121 - INFO  [main:ZooKeeperServer@953] - minSessionTimeout set to 1600
2022-07-05 02:56:05,121 - INFO  [main:ZooKeeperServer@962] - maxSessionTimeout set to 16000
2022-07-05 02:56:05,122 - INFO  [main:ZooKeeperServer@181] - Created server with tickTime 800 minSessionTimeout 1600 maxSessionTimeout 16000 datadir /tmp/kafka-1910287087040448163/version-2 snapdir /tmp/kafka-2594531190512160092/version-2
2022-07-05 02:56:05,134 - INFO  [main:NIOServerCnxnFactory@673] - Configuring NIO connection handler with 10s sessionless connection timeout, 3 selector thread(s), 40 worker threads, and 64 kB direct buffers.
2022-07-05 02:56:05,140 - INFO  [main:NIOServerCnxnFactory@686] - binding to port /127.0.0.1:0
2022-07-05 02:56:05,149 - INFO  [main:FileTxnSnapLog@404] - Snapshotting: 0x0 to /tmp/kafka-2594531190512160092/version-2/snapshot.0
2022-07-05 02:56:05,153 - INFO  [main:FileTxnSnapLog@404] - Snapshotting: 0x0 to /tmp/kafka-2594531190512160092/version-2/snapshot.0
2022-07-05 02:56:05,178 - INFO  [ProcessThread(sid:0 cport:39129)::PrepRequestProcessor@132] - PrepRequestProcessor (sid:0) started, reconfigEnabled=false
2022-07-05 02:56:05,504 - INFO  [main:AbstractConfig@372] - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 0
	host.name = localhost
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.8-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/junit908791874146947038/junit2077944468751917615
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.8-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000000
	metadata.log.dir = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = -1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 5
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:39129
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 10000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

2022-07-05 02:56:05,524 - INFO  [main:X509Util@79] - Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation
2022-07-05 02:56:05,613 - INFO  [main:Logging@66] - starting
2022-07-05 02:56:05,613 - INFO  [main:Logging@66] - Connecting to zookeeper on 127.0.0.1:39129
2022-07-05 02:56:05,635 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:39129.
2022-07-05 02:56:05,641 - INFO  [main:Environment@109] - Client environment:zookeeper.version=3.5.9-83df9301aa5c2a5d284a9940177808c01bc35cef, built on 01/06/2021 20:03 GMT
2022-07-05 02:56:05,641 - INFO  [main:Environment@109] - Client environment:host.name=razor15
2022-07-05 02:56:05,642 - INFO  [main:Environment@109] - Client environment:java.version=1.8.0_275
2022-07-05 02:56:05,642 - INFO  [main:Environment@109] - Client environment:java.vendor=Private Build
2022-07-05 02:56:05,642 - INFO  [main:Environment@109] - Client environment:java.home=/usr/lib/jvm/java-8-openjdk-amd64/jre
2022-07-05 02:56:05,642 - INFO  [main:Environment@109] - Client environment:java.class.path=/home/tonypan/flaky-reproduction/experiment/kafka-12508/junit-platform-console-standalone-1.7.0.jar
2022-07-05 02:56:05,642 - INFO  [main:Environment@109] - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib
2022-07-05 02:56:05,642 - INFO  [main:Environment@109] - Client environment:java.io.tmpdir=/tmp
2022-07-05 02:56:05,642 - INFO  [main:Environment@109] - Client environment:java.compiler=<NA>
2022-07-05 02:56:05,642 - INFO  [main:Environment@109] - Client environment:os.name=Linux
2022-07-05 02:56:05,642 - INFO  [main:Environment@109] - Client environment:os.arch=amd64
2022-07-05 02:56:05,642 - INFO  [main:Environment@109] - Client environment:os.version=4.15.0-128-generic
2022-07-05 02:56:05,643 - INFO  [main:Environment@109] - Client environment:user.name=tonypan
2022-07-05 02:56:05,643 - INFO  [main:Environment@109] - Client environment:user.home=/home/tonypan
2022-07-05 02:56:05,643 - INFO  [main:Environment@109] - Client environment:user.dir=/home/tonypan/flaky-reproduction/experiment/kafka-12508
2022-07-05 02:56:05,643 - INFO  [main:Environment@109] - Client environment:os.memory.free=336MB
2022-07-05 02:56:05,643 - INFO  [main:Environment@109] - Client environment:os.memory.max=7051MB
2022-07-05 02:56:05,643 - INFO  [main:Environment@109] - Client environment:os.memory.total=441MB
2022-07-05 02:56:05,648 - INFO  [main:ZooKeeper@868] - Initiating client connection, connectString=127.0.0.1:39129 sessionTimeout=10000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@31add175
2022-07-05 02:56:05,652 - INFO  [main:ClientCnxnSocket@237] - jute.maxbuffer value is 4194304 Bytes
2022-07-05 02:56:05,659 - INFO  [main:ClientCnxn@1653] - zookeeper.request.timeout value is 0. feature enabled=
2022-07-05 02:56:05,660 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Waiting until connected.
2022-07-05 02:56:05,666 - INFO  [main-SendThread(127.0.0.1:39129):ClientCnxn$SendThread@1112] - Opening socket connection to server localhost/127.0.0.1:39129. Will not attempt to authenticate using SASL (unknown error)
2022-07-05 02:56:05,668 - INFO  [main-SendThread(127.0.0.1:39129):ClientCnxn$SendThread@959] - Socket connection established, initiating session, client: /127.0.0.1:33416, server: localhost/127.0.0.1:39129
2022-07-05 02:56:05,678 - INFO  [SyncThread:0:FileTxnLog@218] - Creating new log file: log.1
2022-07-05 02:56:05,688 - INFO  [main-SendThread(127.0.0.1:39129):ClientCnxn$SendThread@1394] - Session establishment complete on server localhost/127.0.0.1:39129, sessionid = 0x100a46b12920000, negotiated timeout = 10000
2022-07-05 02:56:05,693 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Connected.
2022-07-05 02:56:05,812 - INFO  [feature-zk-node-event-process-thread:Logging@66] - [feature-zk-node-event-process-thread]: Starting
2022-07-05 02:56:05,824 - INFO  [feature-zk-node-event-process-thread:Logging@66] - Feature ZK node at path: /feature does not exist
2022-07-05 02:56:05,825 - INFO  [feature-zk-node-event-process-thread:Logging@66] - Cleared cache
2022-07-05 02:56:06,104 - INFO  [main:Logging@66] - Cluster ID = KmvgGXfIS1CzvbpBFQ5b4A
2022-07-05 02:56:06,109 - WARN  [main:Logging@70] - No meta.properties file under dir /tmp/junit908791874146947038/junit2077944468751917615/meta.properties
2022-07-05 02:56:06,174 - INFO  [main:AbstractConfig@372] - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 0
	host.name = localhost
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.8-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/junit908791874146947038/junit2077944468751917615
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.8-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000000
	metadata.log.dir = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = -1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 5
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:39129
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 10000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

2022-07-05 02:56:06,186 - INFO  [main:AbstractConfig@372] - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 0
	host.name = localhost
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.8-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/junit908791874146947038/junit2077944468751917615
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.8-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000000
	metadata.log.dir = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = -1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 5
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:39129
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 10000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

2022-07-05 02:56:06,220 - INFO  [ThrottledChannelReaper-Fetch:Logging@66] - [ThrottledChannelReaper-Fetch]: Starting
2022-07-05 02:56:06,221 - INFO  [ThrottledChannelReaper-Produce:Logging@66] - [ThrottledChannelReaper-Produce]: Starting
2022-07-05 02:56:06,222 - INFO  [ThrottledChannelReaper-Request:Logging@66] - [ThrottledChannelReaper-Request]: Starting
2022-07-05 02:56:06,223 - INFO  [ThrottledChannelReaper-ControllerMutation:Logging@66] - [ThrottledChannelReaper-ControllerMutation]: Starting
2022-07-05 02:56:06,255 - INFO  [main:Logging@66] - Loading logs from log dirs ArraySeq(/tmp/junit908791874146947038/junit2077944468751917615)
2022-07-05 02:56:06,259 - INFO  [main:Logging@66] - Attempting recovery for all logs in /tmp/junit908791874146947038/junit2077944468751917615 since no clean shutdown file was found
2022-07-05 02:56:06,264 - INFO  [main:Logging@66] - Loaded 0 logs in 0ms.
2022-07-05 02:56:06,264 - INFO  [main:Logging@66] - Starting log cleanup with a period of 300000 ms.
2022-07-05 02:56:06,267 - INFO  [main:Logging@66] - Starting log flusher with a default period of 9223372036854775807 ms.
2022-07-05 02:56:06,665 - INFO  [main:Logging@66] - Updated connection-accept-rate max connection creation rate to 2147483647
2022-07-05 02:56:06,668 - INFO  [main:Logging@66] - Awaiting socket connections on localhost:41521.
2022-07-05 02:56:06,706 - INFO  [main:Logging@66] - [SocketServer listenerType=ZK_BROKER, nodeId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT)
2022-07-05 02:56:06,735 - INFO  [BrokerToControllerChannelManager broker=0 name=alterIsr:Logging@66] - [BrokerToControllerChannelManager broker=0 name=alterIsr]: Starting
2022-07-05 02:56:06,758 - INFO  [ExpirationReaper-0-Produce:Logging@66] - [ExpirationReaper-0-Produce]: Starting
2022-07-05 02:56:06,759 - INFO  [ExpirationReaper-0-Fetch:Logging@66] - [ExpirationReaper-0-Fetch]: Starting
2022-07-05 02:56:06,759 - INFO  [ExpirationReaper-0-DeleteRecords:Logging@66] - [ExpirationReaper-0-DeleteRecords]: Starting
2022-07-05 02:56:06,760 - INFO  [ExpirationReaper-0-ElectLeader:Logging@66] - [ExpirationReaper-0-ElectLeader]: Starting
2022-07-05 02:56:06,773 - INFO  [LogDirFailureHandler:Logging@66] - [LogDirFailureHandler]: Starting
2022-07-05 02:56:06,815 - INFO  [main:Logging@66] - Creating /brokers/ids/0 (is it secure? false)
2022-07-05 02:56:06,843 - INFO  [main:Logging@66] - Stat of the created znode at /brokers/ids/0 is: 25,25,1657004166835,1657004166835,1,0,0,72238373817942016,204,0,25

2022-07-05 02:56:06,844 - INFO  [main:Logging@66] - Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://localhost:41521, czxid (broker epoch): 25
2022-07-05 02:56:06,907 - INFO  [ExpirationReaper-0-topic:Logging@66] - [ExpirationReaper-0-topic]: Starting
2022-07-05 02:56:06,913 - INFO  [ExpirationReaper-0-Heartbeat:Logging@66] - [ExpirationReaper-0-Heartbeat]: Starting
2022-07-05 02:56:06,913 - INFO  [ExpirationReaper-0-Rebalance:Logging@66] - [ExpirationReaper-0-Rebalance]: Starting
2022-07-05 02:56:06,915 - INFO  [controller-event-thread:Logging@66] - Successfully created /controller_epoch with initial epoch 0
2022-07-05 02:56:06,926 - INFO  [main-EventThread:Logging@66] - Feature ZK node created at path: /feature
2022-07-05 02:56:06,930 - INFO  [main:Logging@66] - [GroupCoordinator 0]: Starting up.
2022-07-05 02:56:06,934 - INFO  [main:Logging@66] - [GroupCoordinator 0]: Startup complete.
2022-07-05 02:56:06,959 - INFO  [feature-zk-node-event-process-thread:Logging@66] - Updated cache from existing <empty> to latest FinalizedFeaturesAndEpoch(features=Features{}, epoch=0).
2022-07-05 02:56:06,959 - INFO  [main:Logging@66] - [ProducerId Manager 0]: Acquired new producerId block (brokerId:0,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1
2022-07-05 02:56:06,960 - INFO  [main:Logging@66] - [TransactionCoordinator id=0] Starting up.
2022-07-05 02:56:06,962 - INFO  [TxnMarkerSenderThread-0:Logging@66] - [Transaction Marker Channel Manager 0]: Starting
2022-07-05 02:56:06,963 - INFO  [main:Logging@66] - [TransactionCoordinator id=0] Startup complete.
2022-07-05 02:56:06,989 - INFO  [ExpirationReaper-0-AlterAcls:Logging@66] - [ExpirationReaper-0-AlterAcls]: Starting
2022-07-05 02:56:07,015 - INFO  [/config/changes-event-process-thread:Logging@66] - [/config/changes-event-process-thread]: Starting
2022-07-05 02:56:07,022 - INFO  [main:Logging@66] - [SocketServer listenerType=ZK_BROKER, nodeId=0] Starting socket server acceptors and processors
2022-07-05 02:56:07,026 - INFO  [main:Logging@66] - [SocketServer listenerType=ZK_BROKER, nodeId=0] Started data-plane acceptor and processor(s) for endpoint : ListenerName(PLAINTEXT)
2022-07-05 02:56:07,027 - INFO  [main:Logging@66] - [SocketServer listenerType=ZK_BROKER, nodeId=0] Started socket server acceptors and processors
2022-07-05 02:56:07,028 - WARN  [main:AppInfoParser@46] - Error while loading kafka-version.properties: null
2022-07-05 02:56:07,029 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2022-07-05 02:56:07,030 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2022-07-05 02:56:07,030 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1657004164850
2022-07-05 02:56:07,031 - INFO  [main:Logging@66] - [KafkaServer id=0] started
2022-07-05 02:56:07,050 - INFO  [main:AbstractConfig@372] - AdminClientConfig values: 
	bootstrap.servers = [localhost:41521]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2022-07-05 02:56:07,072 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2022-07-05 02:56:07,073 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2022-07-05 02:56:07,073 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1657004167072
2022-07-05 02:56:07,130 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Creating topic inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0))
2022-07-05 02:56:07,145 - INFO  [BrokerToControllerChannelManager broker=0 name=alterIsr:Logging@66] - [BrokerToControllerChannelManager broker=0 name=alterIsr]: Recorded new controller, from now on will use broker localhost:41521 (id: 0 rack: null)
2022-07-05 02:56:07,200 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0)
2022-07-05 02:56:07,268 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0, dir=/tmp/junit908791874146947038/junit2077944468751917615] Loading producer state till offset 0 with message format version 2
2022-07-05 02:56:07,271 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 in /tmp/junit908791874146947038/junit2077944468751917615/inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.8-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2022-07-05 02:56:07,274 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 broker=0] No checkpointed highwatermark is found for partition inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0
2022-07-05 02:56:07,275 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 broker=0] Log loaded for partition inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 with initial high watermark 0
2022-07-05 02:56:07,302 - INFO  [kafka-admin-client-thread | adminclient-1:AppInfoParser@83] - App info kafka.admin.client for adminclient-1 unregistered
2022-07-05 02:56:07,304 - INFO  [kafka-admin-client-thread | adminclient-1:Metrics@659] - Metrics scheduler closed
2022-07-05 02:56:07,304 - INFO  [kafka-admin-client-thread | adminclient-1:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:56:07,305 - INFO  [kafka-admin-client-thread | adminclient-1:Metrics@669] - Metrics reporters closed
2022-07-05 02:56:07,307 - INFO  [main:AbstractConfig@372] - AdminClientConfig values: 
	bootstrap.servers = [localhost:41521]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2022-07-05 02:56:07,309 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2022-07-05 02:56:07,309 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2022-07-05 02:56:07,309 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1657004167309
2022-07-05 02:56:07,320 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Creating topic outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0))
2022-07-05 02:56:07,332 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0)
2022-07-05 02:56:07,335 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Log partition=outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0, dir=/tmp/junit908791874146947038/junit2077944468751917615] Loading producer state till offset 0 with message format version 2
2022-07-05 02:56:07,336 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Created log for partition outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 in /tmp/junit908791874146947038/junit2077944468751917615/outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.8-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2022-07-05 02:56:07,336 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 broker=0] No checkpointed highwatermark is found for partition outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0
2022-07-05 02:56:07,336 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 broker=0] Log loaded for partition outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 with initial high watermark 0
2022-07-05 02:56:07,341 - INFO  [kafka-admin-client-thread | adminclient-2:AppInfoParser@83] - App info kafka.admin.client for adminclient-2 unregistered
2022-07-05 02:56:07,342 - INFO  [kafka-admin-client-thread | adminclient-2:Metrics@659] - Metrics scheduler closed
2022-07-05 02:56:07,342 - INFO  [kafka-admin-client-thread | adminclient-2:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:56:07,342 - INFO  [kafka-admin-client-thread | adminclient-2:Metrics@669] - Metrics reporters closed
2022-07-05 02:56:07,396 - INFO  [main:AbstractConfig@372] - StreamsConfig values: 
	acceptable.recovery.lag = 10000
	application.id = appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover
	application.server = 
	bootstrap.servers = [localhost:41521]
	buffered.records.per.partition = 1000
	built.in.metrics.version = latest
	cache.max.bytes.buffering = 0
	client.id = 
	commit.interval.ms = 300000
	connections.max.idle.ms = 540000
	default.deserialization.exception.handler = class org.apache.kafka.streams.errors.LogAndFailExceptionHandler
	default.key.serde = class org.apache.kafka.common.serialization.Serdes$IntegerSerde
	default.production.exception.handler = class org.apache.kafka.streams.errors.DefaultProductionExceptionHandler
	default.timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	default.value.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	default.windowed.key.serde.inner = null
	default.windowed.value.serde.inner = null
	max.task.idle.ms = 0
	max.warmup.replicas = 2
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	probing.rebalance.interval.ms = 600000
	processing.guarantee = at_least_once
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retries = 0
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 600000
	state.dir = /tmp/kafka-1067299138980771220
	task.timeout.ms = 300000
	topology.optimization = none
	upgrade.from = null
	window.size.ms = null
	windowstore.changelog.additional.retention.ms = 86400000

2022-07-05 02:56:07,427 - WARN  [main:StateDirectory@138] - Using an OS temp directory in the state.dir property can cause failures with writing the checkpoint file due to the fact that this directory can be cleared by the OS. Resolved state.dir: [/tmp/kafka-1067299138980771220]
2022-07-05 02:56:07,428 - INFO  [main:StateDirectory@212] - No process id found on disk, got fresh process id f8581bf3-f2a6-421b-9b71-c8a609d4a046
2022-07-05 02:56:07,461 - INFO  [main:AbstractConfig@372] - AdminClientConfig values: 
	bootstrap.servers = [localhost:41521]
	client.dns.lookup = use_all_dns_ips
	client.id = appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-admin
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2022-07-05 02:56:07,463 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2022-07-05 02:56:07,464 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2022-07-05 02:56:07,464 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1657004167463
2022-07-05 02:56:07,467 - WARN  [main:ClientMetrics@55] - Error while loading kafka-streams-version.properties
java.lang.NullPointerException
	at java.util.Properties$LineReader.readLine(Properties.java:434)
	at java.util.Properties.load0(Properties.java:353)
	at java.util.Properties.load(Properties.java:341)
	at org.apache.kafka.streams.internals.metrics.ClientMetrics.<clinit>(ClientMetrics.java:53)
	at org.apache.kafka.streams.KafkaStreams.<init>(KafkaStreams.java:825)
	at org.apache.kafka.streams.KafkaStreams.<init>(KafkaStreams.java:781)
	at org.apache.kafka.streams.KafkaStreams.<init>(KafkaStreams.java:691)
	at org.apache.kafka.streams.integration.EmitOnChangeIntegrationTest.shouldEmitSameRecordAfterFailover(EmitOnChangeIntegrationTest.java:106)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
2022-07-05 02:56:07,470 - INFO  [main:KafkaStreams@825] - stream-client [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046] Kafka Streams version: unknown
2022-07-05 02:56:07,470 - INFO  [main:KafkaStreams@826] - stream-client [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046] Kafka Streams commit ID: unknown
2022-07-05 02:56:07,479 - INFO  [main:StreamThread@337] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1] Creating restore consumer client
2022-07-05 02:56:07,483 - INFO  [main:AbstractConfig@372] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [localhost:41521]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1-restore-consumer
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = false
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2022-07-05 02:56:07,503 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2022-07-05 02:56:07,504 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2022-07-05 02:56:07,504 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1657004167503
2022-07-05 02:56:07,511 - INFO  [main:ActiveTaskCreator@96] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1] Creating thread producer client
2022-07-05 02:56:07,515 - INFO  [main:AbstractConfig@372] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:41521]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-07-05 02:56:07,527 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2022-07-05 02:56:07,528 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2022-07-05 02:56:07,528 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1657004167527
2022-07-05 02:56:07,532 - INFO  [main:StreamThread@389] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1] Creating consumer client
2022-07-05 02:56:07,534 - INFO  [kafka-producer-network-thread | appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1-producer:Metadata@279] - [Producer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1-producer] Cluster ID: KmvgGXfIS1CzvbpBFQ5b4A
2022-07-05 02:56:07,535 - INFO  [main:AbstractConfig@372] - ConsumerConfig values: 
	allow.auto.create.topics = false
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:41521]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1-consumer
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = false
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2022-07-05 02:56:07,545 - INFO  [main:AssignorConfiguration@114] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1-consumer] Cooperative rebalancing enabled now
2022-07-05 02:56:07,557 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2022-07-05 02:56:07,557 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2022-07-05 02:56:07,557 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1657004167557
2022-07-05 02:56:07,564 - INFO  [main:KafkaStreams@321] - stream-client [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046] State transition from CREATED to REBALANCING
2022-07-05 02:56:07,565 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:StreamThread@550] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1] Starting
2022-07-05 02:56:07,565 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:StreamThread@230] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1] State transition from CREATED to STARTING
2022-07-05 02:56:07,565 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:KafkaConsumer@968] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Subscribed to topic(s): inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover
2022-07-05 02:56:07,577 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:Metadata@279] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Cluster ID: KmvgGXfIS1CzvbpBFQ5b4A
2022-07-05 02:56:07,579 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Creating topic __consumer_offsets with configuration {segment.bytes=104857600, cleanup.policy=compact, compression.type=producer} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0))
2022-07-05 02:56:07,599 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(__consumer_offsets-4, __consumer_offsets-3, __consumer_offsets-2, __consumer_offsets-0, __consumer_offsets-1)
2022-07-05 02:56:07,602 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-3, dir=/tmp/junit908791874146947038/junit2077944468751917615] Loading producer state till offset 0 with message format version 2
2022-07-05 02:56:07,603 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-3 in /tmp/junit908791874146947038/junit2077944468751917615/__consumer_offsets-3 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.8-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2022-07-05 02:56:07,603 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3
2022-07-05 02:56:07,603 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-3 broker=0] Log loaded for partition __consumer_offsets-3 with initial high watermark 0
2022-07-05 02:56:07,606 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-2, dir=/tmp/junit908791874146947038/junit2077944468751917615] Loading producer state till offset 0 with message format version 2
2022-07-05 02:56:07,607 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-2 in /tmp/junit908791874146947038/junit2077944468751917615/__consumer_offsets-2 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.8-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2022-07-05 02:56:07,607 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2
2022-07-05 02:56:07,607 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-2 broker=0] Log loaded for partition __consumer_offsets-2 with initial high watermark 0
2022-07-05 02:56:07,614 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-4, dir=/tmp/junit908791874146947038/junit2077944468751917615] Loading producer state till offset 0 with message format version 2
2022-07-05 02:56:07,615 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-4 in /tmp/junit908791874146947038/junit2077944468751917615/__consumer_offsets-4 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.8-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2022-07-05 02:56:07,615 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4
2022-07-05 02:56:07,615 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-4 broker=0] Log loaded for partition __consumer_offsets-4 with initial high watermark 0
2022-07-05 02:56:07,623 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-1, dir=/tmp/junit908791874146947038/junit2077944468751917615] Loading producer state till offset 0 with message format version 2
2022-07-05 02:56:07,623 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-1 in /tmp/junit908791874146947038/junit2077944468751917615/__consumer_offsets-1 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.8-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2022-07-05 02:56:07,624 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1
2022-07-05 02:56:07,624 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-1 broker=0] Log loaded for partition __consumer_offsets-1 with initial high watermark 0
2022-07-05 02:56:07,632 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-0, dir=/tmp/junit908791874146947038/junit2077944468751917615] Loading producer state till offset 0 with message format version 2
2022-07-05 02:56:07,632 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-0 in /tmp/junit908791874146947038/junit2077944468751917615/__consumer_offsets-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.8-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2022-07-05 02:56:07,632 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0
2022-07-05 02:56:07,632 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-0 broker=0] Log loaded for partition __consumer_offsets-0 with initial high watermark 0
2022-07-05 02:56:07,638 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Elected as the group coordinator for partition 3
2022-07-05 02:56:07,639 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3
2022-07-05 02:56:07,640 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Elected as the group coordinator for partition 2
2022-07-05 02:56:07,640 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2
2022-07-05 02:56:07,640 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Elected as the group coordinator for partition 4
2022-07-05 02:56:07,640 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4
2022-07-05 02:56:07,640 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Elected as the group coordinator for partition 1
2022-07-05 02:56:07,640 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1
2022-07-05 02:56:07,640 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Elected as the group coordinator for partition 0
2022-07-05 02:56:07,640 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0
2022-07-05 02:56:07,644 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 5 milliseconds, of which 1 milliseconds was spent in the scheduler.
2022-07-05 02:56:07,644 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 4 milliseconds, of which 4 milliseconds was spent in the scheduler.
2022-07-05 02:56:07,644 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 4 milliseconds, of which 4 milliseconds was spent in the scheduler.
2022-07-05 02:56:07,644 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 4 milliseconds, of which 4 milliseconds was spent in the scheduler.
2022-07-05 02:56:07,645 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 5 milliseconds, of which 5 milliseconds was spent in the scheduler.
2022-07-05 02:56:07,669 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:StreamThread@829] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update
2022-07-05 02:56:07,674 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:AbstractCoordinator$FindCoordinatorResponseHandler@844] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Discovered group coordinator localhost:41521 (id: 2147483647 rack: null)
2022-07-05 02:56:07,675 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:AbstractCoordinator@534] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] (Re-)joining group
2022-07-05 02:56:07,695 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [GroupCoordinator 0]: Dynamic member with unknown member id joins group appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover in Empty state. Created a new member id appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1-consumer-d896e560-5730-41d0-933f-8db582530872 and request the member to rejoin with this id.
2022-07-05 02:56:07,699 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:AbstractCoordinator@982] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Request joining group due to: need to re-join with the given member-id
2022-07-05 02:56:07,699 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:AbstractCoordinator@534] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] (Re-)joining group
2022-07-05 02:56:07,704 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover in state PreparingRebalance with old generation 0 (__consumer_offsets-0) (reason: Adding new member appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1-consumer-d896e560-5730-41d0-933f-8db582530872 with group instance id None)
2022-07-05 02:56:07,709 - INFO  [executor-Rebalance:Logging@66] - [GroupCoordinator 0]: Stabilized group appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover generation 1 (__consumer_offsets-0) with 1 members
2022-07-05 02:56:07,711 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Successfully joined group with generation Generation{generationId=1, memberId='appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1-consumer-d896e560-5730-41d0-933f-8db582530872', protocol='stream'}
2022-07-05 02:56:07,728 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Creating topic appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog with configuration {message.timestamp.type=CreateTime, cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0))
2022-07-05 02:56:07,738 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0)
2022-07-05 02:56:07,741 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Log partition=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0, dir=/tmp/junit908791874146947038/junit2077944468751917615] Loading producer state till offset 0 with message format version 2
2022-07-05 02:56:07,741 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Created log for partition appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0 in /tmp/junit908791874146947038/junit2077944468751917615/appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.8-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2022-07-05 02:56:07,741 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0 broker=0] No checkpointed highwatermark is found for partition appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0
2022-07-05 02:56:07,742 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0 broker=0] Log loaded for partition appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0 with initial high watermark 0
2022-07-05 02:56:07,753 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:HighAvailabilityTaskAssignor@95] - Decided on assignment: {f8581bf3-f2a6-421b-9b71-c8a609d4a046=[activeTasks: ([0_0]) standbyTasks: ([]) prevActiveTasks: ([]) prevStandbyTasks: ([]) changelogOffsetTotalsByTask: ([]) taskLagTotals: ([0_0=0]) capacity: 1 assigned: 1]} with no followup probing rebalance.
2022-07-05 02:56:07,754 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:StreamsPartitionAssignor@595] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1-consumer] Assigned tasks [0_0] including stateful [0_0] to clients as: 
f8581bf3-f2a6-421b-9b71-c8a609d4a046=[activeTasks: ([0_0]) standbyTasks: ([])].
2022-07-05 02:56:07,758 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:StreamsPartitionAssignor@803] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1-consumer] Client f8581bf3-f2a6-421b-9b71-c8a609d4a046 per-consumer assignment:
	prev owned active {}
	prev owned standby {appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1-consumer-d896e560-5730-41d0-933f-8db582530872=[]}
	assigned active {appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1-consumer-d896e560-5730-41d0-933f-8db582530872=[0_0]}
	revoking active {}
	assigned standby {}

2022-07-05 02:56:07,758 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:StreamsPartitionAssignor@822] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1-consumer] Finished stable assignment of tasks, no followup rebalances required.
2022-07-05 02:56:07,758 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:ConsumerCoordinator@626] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Finished assignment for group at generation 1: {appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1-consumer-d896e560-5730-41d0-933f-8db582530872=Assignment(partitions=[inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0], userDataSize=48)}
2022-07-05 02:56:07,764 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover for generation 1. The group has 1 members, 0 of which are static.
2022-07-05 02:56:07,821 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:AbstractCoordinator$SyncGroupResponseHandler@756] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Successfully synced group in generation Generation{generationId=1, memberId='appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1-consumer-d896e560-5730-41d0-933f-8db582530872', protocol='stream'}
2022-07-05 02:56:07,822 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:ConsumerCoordinator@392] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Updating assignment with
	Assigned partitions:                       [inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0]
	Current owned partitions:                  []
	Added partitions (assigned - owned):       [inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0]
	Revoked partitions (owned - assigned):     []

2022-07-05 02:56:07,822 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:ConsumerCoordinator@276] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Notifying assignor about the new Assignment(partitions=[inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0], userDataSize=48)
2022-07-05 02:56:07,822 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:StreamsPartitionAssignor@1306] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1-consumer] No followup rebalance was requested, resetting the rebalance schedule.
2022-07-05 02:56:07,824 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:TaskManager@254] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1] Handle new assignment with:
	New active tasks: [0_0]
	New standby tasks: []
	Existing active tasks: []
	Existing standby tasks: []
2022-07-05 02:56:07,840 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:ConsumerCoordinator@288] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Adding newly assigned partitions: inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0
2022-07-05 02:56:07,840 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:StreamThread@230] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1] State transition from STARTING to PARTITIONS_ASSIGNED
2022-07-05 02:56:07,849 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:ConsumerCoordinator$OffsetFetchResponseHandler@1352] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Found no committed offset for partition inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0
2022-07-05 02:56:07,862 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:SubscriptionState@398] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Resetting offset for partition inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41521 (id: 0 rack: null)], epoch=0}}.
2022-07-05 02:56:08,023 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:RocksDBTimestampedStore@100] - Opening store test-store in regular mode
2022-07-05 02:56:08,027 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:ProcessorStateManager@256] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1] task [0_0] State store test-store did not find checkpoint offset, hence would default to the starting offset at changelog appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0
2022-07-05 02:56:08,027 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:StreamTask@235] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1] task [0_0] Initialized
2022-07-05 02:56:08,033 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:KafkaConsumer@1123] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1-restore-consumer, groupId=null] Subscribed to partition(s): appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0
2022-07-05 02:56:08,034 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:SubscriptionState@619] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1-restore-consumer, groupId=null] Seeking to EARLIEST offset of partition appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0
2022-07-05 02:56:08,037 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:Metadata@279] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1-restore-consumer, groupId=null] Cluster ID: KmvgGXfIS1CzvbpBFQ5b4A
2022-07-05 02:56:08,040 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:SubscriptionState@398] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1-restore-consumer, groupId=null] Resetting offset for partition appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41521 (id: 0 rack: null)], epoch=0}}.
2022-07-05 02:56:08,141 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:StoreChangelogReader@609] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1] Finished restoring changelog appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0 to store test-store with a total number of 0 records
2022-07-05 02:56:08,144 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:ConsumerCoordinator$OffsetFetchResponseHandler@1352] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1-consumer, groupId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover] Found no committed offset for partition inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0
2022-07-05 02:56:08,147 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:StreamTask@255] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1] task [0_0] Restored and ready to run
2022-07-05 02:56:08,148 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:StreamThread@853] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1] Restoration took 308 ms for all tasks [0_0]
2022-07-05 02:56:08,148 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:StreamThread@230] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1] State transition from PARTITIONS_ASSIGNED to RUNNING
2022-07-05 02:56:08,149 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:KafkaStreams@321] - stream-client [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046] State transition from REBALANCING to RUNNING
2022-07-05 02:56:08,151 - INFO  [main:AbstractConfig@372] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:41521]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.IntegerSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2022-07-05 02:56:08,153 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2022-07-05 02:56:08,153 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2022-07-05 02:56:08,153 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1657004168153
2022-07-05 02:56:08,156 - INFO  [kafka-producer-network-thread | producer-1:Metadata@279] - [Producer clientId=producer-1] Cluster ID: KmvgGXfIS1CzvbpBFQ5b4A
2022-07-05 02:56:08,160 - INFO  [main:KafkaProducer@1204] - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2022-07-05 02:56:08,175 - INFO  [main:Metrics@659] - Metrics scheduler closed
2022-07-05 02:56:08,175 - INFO  [main:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:56:08,175 - INFO  [main:Metrics@669] - Metrics reporters closed
2022-07-05 02:56:08,175 - INFO  [main:AppInfoParser@83] - App info kafka.producer for producer-1 unregistered
2022-07-05 02:56:08,176 - INFO  [main:AbstractConfig@372] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:41521]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-f8fb62cd-4620-4e8f-945f-bc0ef5388888-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = f8fb62cd-4620-4e8f-945f-bc0ef5388888
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.IntegerDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2022-07-05 02:56:08,178 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2022-07-05 02:56:08,179 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2022-07-05 02:56:08,179 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1657004168178
2022-07-05 02:56:08,180 - INFO  [main:KafkaConsumer@968] - [Consumer clientId=consumer-f8fb62cd-4620-4e8f-945f-bc0ef5388888-1, groupId=f8fb62cd-4620-4e8f-945f-bc0ef5388888] Subscribed to topic(s): outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover
2022-07-05 02:56:08,183 - INFO  [main:Metadata@279] - [Consumer clientId=consumer-f8fb62cd-4620-4e8f-945f-bc0ef5388888-1, groupId=f8fb62cd-4620-4e8f-945f-bc0ef5388888] Cluster ID: KmvgGXfIS1CzvbpBFQ5b4A
2022-07-05 02:56:08,183 - INFO  [main:AbstractCoordinator$FindCoordinatorResponseHandler@844] - [Consumer clientId=consumer-f8fb62cd-4620-4e8f-945f-bc0ef5388888-1, groupId=f8fb62cd-4620-4e8f-945f-bc0ef5388888] Discovered group coordinator localhost:41521 (id: 2147483647 rack: null)
2022-07-05 02:56:08,184 - INFO  [main:AbstractCoordinator@534] - [Consumer clientId=consumer-f8fb62cd-4620-4e8f-945f-bc0ef5388888-1, groupId=f8fb62cd-4620-4e8f-945f-bc0ef5388888] (Re-)joining group
2022-07-05 02:56:08,186 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [GroupCoordinator 0]: Dynamic member with unknown member id joins group f8fb62cd-4620-4e8f-945f-bc0ef5388888 in Empty state. Created a new member id consumer-f8fb62cd-4620-4e8f-945f-bc0ef5388888-1-67ec25ad-32ab-4162-bff2-9458e2e256b4 and request the member to rejoin with this id.
2022-07-05 02:56:08,187 - INFO  [main:AbstractCoordinator@982] - [Consumer clientId=consumer-f8fb62cd-4620-4e8f-945f-bc0ef5388888-1, groupId=f8fb62cd-4620-4e8f-945f-bc0ef5388888] Request joining group due to: need to re-join with the given member-id
2022-07-05 02:56:08,187 - INFO  [main:AbstractCoordinator@534] - [Consumer clientId=consumer-f8fb62cd-4620-4e8f-945f-bc0ef5388888-1, groupId=f8fb62cd-4620-4e8f-945f-bc0ef5388888] (Re-)joining group
2022-07-05 02:56:08,188 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group f8fb62cd-4620-4e8f-945f-bc0ef5388888 in state PreparingRebalance with old generation 0 (__consumer_offsets-3) (reason: Adding new member consumer-f8fb62cd-4620-4e8f-945f-bc0ef5388888-1-67ec25ad-32ab-4162-bff2-9458e2e256b4 with group instance id None)
2022-07-05 02:56:08,188 - INFO  [executor-Rebalance:Logging@66] - [GroupCoordinator 0]: Stabilized group f8fb62cd-4620-4e8f-945f-bc0ef5388888 generation 1 (__consumer_offsets-3) with 1 members
2022-07-05 02:56:08,189 - INFO  [main:AbstractCoordinator$JoinGroupResponseHandler@590] - [Consumer clientId=consumer-f8fb62cd-4620-4e8f-945f-bc0ef5388888-1, groupId=f8fb62cd-4620-4e8f-945f-bc0ef5388888] Successfully joined group with generation Generation{generationId=1, memberId='consumer-f8fb62cd-4620-4e8f-945f-bc0ef5388888-1-67ec25ad-32ab-4162-bff2-9458e2e256b4', protocol='range'}
2022-07-05 02:56:08,190 - INFO  [main:ConsumerCoordinator@626] - [Consumer clientId=consumer-f8fb62cd-4620-4e8f-945f-bc0ef5388888-1, groupId=f8fb62cd-4620-4e8f-945f-bc0ef5388888] Finished assignment for group at generation 1: {consumer-f8fb62cd-4620-4e8f-945f-bc0ef5388888-1-67ec25ad-32ab-4162-bff2-9458e2e256b4=Assignment(partitions=[outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0])}
2022-07-05 02:56:08,191 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group f8fb62cd-4620-4e8f-945f-bc0ef5388888 for generation 1. The group has 1 members, 0 of which are static.
2022-07-05 02:56:08,192 - INFO  [main:AbstractCoordinator$SyncGroupResponseHandler@756] - [Consumer clientId=consumer-f8fb62cd-4620-4e8f-945f-bc0ef5388888-1, groupId=f8fb62cd-4620-4e8f-945f-bc0ef5388888] Successfully synced group in generation Generation{generationId=1, memberId='consumer-f8fb62cd-4620-4e8f-945f-bc0ef5388888-1-67ec25ad-32ab-4162-bff2-9458e2e256b4', protocol='range'}
2022-07-05 02:56:08,192 - INFO  [main:ConsumerCoordinator@276] - [Consumer clientId=consumer-f8fb62cd-4620-4e8f-945f-bc0ef5388888-1, groupId=f8fb62cd-4620-4e8f-945f-bc0ef5388888] Notifying assignor about the new Assignment(partitions=[outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0])
2022-07-05 02:56:08,192 - INFO  [main:ConsumerCoordinator@288] - [Consumer clientId=consumer-f8fb62cd-4620-4e8f-945f-bc0ef5388888-1, groupId=f8fb62cd-4620-4e8f-945f-bc0ef5388888] Adding newly assigned partitions: outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0
2022-07-05 02:56:08,193 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1352] - [Consumer clientId=consumer-f8fb62cd-4620-4e8f-945f-bc0ef5388888-1, groupId=f8fb62cd-4620-4e8f-945f-bc0ef5388888] Found no committed offset for partition outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0
2022-07-05 02:56:08,197 - INFO  [main:SubscriptionState@398] - [Consumer clientId=consumer-f8fb62cd-4620-4e8f-945f-bc0ef5388888-1, groupId=f8fb62cd-4620-4e8f-945f-bc0ef5388888] Resetting offset for partition outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41521 (id: 0 rack: null)], epoch=0}}.
2022-07-05 02:56:08,320 - INFO  [main:ConsumerCoordinator@307] - [Consumer clientId=consumer-f8fb62cd-4620-4e8f-945f-bc0ef5388888-1, groupId=f8fb62cd-4620-4e8f-945f-bc0ef5388888] Revoke previously assigned partitions outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0
2022-07-05 02:56:08,320 - INFO  [main:AbstractCoordinator@1038] - [Consumer clientId=consumer-f8fb62cd-4620-4e8f-945f-bc0ef5388888-1, groupId=f8fb62cd-4620-4e8f-945f-bc0ef5388888] Member consumer-f8fb62cd-4620-4e8f-945f-bc0ef5388888-1-67ec25ad-32ab-4162-bff2-9458e2e256b4 sending LeaveGroup request to coordinator localhost:41521 (id: 2147483647 rack: null) due to the consumer is being closed
2022-07-05 02:56:08,321 - INFO  [main:AbstractCoordinator@961] - [Consumer clientId=consumer-f8fb62cd-4620-4e8f-945f-bc0ef5388888-1, groupId=f8fb62cd-4620-4e8f-945f-bc0ef5388888] Resetting generation due to: consumer pro-actively leaving the group
2022-07-05 02:56:08,321 - INFO  [main:AbstractCoordinator@982] - [Consumer clientId=consumer-f8fb62cd-4620-4e8f-945f-bc0ef5388888-1, groupId=f8fb62cd-4620-4e8f-945f-bc0ef5388888] Request joining group due to: consumer pro-actively leaving the group
2022-07-05 02:56:08,324 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group f8fb62cd-4620-4e8f-945f-bc0ef5388888 in state PreparingRebalance with old generation 1 (__consumer_offsets-3) (reason: removing member consumer-f8fb62cd-4620-4e8f-945f-bc0ef5388888-1-67ec25ad-32ab-4162-bff2-9458e2e256b4 on LeaveGroup)
2022-07-05 02:56:08,324 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupCoordinator 0]: Group f8fb62cd-4620-4e8f-945f-bc0ef5388888 with generation 2 is now empty (__consumer_offsets-3)
2022-07-05 02:56:08,327 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupCoordinator 0]: Member MemberMetadata(memberId=consumer-f8fb62cd-4620-4e8f-945f-bc0ef5388888-1-67ec25ad-32ab-4162-bff2-9458e2e256b4, groupInstanceId=None, clientId=consumer-f8fb62cd-4620-4e8f-945f-bc0ef5388888-1, clientHost=/127.0.0.1, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(range)) has left group f8fb62cd-4620-4e8f-945f-bc0ef5388888 through explicit `LeaveGroup` request
2022-07-05 02:56:08,329 - INFO  [main:Metrics@659] - Metrics scheduler closed
2022-07-05 02:56:08,329 - INFO  [main:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:56:08,329 - INFO  [main:Metrics@669] - Metrics reporters closed
2022-07-05 02:56:08,330 - INFO  [main:AppInfoParser@83] - App info kafka.consumer for consumer-f8fb62cd-4620-4e8f-945f-bc0ef5388888-1 unregistered
2022-07-05 02:56:08,330 - INFO  [main:KafkaStreams@321] - stream-client [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046] State transition from RUNNING to PENDING_SHUTDOWN
2022-07-05 02:56:08,331 - INFO  [kafka-streams-close-thread:StreamThread@1057] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1] Informed to shut down
2022-07-05 02:56:08,331 - INFO  [kafka-streams-close-thread:StreamThread@230] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1] State transition from RUNNING to PENDING_SHUTDOWN
2022-07-05 02:56:08,404 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:StreamThread@729] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1] Thread state is already PENDING_SHUTDOWN, skipping the run once call after poll request
2022-07-05 02:56:08,404 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:StreamThread@1071] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1] Shutting down
2022-07-05 02:56:08,426 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:StreamTask@1144] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1] task [0_0] Suspended RUNNING
2022-07-05 02:56:08,426 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:StreamTask@290] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1] task [0_0] Suspended running
2022-07-05 02:56:08,428 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:KafkaConsumer@1077] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions
2022-07-05 02:56:08,429 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:RecordCollectorImpl@268] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1] task [0_0] Closing record collector clean
2022-07-05 02:56:08,430 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:StreamTask@508] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1] task [0_0] Closed clean
2022-07-05 02:56:08,430 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:KafkaProducer@1204] - [Producer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1-producer] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2022-07-05 02:56:08,432 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:Metrics@659] - Metrics scheduler closed
2022-07-05 02:56:08,432 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:56:08,432 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:Metrics@669] - Metrics reporters closed
2022-07-05 02:56:08,432 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:AppInfoParser@83] - App info kafka.producer for appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1-producer unregistered
2022-07-05 02:56:08,432 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:KafkaConsumer@1077] - [Consumer clientId=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions
2022-07-05 02:56:08,433 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:Metrics@659] - Metrics scheduler closed
2022-07-05 02:56:08,433 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:56:08,433 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:Metrics@669] - Metrics reporters closed
2022-07-05 02:56:08,434 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:AppInfoParser@83] - App info kafka.consumer for appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1-consumer unregistered
2022-07-05 02:56:08,434 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:Metrics@659] - Metrics scheduler closed
2022-07-05 02:56:08,434 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:56:08,434 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:Metrics@669] - Metrics reporters closed
2022-07-05 02:56:08,436 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:AppInfoParser@83] - App info kafka.consumer for appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1-restore-consumer unregistered
2022-07-05 02:56:08,436 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:StreamThread@230] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1] State transition from PENDING_SHUTDOWN to DEAD
2022-07-05 02:56:08,436 - INFO  [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1:StreamThread@1100] - stream-thread [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-StreamThread-1] Shutdown complete
2022-07-05 02:56:08,436 - ERROR [kafka-streams-close-thread:StateDirectory@414] - Some task directories still locked while closing state, this indicates unclean shutdown: {}
2022-07-05 02:56:08,436 - INFO  [kafka-admin-client-thread | appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-admin:AppInfoParser@83] - App info kafka.admin.client for appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-admin unregistered
2022-07-05 02:56:08,437 - INFO  [kafka-admin-client-thread | appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-admin:Metrics@659] - Metrics scheduler closed
2022-07-05 02:56:08,437 - INFO  [kafka-admin-client-thread | appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-admin:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:56:08,437 - INFO  [kafka-admin-client-thread | appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046-admin:Metrics@669] - Metrics reporters closed
2022-07-05 02:56:08,438 - INFO  [kafka-streams-close-thread:Metrics@659] - Metrics scheduler closed
2022-07-05 02:56:08,438 - INFO  [kafka-streams-close-thread:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:56:08,438 - INFO  [kafka-streams-close-thread:Metrics@669] - Metrics reporters closed
2022-07-05 02:56:08,438 - INFO  [kafka-streams-close-thread:KafkaStreams@321] - stream-client [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046] State transition from PENDING_SHUTDOWN to NOT_RUNNING
2022-07-05 02:56:08,438 - INFO  [main:KafkaStreams@1367] - stream-client [appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-f8581bf3-f2a6-421b-9b71-c8a609d4a046] Streams client stopped completely
2022-07-05 02:56:08,441 - INFO  [main:Logging@66] - [KafkaServer id=0] shutting down
2022-07-05 02:56:08,442 - INFO  [main:Logging@66] - [KafkaServer id=0] Starting controlled shutdown
2022-07-05 02:56:08,452 - INFO  [main:Logging@66] - [KafkaServer id=0] Controlled shutdown succeeded
2022-07-05 02:56:08,453 - INFO  [main:Logging@66] - [/config/changes-event-process-thread]: Shutting down
2022-07-05 02:56:08,454 - INFO  [main:Logging@66] - [/config/changes-event-process-thread]: Shutdown completed
2022-07-05 02:56:08,454 - INFO  [/config/changes-event-process-thread:Logging@66] - [/config/changes-event-process-thread]: Stopped
2022-07-05 02:56:08,454 - INFO  [main:Logging@66] - [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopping socket server request processors
2022-07-05 02:56:08,458 - INFO  [main:Logging@66] - [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopped socket server request processors
2022-07-05 02:56:08,459 - INFO  [main:Logging@66] - [data-plane Kafka Request Handler on Broker 0], shutting down
2022-07-05 02:56:08,460 - INFO  [main:Logging@66] - [data-plane Kafka Request Handler on Broker 0], shut down completely
2022-07-05 02:56:08,461 - INFO  [main:Logging@66] - [ExpirationReaper-0-AlterAcls]: Shutting down
2022-07-05 02:56:08,590 - INFO  [ExpirationReaper-0-AlterAcls:Logging@66] - [ExpirationReaper-0-AlterAcls]: Stopped
2022-07-05 02:56:08,590 - INFO  [main:Logging@66] - [ExpirationReaper-0-AlterAcls]: Shutdown completed
2022-07-05 02:56:08,591 - INFO  [main:Logging@66] - [KafkaApi-0] Shutdown complete.
2022-07-05 02:56:08,591 - INFO  [main:Logging@66] - [ExpirationReaper-0-topic]: Shutting down
2022-07-05 02:56:08,709 - INFO  [ExpirationReaper-0-topic:Logging@66] - [ExpirationReaper-0-topic]: Stopped
2022-07-05 02:56:08,709 - INFO  [main:Logging@66] - [ExpirationReaper-0-topic]: Shutdown completed
2022-07-05 02:56:08,710 - INFO  [main:Logging@66] - [TransactionCoordinator id=0] Shutting down.
2022-07-05 02:56:08,711 - INFO  [main:Logging@66] - [ProducerId Manager 0]: Shutdown complete: last producerId assigned 0
2022-07-05 02:56:08,711 - INFO  [main:Logging@66] - [Transaction State Manager 0]: Shutdown complete
2022-07-05 02:56:08,711 - INFO  [main:Logging@66] - [Transaction Marker Channel Manager 0]: Shutting down
2022-07-05 02:56:08,711 - INFO  [TxnMarkerSenderThread-0:Logging@66] - [Transaction Marker Channel Manager 0]: Stopped
2022-07-05 02:56:08,711 - INFO  [main:Logging@66] - [Transaction Marker Channel Manager 0]: Shutdown completed
2022-07-05 02:56:08,712 - INFO  [main:Logging@66] - [TransactionCoordinator id=0] Shutdown complete.
2022-07-05 02:56:08,712 - INFO  [main:Logging@66] - [GroupCoordinator 0]: Shutting down.
2022-07-05 02:56:08,712 - INFO  [main:Logging@66] - [ExpirationReaper-0-Heartbeat]: Shutting down
2022-07-05 02:56:08,714 - INFO  [ExpirationReaper-0-Heartbeat:Logging@66] - [ExpirationReaper-0-Heartbeat]: Stopped
2022-07-05 02:56:08,714 - INFO  [main:Logging@66] - [ExpirationReaper-0-Heartbeat]: Shutdown completed
2022-07-05 02:56:08,714 - INFO  [main:Logging@66] - [ExpirationReaper-0-Rebalance]: Shutting down
2022-07-05 02:56:08,789 - INFO  [ExpirationReaper-0-Rebalance:Logging@66] - [ExpirationReaper-0-Rebalance]: Stopped
2022-07-05 02:56:08,789 - INFO  [main:Logging@66] - [ExpirationReaper-0-Rebalance]: Shutdown completed
2022-07-05 02:56:08,789 - INFO  [main:Logging@66] - [GroupCoordinator 0]: Shutdown complete.
2022-07-05 02:56:08,790 - INFO  [main:Logging@66] - [ReplicaManager broker=0] Shutting down
2022-07-05 02:56:08,790 - INFO  [main:Logging@66] - [LogDirFailureHandler]: Shutting down
2022-07-05 02:56:08,790 - INFO  [main:Logging@66] - [LogDirFailureHandler]: Shutdown completed
2022-07-05 02:56:08,790 - INFO  [LogDirFailureHandler:Logging@66] - [LogDirFailureHandler]: Stopped
2022-07-05 02:56:08,791 - INFO  [main:Logging@66] - [ReplicaFetcherManager on broker 0] shutting down
2022-07-05 02:56:08,792 - INFO  [main:Logging@66] - [ReplicaFetcherManager on broker 0] shutdown completed
2022-07-05 02:56:08,792 - INFO  [main:Logging@66] - [ReplicaAlterLogDirsManager on broker 0] shutting down
2022-07-05 02:56:08,792 - INFO  [main:Logging@66] - [ReplicaAlterLogDirsManager on broker 0] shutdown completed
2022-07-05 02:56:08,792 - INFO  [main:Logging@66] - [ExpirationReaper-0-Fetch]: Shutting down
2022-07-05 02:56:08,806 - INFO  [ExpirationReaper-0-Fetch:Logging@66] - [ExpirationReaper-0-Fetch]: Stopped
2022-07-05 02:56:08,806 - INFO  [main:Logging@66] - [ExpirationReaper-0-Fetch]: Shutdown completed
2022-07-05 02:56:08,806 - INFO  [main:Logging@66] - [ExpirationReaper-0-Produce]: Shutting down
2022-07-05 02:56:08,960 - INFO  [ExpirationReaper-0-Produce:Logging@66] - [ExpirationReaper-0-Produce]: Stopped
2022-07-05 02:56:08,960 - INFO  [main:Logging@66] - [ExpirationReaper-0-Produce]: Shutdown completed
2022-07-05 02:56:08,961 - INFO  [main:Logging@66] - [ExpirationReaper-0-DeleteRecords]: Shutting down
2022-07-05 02:56:09,161 - INFO  [ExpirationReaper-0-DeleteRecords:Logging@66] - [ExpirationReaper-0-DeleteRecords]: Stopped
2022-07-05 02:56:09,161 - INFO  [main:Logging@66] - [ExpirationReaper-0-DeleteRecords]: Shutdown completed
2022-07-05 02:56:09,161 - INFO  [main:Logging@66] - [ExpirationReaper-0-ElectLeader]: Shutting down
2022-07-05 02:56:09,361 - INFO  [ExpirationReaper-0-ElectLeader:Logging@66] - [ExpirationReaper-0-ElectLeader]: Stopped
2022-07-05 02:56:09,361 - INFO  [main:Logging@66] - [ExpirationReaper-0-ElectLeader]: Shutdown completed
2022-07-05 02:56:09,363 - INFO  [main:Logging@66] - [ReplicaManager broker=0] Shut down completely
2022-07-05 02:56:09,363 - INFO  [main:Logging@66] - [BrokerToControllerChannelManager broker=0 name=alterIsr]: Shutting down
2022-07-05 02:56:09,364 - INFO  [BrokerToControllerChannelManager broker=0 name=alterIsr:Logging@66] - [BrokerToControllerChannelManager broker=0 name=alterIsr]: Stopped
2022-07-05 02:56:09,364 - INFO  [main:Logging@66] - [BrokerToControllerChannelManager broker=0 name=alterIsr]: Shutdown completed
2022-07-05 02:56:09,364 - INFO  [main:Logging@66] - Broker to controller channel manager for alterIsr shutdown
2022-07-05 02:56:09,365 - INFO  [main:Logging@66] - Shutting down.
2022-07-05 02:56:09,375 - INFO  [pool-8-thread-1:Logging@66] - [ProducerStateManager partition=__consumer_offsets-3] Writing producer snapshot at offset 3
2022-07-05 02:56:09,390 - INFO  [pool-8-thread-1:Logging@66] - [ProducerStateManager partition=__consumer_offsets-0] Writing producer snapshot at offset 2
2022-07-05 02:56:09,407 - INFO  [pool-8-thread-1:Logging@66] - [ProducerStateManager partition=appId_EmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-test-store-changelog-0] Writing producer snapshot at offset 2
2022-07-05 02:56:09,423 - INFO  [pool-8-thread-1:Logging@66] - [ProducerStateManager partition=outputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0] Writing producer snapshot at offset 2
2022-07-05 02:56:09,440 - INFO  [pool-8-thread-1:Logging@66] - [ProducerStateManager partition=inputEmitOnChangeIntegrationTestshouldEmitSameRecordAfterFailover-0] Writing producer snapshot at offset 2
2022-07-05 02:56:09,466 - INFO  [main:Logging@66] - Shutdown complete.
2022-07-05 02:56:09,472 - INFO  [main:Logging@66] - [feature-zk-node-event-process-thread]: Shutting down
2022-07-05 02:56:09,472 - INFO  [feature-zk-node-event-process-thread:Logging@66] - [feature-zk-node-event-process-thread]: Stopped
2022-07-05 02:56:09,472 - INFO  [main:Logging@66] - [feature-zk-node-event-process-thread]: Shutdown completed
2022-07-05 02:56:09,473 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Closing.
2022-07-05 02:56:09,576 - INFO  [main:ZooKeeper@1422] - Session: 0x100a46b12920000 closed
2022-07-05 02:56:09,576 - INFO  [main-EventThread:ClientCnxn$EventThread@524] - EventThread shut down for session: 0x100a46b12920000
2022-07-05 02:56:09,577 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Closed.
2022-07-05 02:56:09,577 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Fetch]: Shutting down
2022-07-05 02:56:10,221 - INFO  [ThrottledChannelReaper-Fetch:Logging@66] - [ThrottledChannelReaper-Fetch]: Stopped
2022-07-05 02:56:10,221 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Fetch]: Shutdown completed
2022-07-05 02:56:10,222 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Produce]: Shutting down
2022-07-05 02:56:11,222 - INFO  [ThrottledChannelReaper-Produce:Logging@66] - [ThrottledChannelReaper-Produce]: Stopped
2022-07-05 02:56:11,222 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Produce]: Shutdown completed
2022-07-05 02:56:11,222 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Request]: Shutting down
2022-07-05 02:56:11,223 - INFO  [ThrottledChannelReaper-Request:Logging@66] - [ThrottledChannelReaper-Request]: Stopped
2022-07-05 02:56:11,223 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Request]: Shutdown completed
2022-07-05 02:56:11,223 - INFO  [main:Logging@66] - [ThrottledChannelReaper-ControllerMutation]: Shutting down
2022-07-05 02:56:11,224 - INFO  [ThrottledChannelReaper-ControllerMutation:Logging@66] - [ThrottledChannelReaper-ControllerMutation]: Stopped
2022-07-05 02:56:11,224 - INFO  [main:Logging@66] - [ThrottledChannelReaper-ControllerMutation]: Shutdown completed
2022-07-05 02:56:11,225 - INFO  [main:Logging@66] - [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutting down socket server
2022-07-05 02:56:11,246 - INFO  [main:Logging@66] - [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutdown completed
2022-07-05 02:56:11,246 - INFO  [main:Metrics@659] - Metrics scheduler closed
2022-07-05 02:56:11,246 - INFO  [main:Metrics@663] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-07-05 02:56:11,247 - INFO  [main:Metrics@669] - Metrics reporters closed
2022-07-05 02:56:11,248 - INFO  [main:Logging@66] - Broker and topic stats closed
2022-07-05 02:56:11,249 - INFO  [main:AppInfoParser@83] - App info kafka.server for 0 unregistered
2022-07-05 02:56:11,249 - INFO  [main:Logging@66] - [KafkaServer id=0] shut down completed
2022-07-05 02:56:11,347 - INFO  [ConnnectionExpirer:NIOServerCnxnFactory$ConnectionExpirerThread@583] - ConnnectionExpirerThread interrupted
2022-07-05 02:56:11,347 - INFO  [NIOServerCxnFactory.AcceptThread:/127.0.0.1:0:NIOServerCnxnFactory$AcceptThread@219] - accept thread exitted run method
2022-07-05 02:56:11,348 - INFO  [NIOServerCxnFactory.SelectorThread-0:NIOServerCnxnFactory$SelectorThread@420] - selector thread exitted run method
2022-07-05 02:56:11,348 - INFO  [NIOServerCxnFactory.SelectorThread-1:NIOServerCnxnFactory$SelectorThread@420] - selector thread exitted run method
2022-07-05 02:56:11,348 - INFO  [NIOServerCxnFactory.SelectorThread-2:NIOServerCnxnFactory$SelectorThread@420] - selector thread exitted run method
2022-07-05 02:56:11,348 - INFO  [main:ZooKeeperServer@573] - shutting down
2022-07-05 02:56:11,349 - INFO  [main:SessionTrackerImpl@237] - Shutting down
2022-07-05 02:56:11,349 - INFO  [main:PrepRequestProcessor@1008] - Shutting down
2022-07-05 02:56:11,349 - INFO  [main:SyncRequestProcessor@191] - Shutting down
2022-07-05 02:56:11,349 - INFO  [ProcessThread(sid:0 cport:39129)::PrepRequestProcessor@156] - PrepRequestProcessor exited loop!
2022-07-05 02:56:11,349 - INFO  [SyncThread:0:SyncRequestProcessor@169] - SyncRequestProcessor exited!
2022-07-05 02:56:11,349 - INFO  [main:FinalRequestProcessor@514] - shutdown of request processor complete
