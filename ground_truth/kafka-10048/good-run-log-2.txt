2023-07-30 13:49:38,423 - INFO  [main:AbstractConfig@361] - MirrorMakerConfig values: 
	clusters = [primary, backup]
	config.providers = []
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:38,657 - INFO  [main:Reflections@239] - Reflections took 204 ms to scan 1 urls, producing 349 keys and 1055 values [using 20 cores]
2023-07-30 13:49:38,665 - WARN  [main:ReflectionUtils@318] - could not get type for name kotlin.jvm.internal.Lambda from any class loader
org.reflections.ReflectionsException: could not get type for name kotlin.jvm.internal.Lambda
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.mirror.MirrorMakerConfig.<init>(MirrorMakerConfig.java:80)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:110)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: kotlin.jvm.internal.Lambda
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 64 more
2023-07-30 13:49:38,672 - WARN  [main:ReflectionUtils@318] - could not get type for name kotlin.jvm.functions.Function0 from any class loader
org.reflections.ReflectionsException: could not get type for name kotlin.jvm.functions.Function0
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.mirror.MirrorMakerConfig.<init>(MirrorMakerConfig.java:80)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:110)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: kotlin.jvm.functions.Function0
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 64 more
2023-07-30 13:49:38,675 - WARN  [main:ReflectionUtils@318] - could not get type for name java.util.spi.ToolProvider from any class loader
org.reflections.ReflectionsException: could not get type for name java.util.spi.ToolProvider
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.mirror.MirrorMakerConfig.<init>(MirrorMakerConfig.java:80)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:110)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: java.util.spi.ToolProvider
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 64 more
2023-07-30 13:49:38,685 - WARN  [main:AppInfoParser@46] - Error while loading kafka-version.properties: null
2023-07-30 13:49:38,697 - INFO  [main:DelegatingClassLoader@269] - Registered loader: java.net.FactoryURLClassLoader@483bf400
2023-07-30 13:49:38,697 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension'
2023-07-30 13:49:38,697 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.runtime.isolation.PluginsTest$TestConnectRestExtension'
2023-07-30 13:49:38,698 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy'
2023-07-30 13:49:38,698 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy'
2023-07-30 13:49:38,698 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy'
2023-07-30 13:49:38,702 - INFO  [main:DelegatingClassLoader@427] - Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension'
2023-07-30 13:49:38,702 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'TestConnectRestExtension' and 'Test' to plugin 'org.apache.kafka.connect.runtime.isolation.PluginsTest$TestConnectRestExtension'
2023-07-30 13:49:38,702 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy'
2023-07-30 13:49:38,702 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy'
2023-07-30 13:49:38,703 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy'
2023-07-30 13:49:38,810 - INFO  [main:Log4jControllerRegistration$@31] - Registered kafka:type=kafka.Log4jController MBean
2023-07-30 13:49:38,871 - INFO  [main:Environment@109] - Server environment:zookeeper.version=3.5.8-f439ca583e70862c3068a1f2a7d4d068eec33315, built on 05/04/2020 15:53 GMT
2023-07-30 13:49:38,871 - INFO  [main:Environment@109] - Server environment:host.name=razor15
2023-07-30 13:49:38,871 - INFO  [main:Environment@109] - Server environment:java.version=1.8.0_275
2023-07-30 13:49:38,872 - INFO  [main:Environment@109] - Server environment:java.vendor=Private Build
2023-07-30 13:49:38,872 - INFO  [main:Environment@109] - Server environment:java.home=/usr/lib/jvm/java-8-openjdk-amd64/jre
2023-07-30 13:49:38,872 - INFO  [main:Environment@109] - Server environment:java.class.path=/home/tonypan/test-env/flaky-reproduction/experiment/kafka-10048/junit-platform-console-standalone-1.7.0.jar
2023-07-30 13:49:38,872 - INFO  [main:Environment@109] - Server environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib
2023-07-30 13:49:38,872 - INFO  [main:Environment@109] - Server environment:java.io.tmpdir=/tmp
2023-07-30 13:49:38,872 - INFO  [main:Environment@109] - Server environment:java.compiler=<NA>
2023-07-30 13:49:38,872 - INFO  [main:Environment@109] - Server environment:os.name=Linux
2023-07-30 13:49:38,872 - INFO  [main:Environment@109] - Server environment:os.arch=amd64
2023-07-30 13:49:38,873 - INFO  [main:Environment@109] - Server environment:os.version=4.15.0-128-generic
2023-07-30 13:49:38,873 - INFO  [main:Environment@109] - Server environment:user.name=tonypan
2023-07-30 13:49:38,873 - INFO  [main:Environment@109] - Server environment:user.home=/home/tonypan
2023-07-30 13:49:38,873 - INFO  [main:Environment@109] - Server environment:user.dir=/home/tonypan/test-env/flaky-reproduction/experiment/kafka-10048
2023-07-30 13:49:38,873 - INFO  [main:Environment@109] - Server environment:os.memory.free=371MB
2023-07-30 13:49:38,873 - INFO  [main:Environment@109] - Server environment:os.memory.max=7051MB
2023-07-30 13:49:38,873 - INFO  [main:Environment@109] - Server environment:os.memory.total=475MB
2023-07-30 13:49:38,877 - INFO  [main:FileTxnSnapLog@115] - zookeeper.snapshot.trust.empty : false
2023-07-30 13:49:38,896 - INFO  [main:ZKDatabase@117] - zookeeper.snapshotSizeFactor = 0.33
2023-07-30 13:49:38,898 - INFO  [main:ZooKeeperServer@938] - minSessionTimeout set to 1600
2023-07-30 13:49:38,898 - INFO  [main:ZooKeeperServer@947] - maxSessionTimeout set to 16000
2023-07-30 13:49:38,899 - INFO  [main:ZooKeeperServer@166] - Created server with tickTime 800 minSessionTimeout 1600 maxSessionTimeout 16000 datadir /tmp/kafka-366175305677720227/version-2 snapdir /tmp/kafka-5248596440953144640/version-2
2023-07-30 13:49:38,909 - INFO  [main:NIOServerCnxnFactory@673] - Configuring NIO connection handler with 10s sessionless connection timeout, 3 selector thread(s), 40 worker threads, and 64 kB direct buffers.
2023-07-30 13:49:38,915 - INFO  [main:NIOServerCnxnFactory@686] - binding to port /127.0.0.1:0
2023-07-30 13:49:38,923 - INFO  [main:FileTxnSnapLog@404] - Snapshotting: 0x0 to /tmp/kafka-5248596440953144640/version-2/snapshot.0
2023-07-30 13:49:38,926 - INFO  [main:FileTxnSnapLog@404] - Snapshotting: 0x0 to /tmp/kafka-5248596440953144640/version-2/snapshot.0
2023-07-30 13:49:39,287 - INFO  [main:AbstractConfig@361] - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.7-IV2
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/junit6293896813403981889/junit8934247217114872089
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.7-IV2
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:44067
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

2023-07-30 13:49:39,305 - INFO  [main:X509Util@79] - Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation
2023-07-30 13:49:39,368 - INFO  [main:Logging@66] - starting
2023-07-30 13:49:39,369 - INFO  [main:Logging@66] - Connecting to zookeeper on 127.0.0.1:44067
2023-07-30 13:49:39,393 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:44067.
2023-07-30 13:49:39,398 - INFO  [main:Environment@109] - Client environment:zookeeper.version=3.5.8-f439ca583e70862c3068a1f2a7d4d068eec33315, built on 05/04/2020 15:53 GMT
2023-07-30 13:49:39,399 - INFO  [main:Environment@109] - Client environment:host.name=razor15
2023-07-30 13:49:39,399 - INFO  [main:Environment@109] - Client environment:java.version=1.8.0_275
2023-07-30 13:49:39,399 - INFO  [main:Environment@109] - Client environment:java.vendor=Private Build
2023-07-30 13:49:39,399 - INFO  [main:Environment@109] - Client environment:java.home=/usr/lib/jvm/java-8-openjdk-amd64/jre
2023-07-30 13:49:39,399 - INFO  [main:Environment@109] - Client environment:java.class.path=/home/tonypan/test-env/flaky-reproduction/experiment/kafka-10048/junit-platform-console-standalone-1.7.0.jar
2023-07-30 13:49:39,399 - INFO  [main:Environment@109] - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib
2023-07-30 13:49:39,399 - INFO  [main:Environment@109] - Client environment:java.io.tmpdir=/tmp
2023-07-30 13:49:39,400 - INFO  [main:Environment@109] - Client environment:java.compiler=<NA>
2023-07-30 13:49:39,400 - INFO  [main:Environment@109] - Client environment:os.name=Linux
2023-07-30 13:49:39,400 - INFO  [main:Environment@109] - Client environment:os.arch=amd64
2023-07-30 13:49:39,400 - INFO  [main:Environment@109] - Client environment:os.version=4.15.0-128-generic
2023-07-30 13:49:39,400 - INFO  [main:Environment@109] - Client environment:user.name=tonypan
2023-07-30 13:49:39,400 - INFO  [main:Environment@109] - Client environment:user.home=/home/tonypan
2023-07-30 13:49:39,400 - INFO  [main:Environment@109] - Client environment:user.dir=/home/tonypan/test-env/flaky-reproduction/experiment/kafka-10048
2023-07-30 13:49:39,400 - INFO  [main:Environment@109] - Client environment:os.memory.free=218MB
2023-07-30 13:49:39,401 - INFO  [main:Environment@109] - Client environment:os.memory.max=7051MB
2023-07-30 13:49:39,401 - INFO  [main:Environment@109] - Client environment:os.memory.total=333MB
2023-07-30 13:49:39,404 - INFO  [main:ZooKeeper@868] - Initiating client connection, connectString=127.0.0.1:44067 sessionTimeout=18000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@3b96c42e
2023-07-30 13:49:39,408 - INFO  [main:ClientCnxnSocket@237] - jute.maxbuffer value is 4194304 Bytes
2023-07-30 13:49:39,415 - INFO  [main:ClientCnxn@1653] - zookeeper.request.timeout value is 0. feature enabled=
2023-07-30 13:49:39,417 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Waiting until connected.
2023-07-30 13:49:39,422 - INFO  [main-SendThread(127.0.0.1:44067):ClientCnxn$SendThread@1112] - Opening socket connection to server localhost/127.0.0.1:44067. Will not attempt to authenticate using SASL (unknown error)
2023-07-30 13:49:39,424 - INFO  [main-SendThread(127.0.0.1:44067):ClientCnxn$SendThread@959] - Socket connection established, initiating session, client: /127.0.0.1:44080, server: localhost/127.0.0.1:44067
2023-07-30 13:49:39,433 - INFO  [SyncThread:0:FileTxnLog@218] - Creating new log file: log.1
2023-07-30 13:49:39,444 - INFO  [main-SendThread(127.0.0.1:44067):ClientCnxn$SendThread@1394] - Session establishment complete on server localhost/127.0.0.1:44067, sessionid = 0x1087f31955f0000, negotiated timeout = 16000
2023-07-30 13:49:39,449 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Connected.
2023-07-30 13:49:39,532 - INFO  [feature-zk-node-event-process-thread:Logging@66] - [feature-zk-node-event-process-thread]: Starting
2023-07-30 13:49:39,543 - INFO  [feature-zk-node-event-process-thread:Logging@66] - Feature ZK node at path: /feature does not exist
2023-07-30 13:49:39,544 - INFO  [feature-zk-node-event-process-thread:FinalizedFeatureCache$@42] - Cleared cache
2023-07-30 13:49:39,784 - INFO  [main:Logging@66] - Cluster ID = XnnaS7BkQ22Tt1TiwnBd0Q
2023-07-30 13:49:39,788 - WARN  [main:Logging@70] - No meta.properties file under dir /tmp/junit6293896813403981889/junit8934247217114872089/meta.properties
2023-07-30 13:49:39,842 - INFO  [main:AbstractConfig@361] - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.7-IV2
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/junit6293896813403981889/junit8934247217114872089
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.7-IV2
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:44067
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

2023-07-30 13:49:39,852 - INFO  [main:AbstractConfig@361] - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.7-IV2
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/junit6293896813403981889/junit8934247217114872089
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.7-IV2
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:44067
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

2023-07-30 13:49:39,883 - INFO  [ThrottledChannelReaper-Fetch:Logging@66] - [ThrottledChannelReaper-Fetch]: Starting
2023-07-30 13:49:39,884 - INFO  [ThrottledChannelReaper-Produce:Logging@66] - [ThrottledChannelReaper-Produce]: Starting
2023-07-30 13:49:39,885 - INFO  [ThrottledChannelReaper-Request:Logging@66] - [ThrottledChannelReaper-Request]: Starting
2023-07-30 13:49:39,887 - INFO  [ThrottledChannelReaper-ControllerMutation:Logging@66] - [ThrottledChannelReaper-ControllerMutation]: Starting
2023-07-30 13:49:39,928 - INFO  [main:Logging@66] - Loading logs from log dirs ArraySeq(/tmp/junit6293896813403981889/junit8934247217114872089)
2023-07-30 13:49:39,932 - INFO  [main:Logging@66] - Attempting recovery for all logs in /tmp/junit6293896813403981889/junit8934247217114872089 since no clean shutdown file was found
2023-07-30 13:49:39,938 - INFO  [main:Logging@66] - Loaded 0 logs in 0ms.
2023-07-30 13:49:39,956 - INFO  [main:Logging@66] - Starting log cleanup with a period of 300000 ms.
2023-07-30 13:49:39,960 - INFO  [main:Logging@66] - Starting log flusher with a default period of 9223372036854775807 ms.
2023-07-30 13:49:40,483 - INFO  [main:Logging@66] - Created ConnectionAcceptRate sensor, quotaLimit=2147483647
2023-07-30 13:49:40,486 - INFO  [main:Logging@66] - Created ConnectionAcceptRate-PLAINTEXT sensor, quotaLimit=2147483647
2023-07-30 13:49:40,489 - INFO  [main:Logging@66] - Updated PLAINTEXT max connection creation rate to 2147483647
2023-07-30 13:49:40,492 - INFO  [main:Logging@66] - Awaiting socket connections on localhost:35687.
2023-07-30 13:49:40,525 - INFO  [main:Logging@66] - [SocketServer brokerId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT)
2023-07-30 13:49:40,556 - INFO  [ExpirationReaper-0-Produce:Logging@66] - [ExpirationReaper-0-Produce]: Starting
2023-07-30 13:49:40,556 - INFO  [ExpirationReaper-0-Fetch:Logging@66] - [ExpirationReaper-0-Fetch]: Starting
2023-07-30 13:49:40,556 - INFO  [ExpirationReaper-0-DeleteRecords:Logging@66] - [ExpirationReaper-0-DeleteRecords]: Starting
2023-07-30 13:49:40,556 - INFO  [ExpirationReaper-0-ElectLeader:Logging@66] - [ExpirationReaper-0-ElectLeader]: Starting
2023-07-30 13:49:40,568 - INFO  [LogDirFailureHandler:Logging@66] - [LogDirFailureHandler]: Starting
2023-07-30 13:49:40,568 - INFO  [broker-0-to-controller-send-thread:Logging@66] - [broker-0-to-controller-send-thread]: Starting
2023-07-30 13:49:40,600 - INFO  [main:Logging@66] - Creating /brokers/ids/0 (is it secure? false)
2023-07-30 13:49:40,621 - INFO  [main:Logging@66] - Stat of the created znode at /brokers/ids/0 is: 24,24,1690739380615,1690739380615,1,0,0,74449244787769344,204,0,24

2023-07-30 13:49:40,622 - INFO  [main:Logging@66] - Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://localhost:35687, czxid (broker epoch): 24
2023-07-30 13:49:40,689 - INFO  [ExpirationReaper-0-topic:Logging@66] - [ExpirationReaper-0-topic]: Starting
2023-07-30 13:49:40,693 - INFO  [ExpirationReaper-0-Heartbeat:Logging@66] - [ExpirationReaper-0-Heartbeat]: Starting
2023-07-30 13:49:40,694 - INFO  [ExpirationReaper-0-Rebalance:Logging@66] - [ExpirationReaper-0-Rebalance]: Starting
2023-07-30 13:49:40,696 - INFO  [controller-event-thread:Logging@66] - Successfully created /controller_epoch with initial epoch 0
2023-07-30 13:49:40,717 - INFO  [main:Logging@66] - [GroupCoordinator 0]: Starting up.
2023-07-30 13:49:40,718 - INFO  [main:Logging@66] - [GroupCoordinator 0]: Startup complete.
2023-07-30 13:49:40,730 - INFO  [main:Logging@66] - [ProducerId Manager 0]: Acquired new producerId block (brokerId:0,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1
2023-07-30 13:49:40,744 - INFO  [main:Logging@66] - [TransactionCoordinator id=0] Starting up.
2023-07-30 13:49:40,746 - INFO  [TxnMarkerSenderThread-0:Logging@66] - [Transaction Marker Channel Manager 0]: Starting
2023-07-30 13:49:40,746 - INFO  [main:Logging@66] - [TransactionCoordinator id=0] Startup complete.
2023-07-30 13:49:40,769 - INFO  [ExpirationReaper-0-AlterAcls:Logging@66] - [ExpirationReaper-0-AlterAcls]: Starting
2023-07-30 13:49:40,789 - INFO  [/config/changes-event-process-thread:Logging@66] - [/config/changes-event-process-thread]: Starting
2023-07-30 13:49:40,794 - INFO  [main:Logging@66] - [SocketServer brokerId=0] Starting socket server acceptors and processors
2023-07-30 13:49:40,798 - INFO  [main:Logging@66] - [SocketServer brokerId=0] Started data-plane acceptor and processor(s) for endpoint : ListenerName(PLAINTEXT)
2023-07-30 13:49:40,798 - INFO  [main:Logging@66] - [SocketServer brokerId=0] Started socket server acceptors and processors
2023-07-30 13:49:40,799 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:40,799 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:40,799 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739378720
2023-07-30 13:49:40,800 - INFO  [main:Logging@66] - [KafkaServer id=0] started
2023-07-30 13:49:40,808 - INFO  [main:AbstractConfig@361] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:35687]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:49:40,824 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:40,824 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:40,824 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739380824
2023-07-30 13:49:40,825 - INFO  [main:EmbeddedConnectCluster@235] - Starting Connect cluster 'primary-connect-cluster' with 3 workers
2023-07-30 13:49:40,828 - INFO  [main:ConnectDistributed@90] - Scanning for plugin classes. This might take a moment ...
2023-07-30 13:49:40,843 - INFO  [kafka-producer-network-thread | producer-1:Metadata@279] - [Producer clientId=producer-1] Cluster ID: XnnaS7BkQ22Tt1TiwnBd0Q
2023-07-30 13:49:40,869 - INFO  [broker-0-to-controller-send-thread:Logging@66] - [broker-0-to-controller-send-thread]: Recorded new controller, from now on will use broker 0
2023-07-30 13:49:40,946 - INFO  [main:Reflections@239] - Reflections took 116 ms to scan 1 urls, producing 349 keys and 1055 values [using 20 cores]
2023-07-30 13:49:40,948 - WARN  [main:ReflectionUtils@318] - could not get type for name kotlin.jvm.internal.Lambda from any class loader
org.reflections.ReflectionsException: could not get type for name kotlin.jvm.internal.Lambda
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:91)
	at org.apache.kafka.connect.util.clusters.WorkerHandle.start(WorkerHandle.java:50)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.addWorker(EmbeddedConnectCluster.java:167)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.startConnect(EmbeddedConnectCluster.java:253)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.start(EmbeddedConnectCluster.java:137)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:130)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: kotlin.jvm.internal.Lambda
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 68 more
2023-07-30 13:49:40,950 - WARN  [main:ReflectionUtils@318] - could not get type for name kotlin.jvm.functions.Function0 from any class loader
org.reflections.ReflectionsException: could not get type for name kotlin.jvm.functions.Function0
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:91)
	at org.apache.kafka.connect.util.clusters.WorkerHandle.start(WorkerHandle.java:50)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.addWorker(EmbeddedConnectCluster.java:167)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.startConnect(EmbeddedConnectCluster.java:253)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.start(EmbeddedConnectCluster.java:137)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:130)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: kotlin.jvm.functions.Function0
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 68 more
2023-07-30 13:49:40,952 - WARN  [main:ReflectionUtils@318] - could not get type for name java.util.spi.ToolProvider from any class loader
org.reflections.ReflectionsException: could not get type for name java.util.spi.ToolProvider
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:91)
	at org.apache.kafka.connect.util.clusters.WorkerHandle.start(WorkerHandle.java:50)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.addWorker(EmbeddedConnectCluster.java:167)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.startConnect(EmbeddedConnectCluster.java:253)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.start(EmbeddedConnectCluster.java:137)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:130)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: java.util.spi.ToolProvider
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 68 more
2023-07-30 13:49:40,956 - INFO  [main:DelegatingClassLoader@269] - Registered loader: java.net.FactoryURLClassLoader@483bf400
2023-07-30 13:49:40,956 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension'
2023-07-30 13:49:40,956 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.runtime.isolation.PluginsTest$TestConnectRestExtension'
2023-07-30 13:49:40,957 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy'
2023-07-30 13:49:40,957 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy'
2023-07-30 13:49:40,957 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy'
2023-07-30 13:49:40,957 - INFO  [main:DelegatingClassLoader@427] - Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension'
2023-07-30 13:49:40,957 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'TestConnectRestExtension' and 'Test' to plugin 'org.apache.kafka.connect.runtime.isolation.PluginsTest$TestConnectRestExtension'
2023-07-30 13:49:40,958 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy'
2023-07-30 13:49:40,958 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy'
2023-07-30 13:49:40,958 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy'
2023-07-30 13:49:40,994 - INFO  [main:AbstractConfig@361] - DistributedConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:35687]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	config.providers = []
	config.storage.replication.factor = 1
	config.storage.topic = mm2-configs.backup.internal
	connect.protocol = sessioned
	connections.max.idle.ms = 540000
	connector.client.config.override.policy = None
	group.id = backup-mm2
	header.converter = class org.apache.kafka.connect.converters.ByteArrayConverter
	heartbeat.interval.ms = 3000
	inter.worker.key.generation.algorithm = HmacSHA256
	inter.worker.key.size = null
	inter.worker.key.ttl.ms = 3600000
	inter.worker.signature.algorithm = HmacSHA256
	inter.worker.verification.algorithms = [HmacSHA256]
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.converters.ByteArrayConverter
	listeners = null
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 60000
	offset.flush.timeout.ms = 5000
	offset.storage.partitions = 25
	offset.storage.replication.factor = 1
	offset.storage.topic = mm2-offsets.backup.internal
	plugin.path = null
	rebalance.timeout.ms = 60000
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 40000
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = localhost
	rest.port = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	scheduled.rebalance.max.delay.ms = 300000
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	status.storage.partitions = 5
	status.storage.replication.factor = 1
	status.storage.topic = mm2-status.backup.internal
	task.shutdown.graceful.timeout.ms = 5000
	topic.creation.enable = true
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.converters.ByteArrayConverter
	worker.sync.timeout.ms = 3000
	worker.unsync.backoff.ms = 300000

2023-07-30 13:49:40,994 - WARN  [main:WorkerConfig@420] - Variables cannot be used in the 'plugin.path' property, since the property is used by plugin scanning before the config providers that replace the variables are initialized. The raw value 'null' was used for plugin scanning, as opposed to the transformed value 'null', and this may cause unexpected results.
2023-07-30 13:49:40,996 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:49:40,997 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:35687]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:41,018 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:41,018 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:41,018 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:41,018 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:41,018 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:41,018 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:41,018 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:41,018 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:41,018 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:41,018 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:41,019 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:41,019 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:41,019 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:41,019 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:41,019 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739381019
2023-07-30 13:49:41,030 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: XnnaS7BkQ22Tt1TiwnBd0Q
2023-07-30 13:49:41,030 - INFO  [kafka-admin-client-thread | adminclient-1:AppInfoParser@83] - App info kafka.admin.client for adminclient-1 unregistered
2023-07-30 13:49:41,033 - INFO  [kafka-admin-client-thread | adminclient-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:41,034 - INFO  [kafka-admin-client-thread | adminclient-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:41,034 - INFO  [kafka-admin-client-thread | adminclient-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:41,069 - INFO  [main:Log@169] - Logging initialized @3327ms to org.eclipse.jetty.util.log.Slf4jLog
2023-07-30 13:49:41,146 - INFO  [main:RestServer@132] - Added connector for http://localhost:0
2023-07-30 13:49:41,146 - INFO  [main:RestServer@204] - Initializing REST server
2023-07-30 13:49:41,171 - INFO  [main:Server@360] - jetty-9.4.30.v20200611; built: 2020-06-11T12:34:51.929Z; git: 271836e4c1f4612f12b7bb13ef5a92a927634b0d; jvm 1.8.0_275-8u275-b01-0ubuntu1~18.04-b01
2023-07-30 13:49:41,201 - INFO  [main:AbstractConnector@331] - Started http_localhost0@3113a37{HTTP/1.1, (http/1.1)}{localhost:43083}
2023-07-30 13:49:41,201 - INFO  [main:Server@400] - Started @3459ms
2023-07-30 13:49:41,244 - INFO  [main:RestServer@371] - Advertised URI: http://localhost:43083/
2023-07-30 13:49:41,244 - INFO  [main:RestServer@219] - REST server listening at http://localhost:43083/, advertising URL http://localhost:43083/
2023-07-30 13:49:41,244 - INFO  [main:RestServer@371] - Advertised URI: http://localhost:43083/
2023-07-30 13:49:41,244 - INFO  [main:RestServer@220] - REST admin endpoints at http://localhost:43083/
2023-07-30 13:49:41,245 - INFO  [main:RestServer@371] - Advertised URI: http://localhost:43083/
2023-07-30 13:49:41,246 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:49:41,246 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:35687]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:41,248 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:41,248 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:41,248 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:41,248 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:41,249 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:41,249 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:41,249 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:41,249 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:41,249 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:41,249 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:41,249 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:41,249 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:41,249 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:41,249 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:41,249 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739381249
2023-07-30 13:49:41,258 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: XnnaS7BkQ22Tt1TiwnBd0Q
2023-07-30 13:49:41,258 - INFO  [kafka-admin-client-thread | adminclient-2:AppInfoParser@83] - App info kafka.admin.client for adminclient-2 unregistered
2023-07-30 13:49:41,259 - INFO  [kafka-admin-client-thread | adminclient-2:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:41,259 - INFO  [kafka-admin-client-thread | adminclient-2:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:41,259 - INFO  [kafka-admin-client-thread | adminclient-2:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:41,263 - INFO  [main:NoneConnectorClientConfigOverridePolicy@45] - Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden
2023-07-30 13:49:41,271 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:49:41,271 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:35687]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:41,272 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:41,272 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:41,272 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:41,272 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:41,273 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:41,273 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:41,273 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:41,273 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:41,273 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:41,273 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:41,273 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:41,273 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:41,273 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:41,273 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:41,274 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739381273
2023-07-30 13:49:41,279 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: XnnaS7BkQ22Tt1TiwnBd0Q
2023-07-30 13:49:41,280 - INFO  [kafka-admin-client-thread | adminclient-3:AppInfoParser@83] - App info kafka.admin.client for adminclient-3 unregistered
2023-07-30 13:49:41,281 - INFO  [kafka-admin-client-thread | adminclient-3:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:41,281 - INFO  [kafka-admin-client-thread | adminclient-3:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:41,281 - INFO  [kafka-admin-client-thread | adminclient-3:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:41,283 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:41,283 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:41,283 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739381283
2023-07-30 13:49:41,297 - INFO  [main:AbstractConfig@361] - JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false

2023-07-30 13:49:41,298 - INFO  [main:AbstractConfig@361] - JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false

2023-07-30 13:49:41,298 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:49:41,299 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:35687]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:41,300 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:41,300 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:41,300 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:41,300 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:41,300 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:41,300 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:41,300 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:41,300 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:41,300 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:41,300 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:41,301 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:41,301 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:41,301 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:41,301 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:41,301 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739381301
2023-07-30 13:49:41,306 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: XnnaS7BkQ22Tt1TiwnBd0Q
2023-07-30 13:49:41,307 - INFO  [kafka-admin-client-thread | adminclient-4:AppInfoParser@83] - App info kafka.admin.client for adminclient-4 unregistered
2023-07-30 13:49:41,308 - INFO  [kafka-admin-client-thread | adminclient-4:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:41,308 - INFO  [kafka-admin-client-thread | adminclient-4:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:41,308 - INFO  [kafka-admin-client-thread | adminclient-4:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:41,313 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:49:41,314 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:35687]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:41,315 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:41,315 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:41,315 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:41,315 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:41,315 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:41,315 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:41,316 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:41,316 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:41,316 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:41,316 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:41,316 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:41,316 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:41,316 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:41,316 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:41,316 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739381316
2023-07-30 13:49:41,321 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: XnnaS7BkQ22Tt1TiwnBd0Q
2023-07-30 13:49:41,321 - INFO  [kafka-admin-client-thread | adminclient-5:AppInfoParser@83] - App info kafka.admin.client for adminclient-5 unregistered
2023-07-30 13:49:41,322 - INFO  [kafka-admin-client-thread | adminclient-5:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:41,322 - INFO  [kafka-admin-client-thread | adminclient-5:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:41,322 - INFO  [kafka-admin-client-thread | adminclient-5:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:41,325 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:49:41,325 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:35687]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:41,326 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:41,326 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:41,326 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:41,326 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:41,327 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:41,327 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:41,327 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:41,327 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:41,327 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:41,327 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:41,327 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:41,327 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:41,327 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:41,327 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:41,327 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739381327
2023-07-30 13:49:41,332 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: XnnaS7BkQ22Tt1TiwnBd0Q
2023-07-30 13:49:41,332 - INFO  [kafka-admin-client-thread | adminclient-6:AppInfoParser@83] - App info kafka.admin.client for adminclient-6 unregistered
2023-07-30 13:49:41,333 - INFO  [kafka-admin-client-thread | adminclient-6:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:41,333 - INFO  [kafka-admin-client-thread | adminclient-6:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:41,333 - INFO  [kafka-admin-client-thread | adminclient-6:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:41,346 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:49:41,346 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:35687]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:41,348 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:41,348 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:41,348 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:41,348 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:41,348 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:41,348 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:41,348 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:41,348 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:41,348 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:41,349 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:41,349 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:41,349 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:41,349 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:41,349 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:41,349 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739381349
2023-07-30 13:49:41,355 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: XnnaS7BkQ22Tt1TiwnBd0Q
2023-07-30 13:49:41,355 - INFO  [kafka-admin-client-thread | adminclient-7:AppInfoParser@83] - App info kafka.admin.client for adminclient-7 unregistered
2023-07-30 13:49:41,356 - INFO  [kafka-admin-client-thread | adminclient-7:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:41,356 - INFO  [kafka-admin-client-thread | adminclient-7:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:41,356 - INFO  [kafka-admin-client-thread | adminclient-7:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:41,370 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:41,370 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:41,370 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739381370
2023-07-30 13:49:41,373 - INFO  [main:ConnectDistributed@128] - Kafka Connect distributed worker initialization took 545ms
2023-07-30 13:49:41,373 - INFO  [main:Connect@51] - Kafka Connect starting
2023-07-30 13:49:41,373 - INFO  [main:RestServer@224] - Initializing REST resources
2023-07-30 13:49:41,373 - INFO  [DistributedHerder-connect-1-1:DistributedHerder@286] - [Worker clientId=connect-1, groupId=backup-mm2] Herder starting
2023-07-30 13:49:41,374 - INFO  [DistributedHerder-connect-1-1:Worker@195] - Worker starting
2023-07-30 13:49:41,374 - INFO  [DistributedHerder-connect-1-1:KafkaOffsetBackingStore@127] - Starting KafkaOffsetBackingStore
2023-07-30 13:49:41,374 - INFO  [DistributedHerder-connect-1-1:KafkaBasedLog@127] - Starting KafkaBasedLog with topic mm2-offsets.backup.internal
2023-07-30 13:49:41,374 - INFO  [DistributedHerder-connect-1-1:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:35687]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:41,375 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:41,375 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:49:41,375 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:41,375 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:41,375 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:41,375 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:41,375 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:41,375 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:41,375 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:41,375 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:41,375 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:41,375 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:41,375 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:41,375 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:41,376 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:41,376 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:41,376 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739381376
2023-07-30 13:49:41,414 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Creating topic mm2-offsets.backup.internal with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0), 5 -> ArrayBuffer(0), 6 -> ArrayBuffer(0), 7 -> ArrayBuffer(0), 8 -> ArrayBuffer(0), 9 -> ArrayBuffer(0), 10 -> ArrayBuffer(0), 11 -> ArrayBuffer(0), 12 -> ArrayBuffer(0), 13 -> ArrayBuffer(0), 14 -> ArrayBuffer(0), 15 -> ArrayBuffer(0), 16 -> ArrayBuffer(0), 17 -> ArrayBuffer(0), 18 -> ArrayBuffer(0), 19 -> ArrayBuffer(0), 20 -> ArrayBuffer(0), 21 -> ArrayBuffer(0), 22 -> ArrayBuffer(0), 23 -> ArrayBuffer(0), 24 -> ArrayBuffer(0))
2023-07-30 13:49:41,442 - INFO  [main:RestServer@241] - Adding admin resources to main listener
2023-07-30 13:49:41,506 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(mm2-offsets.backup.internal-20, mm2-offsets.backup.internal-12, mm2-offsets.backup.internal-4, mm2-offsets.backup.internal-19, mm2-offsets.backup.internal-24, mm2-offsets.backup.internal-1, mm2-offsets.backup.internal-9, mm2-offsets.backup.internal-22, mm2-offsets.backup.internal-3, mm2-offsets.backup.internal-2, mm2-offsets.backup.internal-11, mm2-offsets.backup.internal-16, mm2-offsets.backup.internal-8, mm2-offsets.backup.internal-23, mm2-offsets.backup.internal-14, mm2-offsets.backup.internal-6, mm2-offsets.backup.internal-7, mm2-offsets.backup.internal-15, mm2-offsets.backup.internal-0, mm2-offsets.backup.internal-10, mm2-offsets.backup.internal-18, mm2-offsets.backup.internal-21, mm2-offsets.backup.internal-5, mm2-offsets.backup.internal-17, mm2-offsets.backup.internal-13)
2023-07-30 13:49:41,540 - INFO  [main:DefaultSessionIdManager@334] - DefaultSessionIdManager workerName=node0
2023-07-30 13:49:41,540 - INFO  [main:DefaultSessionIdManager@339] - No SessionScavenger set, using defaults
2023-07-30 13:49:41,542 - INFO  [main:HouseKeeper@140] - node0 Scavenging every 600000ms
2023-07-30 13:49:41,566 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-offsets.backup.internal-16, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:41,574 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-offsets.backup.internal-16 in /tmp/junit6293896813403981889/junit8934247217114872089/mm2-offsets.backup.internal-16 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:41,575 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-16 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-16
2023-07-30 13:49:41,575 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-16 broker=0] Log loaded for partition mm2-offsets.backup.internal-16 with initial high watermark 0
2023-07-30 13:49:41,588 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-offsets.backup.internal-12, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:41,589 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-offsets.backup.internal-12 in /tmp/junit6293896813403981889/junit8934247217114872089/mm2-offsets.backup.internal-12 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:41,589 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-12 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-12
2023-07-30 13:49:41,589 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-12 broker=0] Log loaded for partition mm2-offsets.backup.internal-12 with initial high watermark 0
2023-07-30 13:49:41,592 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-offsets.backup.internal-23, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:41,593 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-offsets.backup.internal-23 in /tmp/junit6293896813403981889/junit8934247217114872089/mm2-offsets.backup.internal-23 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:41,593 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-23 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-23
2023-07-30 13:49:41,593 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-23 broker=0] Log loaded for partition mm2-offsets.backup.internal-23 with initial high watermark 0
2023-07-30 13:49:41,601 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-offsets.backup.internal-8, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:41,602 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-offsets.backup.internal-8 in /tmp/junit6293896813403981889/junit8934247217114872089/mm2-offsets.backup.internal-8 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:41,602 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-8 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-8
2023-07-30 13:49:41,602 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-8 broker=0] Log loaded for partition mm2-offsets.backup.internal-8 with initial high watermark 0
2023-07-30 13:49:41,609 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-offsets.backup.internal-19, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:41,610 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-offsets.backup.internal-19 in /tmp/junit6293896813403981889/junit8934247217114872089/mm2-offsets.backup.internal-19 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:41,610 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-19 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-19
2023-07-30 13:49:41,610 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-19 broker=0] Log loaded for partition mm2-offsets.backup.internal-19 with initial high watermark 0
2023-07-30 13:49:41,617 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-offsets.backup.internal-4, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:41,618 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-offsets.backup.internal-4 in /tmp/junit6293896813403981889/junit8934247217114872089/mm2-offsets.backup.internal-4 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:41,618 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-4 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-4
2023-07-30 13:49:41,618 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-4 broker=0] Log loaded for partition mm2-offsets.backup.internal-4 with initial high watermark 0
2023-07-30 13:49:41,625 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-offsets.backup.internal-13, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:41,626 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-offsets.backup.internal-13 in /tmp/junit6293896813403981889/junit8934247217114872089/mm2-offsets.backup.internal-13 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:41,626 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-13 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-13
2023-07-30 13:49:41,626 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-13 broker=0] Log loaded for partition mm2-offsets.backup.internal-13 with initial high watermark 0
2023-07-30 13:49:41,634 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-offsets.backup.internal-9, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:41,635 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-offsets.backup.internal-9 in /tmp/junit6293896813403981889/junit8934247217114872089/mm2-offsets.backup.internal-9 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:41,635 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-9 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-9
2023-07-30 13:49:41,635 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-9 broker=0] Log loaded for partition mm2-offsets.backup.internal-9 with initial high watermark 0
2023-07-30 13:49:41,642 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-offsets.backup.internal-24, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:41,643 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-offsets.backup.internal-24 in /tmp/junit6293896813403981889/junit8934247217114872089/mm2-offsets.backup.internal-24 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:41,643 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-24 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-24
2023-07-30 13:49:41,643 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-24 broker=0] Log loaded for partition mm2-offsets.backup.internal-24 with initial high watermark 0
2023-07-30 13:49:41,650 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-offsets.backup.internal-5, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:41,651 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-offsets.backup.internal-5 in /tmp/junit6293896813403981889/junit8934247217114872089/mm2-offsets.backup.internal-5 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:41,651 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-5 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-5
2023-07-30 13:49:41,651 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-5 broker=0] Log loaded for partition mm2-offsets.backup.internal-5 with initial high watermark 0
2023-07-30 13:49:41,659 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-offsets.backup.internal-20, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:41,659 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-offsets.backup.internal-20 in /tmp/junit6293896813403981889/junit8934247217114872089/mm2-offsets.backup.internal-20 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:41,660 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-20 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-20
2023-07-30 13:49:41,660 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-20 broker=0] Log loaded for partition mm2-offsets.backup.internal-20 with initial high watermark 0
2023-07-30 13:49:41,667 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-offsets.backup.internal-1, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:41,668 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-offsets.backup.internal-1 in /tmp/junit6293896813403981889/junit8934247217114872089/mm2-offsets.backup.internal-1 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:41,668 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-1 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-1
2023-07-30 13:49:41,668 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-1 broker=0] Log loaded for partition mm2-offsets.backup.internal-1 with initial high watermark 0
2023-07-30 13:49:41,675 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-offsets.backup.internal-14, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:41,676 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-offsets.backup.internal-14 in /tmp/junit6293896813403981889/junit8934247217114872089/mm2-offsets.backup.internal-14 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:41,676 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-14 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-14
2023-07-30 13:49:41,676 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-14 broker=0] Log loaded for partition mm2-offsets.backup.internal-14 with initial high watermark 0
2023-07-30 13:49:41,684 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-offsets.backup.internal-10, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:41,684 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-offsets.backup.internal-10 in /tmp/junit6293896813403981889/junit8934247217114872089/mm2-offsets.backup.internal-10 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:41,684 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-10 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-10
2023-07-30 13:49:41,684 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-10 broker=0] Log loaded for partition mm2-offsets.backup.internal-10 with initial high watermark 0
2023-07-30 13:49:41,692 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-offsets.backup.internal-21, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:41,692 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-offsets.backup.internal-21 in /tmp/junit6293896813403981889/junit8934247217114872089/mm2-offsets.backup.internal-21 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:41,692 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-21 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-21
2023-07-30 13:49:41,693 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-21 broker=0] Log loaded for partition mm2-offsets.backup.internal-21 with initial high watermark 0
2023-07-30 13:49:41,700 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-offsets.backup.internal-6, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:41,701 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-offsets.backup.internal-6 in /tmp/junit6293896813403981889/junit8934247217114872089/mm2-offsets.backup.internal-6 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:41,701 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-6 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-6
2023-07-30 13:49:41,701 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-6 broker=0] Log loaded for partition mm2-offsets.backup.internal-6 with initial high watermark 0
2023-07-30 13:49:41,708 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-offsets.backup.internal-17, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:41,709 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-offsets.backup.internal-17 in /tmp/junit6293896813403981889/junit8934247217114872089/mm2-offsets.backup.internal-17 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:41,709 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-17 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-17
2023-07-30 13:49:41,709 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-17 broker=0] Log loaded for partition mm2-offsets.backup.internal-17 with initial high watermark 0
2023-07-30 13:49:41,717 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-offsets.backup.internal-2, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:41,717 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-offsets.backup.internal-2 in /tmp/junit6293896813403981889/junit8934247217114872089/mm2-offsets.backup.internal-2 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:41,718 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-2 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-2
2023-07-30 13:49:41,718 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-2 broker=0] Log loaded for partition mm2-offsets.backup.internal-2 with initial high watermark 0
2023-07-30 13:49:41,725 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-offsets.backup.internal-15, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:41,726 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-offsets.backup.internal-15 in /tmp/junit6293896813403981889/junit8934247217114872089/mm2-offsets.backup.internal-15 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:41,726 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-15 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-15
2023-07-30 13:49:41,726 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-15 broker=0] Log loaded for partition mm2-offsets.backup.internal-15 with initial high watermark 0
2023-07-30 13:49:41,734 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-offsets.backup.internal-0, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:41,734 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-offsets.backup.internal-0 in /tmp/junit6293896813403981889/junit8934247217114872089/mm2-offsets.backup.internal-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:41,734 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-0 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-0
2023-07-30 13:49:41,734 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-0 broker=0] Log loaded for partition mm2-offsets.backup.internal-0 with initial high watermark 0
2023-07-30 13:49:41,742 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-offsets.backup.internal-11, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:41,751 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-offsets.backup.internal-11 in /tmp/junit6293896813403981889/junit8934247217114872089/mm2-offsets.backup.internal-11 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:41,752 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-11 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-11
2023-07-30 13:49:41,752 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-11 broker=0] Log loaded for partition mm2-offsets.backup.internal-11 with initial high watermark 0
2023-07-30 13:49:41,755 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-offsets.backup.internal-7, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:41,756 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-offsets.backup.internal-7 in /tmp/junit6293896813403981889/junit8934247217114872089/mm2-offsets.backup.internal-7 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:41,756 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-7 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-7
2023-07-30 13:49:41,756 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-7 broker=0] Log loaded for partition mm2-offsets.backup.internal-7 with initial high watermark 0
2023-07-30 13:49:41,763 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-offsets.backup.internal-22, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:41,764 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-offsets.backup.internal-22 in /tmp/junit6293896813403981889/junit8934247217114872089/mm2-offsets.backup.internal-22 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:41,764 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-22 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-22
2023-07-30 13:49:41,764 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-22 broker=0] Log loaded for partition mm2-offsets.backup.internal-22 with initial high watermark 0
2023-07-30 13:49:41,771 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-offsets.backup.internal-3, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:41,772 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-offsets.backup.internal-3 in /tmp/junit6293896813403981889/junit8934247217114872089/mm2-offsets.backup.internal-3 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:41,772 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-3 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-3
2023-07-30 13:49:41,772 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-3 broker=0] Log loaded for partition mm2-offsets.backup.internal-3 with initial high watermark 0
2023-07-30 13:49:41,780 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-offsets.backup.internal-18, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:41,780 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-offsets.backup.internal-18 in /tmp/junit6293896813403981889/junit8934247217114872089/mm2-offsets.backup.internal-18 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:41,781 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-18 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.backup.internal-18
2023-07-30 13:49:41,781 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-offsets.backup.internal-18 broker=0] Log loaded for partition mm2-offsets.backup.internal-18 with initial high watermark 0
2023-07-30 13:49:41,805 - INFO  [DistributedHerder-connect-1-1:TopicAdmin@284] - Created topic (name=mm2-offsets.backup.internal, numPartitions=25, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at localhost:35687
2023-07-30 13:49:41,806 - INFO  [kafka-admin-client-thread | adminclient-8:AppInfoParser@83] - App info kafka.admin.client for adminclient-8 unregistered
2023-07-30 13:49:41,806 - INFO  [kafka-admin-client-thread | adminclient-8:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:41,806 - INFO  [kafka-admin-client-thread | adminclient-8:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:41,806 - INFO  [kafka-admin-client-thread | adminclient-8:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:41,807 - INFO  [DistributedHerder-connect-1-1:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:35687]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-2
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:49:41,809 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:41,809 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:49:41,809 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:41,809 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:41,809 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:41,809 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:41,809 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:41,809 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:41,810 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:41,810 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:41,810 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:41,810 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:41,810 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:41,810 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:41,810 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:41,810 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:41,810 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739381810
2023-07-30 13:49:41,813 - INFO  [kafka-producer-network-thread | producer-2:Metadata@279] - [Producer clientId=producer-2] Cluster ID: XnnaS7BkQ22Tt1TiwnBd0Q
2023-07-30 13:49:41,818 - INFO  [DistributedHerder-connect-1-1:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:35687]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-backup-mm2-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = backup-mm2
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:49:41,839 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:41,840 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:49:41,840 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:41,840 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:41,840 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:41,840 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:41,840 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:41,840 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:41,840 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:41,840 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:41,840 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:41,840 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:41,840 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:41,840 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:41,840 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:41,840 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739381840
2023-07-30 13:49:41,845 - INFO  [DistributedHerder-connect-1-1:Metadata@279] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Cluster ID: XnnaS7BkQ22Tt1TiwnBd0Q
2023-07-30 13:49:41,861 - INFO  [DistributedHerder-connect-1-1:KafkaConsumer@1116] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Subscribed to partition(s): mm2-offsets.backup.internal-0, mm2-offsets.backup.internal-5, mm2-offsets.backup.internal-10, mm2-offsets.backup.internal-20, mm2-offsets.backup.internal-15, mm2-offsets.backup.internal-9, mm2-offsets.backup.internal-11, mm2-offsets.backup.internal-16, mm2-offsets.backup.internal-4, mm2-offsets.backup.internal-17, mm2-offsets.backup.internal-3, mm2-offsets.backup.internal-24, mm2-offsets.backup.internal-23, mm2-offsets.backup.internal-13, mm2-offsets.backup.internal-18, mm2-offsets.backup.internal-22, mm2-offsets.backup.internal-2, mm2-offsets.backup.internal-8, mm2-offsets.backup.internal-12, mm2-offsets.backup.internal-19, mm2-offsets.backup.internal-14, mm2-offsets.backup.internal-1, mm2-offsets.backup.internal-6, mm2-offsets.backup.internal-7, mm2-offsets.backup.internal-21
2023-07-30 13:49:41,863 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-0
2023-07-30 13:49:41,863 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-5
2023-07-30 13:49:41,864 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-10
2023-07-30 13:49:41,864 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-20
2023-07-30 13:49:41,864 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-15
2023-07-30 13:49:41,864 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-9
2023-07-30 13:49:41,864 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-11
2023-07-30 13:49:41,864 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-16
2023-07-30 13:49:41,864 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-4
2023-07-30 13:49:41,864 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-17
2023-07-30 13:49:41,864 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-3
2023-07-30 13:49:41,864 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-24
2023-07-30 13:49:41,864 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-23
2023-07-30 13:49:41,864 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-13
2023-07-30 13:49:41,864 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-18
2023-07-30 13:49:41,864 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-22
2023-07-30 13:49:41,864 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-2
2023-07-30 13:49:41,864 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-8
2023-07-30 13:49:41,864 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-12
2023-07-30 13:49:41,864 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-19
2023-07-30 13:49:41,864 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-14
2023-07-30 13:49:41,864 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-1
2023-07-30 13:49:41,864 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-6
2023-07-30 13:49:41,864 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-7
2023-07-30 13:49:41,864 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-21
2023-07-30 13:49:41,886 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-18 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:41,887 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-20 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:41,887 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-22 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:41,887 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-24 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:41,887 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:41,887 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:41,887 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:41,887 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-6 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:41,888 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-8 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:41,888 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-10 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:41,888 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-12 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:41,888 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-14 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:41,888 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-16 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:41,888 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-19 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:41,888 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-21 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:41,888 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-23 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:41,888 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:41,888 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:41,889 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:41,889 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-7 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:41,889 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-9 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:41,889 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-11 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:41,889 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-13 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:41,889 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-15 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:41,889 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-1, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-17 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:41,889 - INFO  [DistributedHerder-connect-1-1:KafkaBasedLog@161] - Finished reading KafkaBasedLog for topic mm2-offsets.backup.internal
2023-07-30 13:49:41,890 - INFO  [DistributedHerder-connect-1-1:KafkaBasedLog@163] - Started KafkaBasedLog for topic mm2-offsets.backup.internal
2023-07-30 13:49:41,890 - INFO  [DistributedHerder-connect-1-1:KafkaOffsetBackingStore@129] - Finished reading offsets topic and starting KafkaOffsetBackingStore
2023-07-30 13:49:41,892 - INFO  [DistributedHerder-connect-1-1:Worker@202] - Worker started
2023-07-30 13:49:41,892 - INFO  [DistributedHerder-connect-1-1:KafkaBasedLog@127] - Starting KafkaBasedLog with topic mm2-status.backup.internal
2023-07-30 13:49:41,892 - INFO  [DistributedHerder-connect-1-1:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:35687]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:41,893 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:41,893 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:49:41,893 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:41,893 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:41,893 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:41,893 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:41,893 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:41,893 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:41,893 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:41,894 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:41,894 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:41,894 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:41,894 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:41,894 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:41,894 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:41,894 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:41,894 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739381894
2023-07-30 13:49:41,908 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Creating topic mm2-status.backup.internal with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0))
2023-07-30 13:49:41,924 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(mm2-status.backup.internal-0, mm2-status.backup.internal-3, mm2-status.backup.internal-4, mm2-status.backup.internal-1, mm2-status.backup.internal-2)
2023-07-30 13:49:41,927 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-status.backup.internal-4, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:41,927 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-status.backup.internal-4 in /tmp/junit6293896813403981889/junit8934247217114872089/mm2-status.backup.internal-4 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:41,928 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-status.backup.internal-4 broker=0] No checkpointed highwatermark is found for partition mm2-status.backup.internal-4
2023-07-30 13:49:41,928 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-status.backup.internal-4 broker=0] Log loaded for partition mm2-status.backup.internal-4 with initial high watermark 0
2023-07-30 13:49:41,931 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-status.backup.internal-3, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:41,932 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-status.backup.internal-3 in /tmp/junit6293896813403981889/junit8934247217114872089/mm2-status.backup.internal-3 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:41,932 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-status.backup.internal-3 broker=0] No checkpointed highwatermark is found for partition mm2-status.backup.internal-3
2023-07-30 13:49:41,932 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-status.backup.internal-3 broker=0] Log loaded for partition mm2-status.backup.internal-3 with initial high watermark 0
2023-07-30 13:49:41,940 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-status.backup.internal-2, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:41,941 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-status.backup.internal-2 in /tmp/junit6293896813403981889/junit8934247217114872089/mm2-status.backup.internal-2 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:41,941 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-status.backup.internal-2 broker=0] No checkpointed highwatermark is found for partition mm2-status.backup.internal-2
2023-07-30 13:49:41,941 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-status.backup.internal-2 broker=0] Log loaded for partition mm2-status.backup.internal-2 with initial high watermark 0
2023-07-30 13:49:41,948 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-status.backup.internal-1, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:41,948 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-status.backup.internal-1 in /tmp/junit6293896813403981889/junit8934247217114872089/mm2-status.backup.internal-1 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:41,948 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-status.backup.internal-1 broker=0] No checkpointed highwatermark is found for partition mm2-status.backup.internal-1
2023-07-30 13:49:41,949 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-status.backup.internal-1 broker=0] Log loaded for partition mm2-status.backup.internal-1 with initial high watermark 0
2023-07-30 13:49:41,956 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-status.backup.internal-0, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:41,957 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-status.backup.internal-0 in /tmp/junit6293896813403981889/junit8934247217114872089/mm2-status.backup.internal-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:41,957 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-status.backup.internal-0 broker=0] No checkpointed highwatermark is found for partition mm2-status.backup.internal-0
2023-07-30 13:49:41,957 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-status.backup.internal-0 broker=0] Log loaded for partition mm2-status.backup.internal-0 with initial high watermark 0
2023-07-30 13:49:41,966 - INFO  [DistributedHerder-connect-1-1:TopicAdmin@284] - Created topic (name=mm2-status.backup.internal, numPartitions=5, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at localhost:35687
2023-07-30 13:49:41,967 - INFO  [kafka-admin-client-thread | adminclient-9:AppInfoParser@83] - App info kafka.admin.client for adminclient-9 unregistered
2023-07-30 13:49:41,968 - INFO  [kafka-admin-client-thread | adminclient-9:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:41,968 - INFO  [kafka-admin-client-thread | adminclient-9:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:41,968 - INFO  [kafka-admin-client-thread | adminclient-9:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:41,968 - INFO  [DistributedHerder-connect-1-1:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:35687]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-3
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:49:41,970 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:41,970 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:49:41,970 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:41,971 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:41,971 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:41,971 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:41,971 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:41,971 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:41,971 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:41,971 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:41,971 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:41,971 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:41,971 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:41,971 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:41,971 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:41,972 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:41,972 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739381971
2023-07-30 13:49:41,973 - INFO  [DistributedHerder-connect-1-1:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:35687]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-backup-mm2-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = backup-mm2
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:49:41,974 - INFO  [kafka-producer-network-thread | producer-3:Metadata@279] - [Producer clientId=producer-3] Cluster ID: XnnaS7BkQ22Tt1TiwnBd0Q
2023-07-30 13:49:41,975 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:41,975 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:49:41,975 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:41,975 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:41,975 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:41,975 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:41,975 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:41,976 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:41,976 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:41,976 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:41,976 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:41,976 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:41,976 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:41,976 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:41,976 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:41,976 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739381976
2023-07-30 13:49:41,979 - INFO  [DistributedHerder-connect-1-1:Metadata@279] - [Consumer clientId=consumer-backup-mm2-2, groupId=backup-mm2] Cluster ID: XnnaS7BkQ22Tt1TiwnBd0Q
2023-07-30 13:49:41,984 - INFO  [DistributedHerder-connect-1-1:KafkaConsumer@1116] - [Consumer clientId=consumer-backup-mm2-2, groupId=backup-mm2] Subscribed to partition(s): mm2-status.backup.internal-0, mm2-status.backup.internal-4, mm2-status.backup.internal-1, mm2-status.backup.internal-2, mm2-status.backup.internal-3
2023-07-30 13:49:41,984 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-2, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-status.backup.internal-0
2023-07-30 13:49:41,984 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-2, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-status.backup.internal-4
2023-07-30 13:49:41,984 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-2, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-status.backup.internal-1
2023-07-30 13:49:41,984 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-2, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-status.backup.internal-2
2023-07-30 13:49:41,984 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-2, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-status.backup.internal-3
2023-07-30 13:49:41,990 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-2, groupId=backup-mm2] Resetting offset for partition mm2-status.backup.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:41,990 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-2, groupId=backup-mm2] Resetting offset for partition mm2-status.backup.internal-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:41,990 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-2, groupId=backup-mm2] Resetting offset for partition mm2-status.backup.internal-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:41,990 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-2, groupId=backup-mm2] Resetting offset for partition mm2-status.backup.internal-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:41,990 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-2, groupId=backup-mm2] Resetting offset for partition mm2-status.backup.internal-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:41,991 - INFO  [DistributedHerder-connect-1-1:KafkaBasedLog@161] - Finished reading KafkaBasedLog for topic mm2-status.backup.internal
2023-07-30 13:49:41,991 - INFO  [DistributedHerder-connect-1-1:KafkaBasedLog@163] - Started KafkaBasedLog for topic mm2-status.backup.internal
2023-07-30 13:49:41,994 - INFO  [DistributedHerder-connect-1-1:KafkaConfigBackingStore@265] - Starting KafkaConfigBackingStore
2023-07-30 13:49:41,994 - INFO  [DistributedHerder-connect-1-1:KafkaBasedLog@127] - Starting KafkaBasedLog with topic mm2-configs.backup.internal
2023-07-30 13:49:41,995 - INFO  [DistributedHerder-connect-1-1:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:35687]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:41,996 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:41,996 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:49:41,996 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:41,996 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:41,996 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:41,996 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:41,996 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:41,996 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:41,996 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:41,996 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:41,996 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:41,996 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:41,996 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:41,996 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:41,997 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:41,997 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:41,997 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739381997
2023-07-30 13:49:42,008 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Creating topic mm2-configs.backup.internal with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0))
2023-07-30 13:49:42,018 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(mm2-configs.backup.internal-0)
2023-07-30 13:49:42,020 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Log partition=mm2-configs.backup.internal-0, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:42,021 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - Created log for partition mm2-configs.backup.internal-0 in /tmp/junit6293896813403981889/junit8934247217114872089/mm2-configs.backup.internal-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:42,022 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-configs.backup.internal-0 broker=0] No checkpointed highwatermark is found for partition mm2-configs.backup.internal-0
2023-07-30 13:49:42,022 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [Partition mm2-configs.backup.internal-0 broker=0] Log loaded for partition mm2-configs.backup.internal-0 with initial high watermark 0
2023-07-30 13:49:42,026 - INFO  [DistributedHerder-connect-1-1:TopicAdmin@284] - Created topic (name=mm2-configs.backup.internal, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at localhost:35687
2023-07-30 13:49:42,026 - INFO  [kafka-admin-client-thread | adminclient-10:AppInfoParser@83] - App info kafka.admin.client for adminclient-10 unregistered
2023-07-30 13:49:42,027 - INFO  [kafka-admin-client-thread | adminclient-10:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:42,027 - INFO  [kafka-admin-client-thread | adminclient-10:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:42,027 - INFO  [kafka-admin-client-thread | adminclient-10:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:42,028 - INFO  [DistributedHerder-connect-1-1:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:35687]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-4
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:49:42,029 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,029 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:49:42,030 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:42,030 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,030 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:42,030 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,030 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,030 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:42,030 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:42,030 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,030 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,030 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,030 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,030 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,030 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:42,030 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:42,030 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739382030
2023-07-30 13:49:42,031 - INFO  [DistributedHerder-connect-1-1:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:35687]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-backup-mm2-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = backup-mm2
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:49:42,033 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,033 - INFO  [kafka-producer-network-thread | producer-4:Metadata@279] - [Producer clientId=producer-4] Cluster ID: XnnaS7BkQ22Tt1TiwnBd0Q
2023-07-30 13:49:42,033 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:49:42,034 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,034 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:42,034 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,034 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,034 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:42,034 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:42,034 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,034 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,034 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,034 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,034 - WARN  [DistributedHerder-connect-1-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,034 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:42,034 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:42,034 - INFO  [DistributedHerder-connect-1-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739382034
2023-07-30 13:49:42,037 - INFO  [DistributedHerder-connect-1-1:Metadata@279] - [Consumer clientId=consumer-backup-mm2-3, groupId=backup-mm2] Cluster ID: XnnaS7BkQ22Tt1TiwnBd0Q
2023-07-30 13:49:42,040 - INFO  [DistributedHerder-connect-1-1:KafkaConsumer@1116] - [Consumer clientId=consumer-backup-mm2-3, groupId=backup-mm2] Subscribed to partition(s): mm2-configs.backup.internal-0
2023-07-30 13:49:42,041 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-3, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-configs.backup.internal-0
2023-07-30 13:49:42,045 - INFO  [DistributedHerder-connect-1-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-3, groupId=backup-mm2] Resetting offset for partition mm2-configs.backup.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,045 - INFO  [DistributedHerder-connect-1-1:KafkaBasedLog@161] - Finished reading KafkaBasedLog for topic mm2-configs.backup.internal
2023-07-30 13:49:42,045 - INFO  [DistributedHerder-connect-1-1:KafkaBasedLog@163] - Started KafkaBasedLog for topic mm2-configs.backup.internal
2023-07-30 13:49:42,045 - INFO  [DistributedHerder-connect-1-1:KafkaConfigBackingStore@280] - Started KafkaConfigBackingStore
2023-07-30 13:49:42,046 - INFO  [DistributedHerder-connect-1-1:DistributedHerder@290] - [Worker clientId=connect-1, groupId=backup-mm2] Herder started
Jul 30, 2023 1:49:42 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.RootResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.RootResource will be ignored. 
Jul 30, 2023 1:49:42 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource will be ignored. 
Jul 30, 2023 1:49:42 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource will be ignored. 
Jul 30, 2023 1:49:42 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource will be ignored. 
2023-07-30 13:49:42,052 - INFO  [DistributedHerder-connect-1-1:Metadata@279] - [Worker clientId=connect-1, groupId=backup-mm2] Cluster ID: XnnaS7BkQ22Tt1TiwnBd0Q
2023-07-30 13:49:42,059 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Creating topic __consumer_offsets with configuration {segment.bytes=104857600, compression.type=producer, cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0), 5 -> ArrayBuffer(0), 6 -> ArrayBuffer(0), 7 -> ArrayBuffer(0), 8 -> ArrayBuffer(0), 9 -> ArrayBuffer(0), 10 -> ArrayBuffer(0), 11 -> ArrayBuffer(0), 12 -> ArrayBuffer(0), 13 -> ArrayBuffer(0), 14 -> ArrayBuffer(0), 15 -> ArrayBuffer(0), 16 -> ArrayBuffer(0), 17 -> ArrayBuffer(0), 18 -> ArrayBuffer(0), 19 -> ArrayBuffer(0), 20 -> ArrayBuffer(0), 21 -> ArrayBuffer(0), 22 -> ArrayBuffer(0), 23 -> ArrayBuffer(0), 24 -> ArrayBuffer(0), 25 -> ArrayBuffer(0), 26 -> ArrayBuffer(0), 27 -> ArrayBuffer(0), 28 -> ArrayBuffer(0), 29 -> ArrayBuffer(0), 30 -> ArrayBuffer(0), 31 -> ArrayBuffer(0), 32 -> ArrayBuffer(0), 33 -> ArrayBuffer(0), 34 -> ArrayBuffer(0), 35 -> ArrayBuffer(0), 36 -> ArrayBuffer(0), 37 -> ArrayBuffer(0), 38 -> ArrayBuffer(0), 39 -> ArrayBuffer(0), 40 -> ArrayBuffer(0), 41 -> ArrayBuffer(0), 42 -> ArrayBuffer(0), 43 -> ArrayBuffer(0), 44 -> ArrayBuffer(0), 45 -> ArrayBuffer(0), 46 -> ArrayBuffer(0), 47 -> ArrayBuffer(0), 48 -> ArrayBuffer(0), 49 -> ArrayBuffer(0))
2023-07-30 13:49:42,063 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [KafkaApi-0] Auto creation of topic __consumer_offsets with 50 partitions and replication factor 1 is successful
2023-07-30 13:49:42,121 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-37, __consumer_offsets-38, __consumer_offsets-13, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40)
2023-07-30 13:49:42,125 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-3, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:42,126 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-3 in /tmp/junit6293896813403981889/junit8934247217114872089/__consumer_offsets-3 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:42,126 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3
2023-07-30 13:49:42,126 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-3 broker=0] Log loaded for partition __consumer_offsets-3 with initial high watermark 0
2023-07-30 13:49:42,130 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-18, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:42,131 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-18 in /tmp/junit6293896813403981889/junit8934247217114872089/__consumer_offsets-18 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:42,131 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-18 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-18
2023-07-30 13:49:42,131 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-18 broker=0] Log loaded for partition __consumer_offsets-18 with initial high watermark 0
2023-07-30 13:49:42,272 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-41, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:42,277 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-41 in /tmp/junit6293896813403981889/junit8934247217114872089/__consumer_offsets-41 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:42,277 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-41 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-41
2023-07-30 13:49:42,277 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-41 broker=0] Log loaded for partition __consumer_offsets-41 with initial high watermark 0
2023-07-30 13:49:42,281 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-10, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:42,281 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-10 in /tmp/junit6293896813403981889/junit8934247217114872089/__consumer_offsets-10 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:42,282 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-10 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-10
2023-07-30 13:49:42,282 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-10 broker=0] Log loaded for partition __consumer_offsets-10 with initial high watermark 0
2023-07-30 13:49:42,288 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-33, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:42,289 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-33 in /tmp/junit6293896813403981889/junit8934247217114872089/__consumer_offsets-33 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:42,289 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-33 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-33
2023-07-30 13:49:42,289 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-33 broker=0] Log loaded for partition __consumer_offsets-33 with initial high watermark 0
2023-07-30 13:49:42,296 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-48, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:42,297 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-48 in /tmp/junit6293896813403981889/junit8934247217114872089/__consumer_offsets-48 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:42,297 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-48 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-48
2023-07-30 13:49:42,297 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-48 broker=0] Log loaded for partition __consumer_offsets-48 with initial high watermark 0
2023-07-30 13:49:42,305 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-19, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:42,306 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-19 in /tmp/junit6293896813403981889/junit8934247217114872089/__consumer_offsets-19 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:42,306 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-19 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-19
2023-07-30 13:49:42,306 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-19 broker=0] Log loaded for partition __consumer_offsets-19 with initial high watermark 0
2023-07-30 13:49:42,313 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-34, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:42,314 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-34 in /tmp/junit6293896813403981889/junit8934247217114872089/__consumer_offsets-34 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:42,314 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-34 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-34
2023-07-30 13:49:42,314 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-34 broker=0] Log loaded for partition __consumer_offsets-34 with initial high watermark 0
2023-07-30 13:49:42,322 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-4, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:42,323 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-4 in /tmp/junit6293896813403981889/junit8934247217114872089/__consumer_offsets-4 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:42,323 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4
2023-07-30 13:49:42,323 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-4 broker=0] Log loaded for partition __consumer_offsets-4 with initial high watermark 0
2023-07-30 13:49:42,330 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-11, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:42,330 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-11 in /tmp/junit6293896813403981889/junit8934247217114872089/__consumer_offsets-11 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:42,331 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-11 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-11
2023-07-30 13:49:42,331 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-11 broker=0] Log loaded for partition __consumer_offsets-11 with initial high watermark 0
2023-07-30 13:49:42,338 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-26, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:42,339 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-26 in /tmp/junit6293896813403981889/junit8934247217114872089/__consumer_offsets-26 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:42,339 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-26 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-26
2023-07-30 13:49:42,339 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-26 broker=0] Log loaded for partition __consumer_offsets-26 with initial high watermark 0
2023-07-30 13:49:42,346 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-49, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:42,347 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-49 in /tmp/junit6293896813403981889/junit8934247217114872089/__consumer_offsets-49 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:42,347 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-49 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-49
2023-07-30 13:49:42,347 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-49 broker=0] Log loaded for partition __consumer_offsets-49 with initial high watermark 0
2023-07-30 13:49:42,354 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-39, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:42,355 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-39 in /tmp/junit6293896813403981889/junit8934247217114872089/__consumer_offsets-39 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:42,355 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-39 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-39
2023-07-30 13:49:42,355 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-39 broker=0] Log loaded for partition __consumer_offsets-39 with initial high watermark 0
2023-07-30 13:49:42,363 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-9, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:42,363 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-9 in /tmp/junit6293896813403981889/junit8934247217114872089/__consumer_offsets-9 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:42,364 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-9 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-9
2023-07-30 13:49:42,364 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-9 broker=0] Log loaded for partition __consumer_offsets-9 with initial high watermark 0
Jul 30, 2023 1:49:42 PM org.glassfish.jersey.internal.Errors logErrors
WARNING: The following warnings have been detected: WARNING: The (sub)resource method listLoggers in org.apache.kafka.connect.runtime.rest.resources.LoggingResource contains empty path annotation.
WARNING: The (sub)resource method createConnector in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectors in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectorPlugins in org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource contains empty path annotation.
WARNING: The (sub)resource method serverInfo in org.apache.kafka.connect.runtime.rest.resources.RootResource contains empty path annotation.

2023-07-30 13:49:42,368 - INFO  [main:ContextHandler@849] - Started o.e.j.s.ServletContextHandler@2424686b{/,null,AVAILABLE}
2023-07-30 13:49:42,369 - INFO  [main:RestServer@319] - REST resources initialized; server is started and ready to handle requests
2023-07-30 13:49:42,369 - INFO  [main:Connect@57] - Kafka Connect started
2023-07-30 13:49:42,369 - INFO  [main:EmbeddedConnectCluster@169] - Started worker WorkerHandle{workerName='connect-worker-0'workerURL='http://localhost:43083/'}
2023-07-30 13:49:42,370 - INFO  [main:ConnectDistributed@90] - Scanning for plugin classes. This might take a moment ...
2023-07-30 13:49:42,371 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-24, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:42,372 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-24 in /tmp/junit6293896813403981889/junit8934247217114872089/__consumer_offsets-24 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:42,372 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-24 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-24
2023-07-30 13:49:42,372 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-24 broker=0] Log loaded for partition __consumer_offsets-24 with initial high watermark 0
2023-07-30 13:49:42,379 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-31, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:42,380 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-31 in /tmp/junit6293896813403981889/junit8934247217114872089/__consumer_offsets-31 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:42,380 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-31 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-31
2023-07-30 13:49:42,380 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-31 broker=0] Log loaded for partition __consumer_offsets-31 with initial high watermark 0
2023-07-30 13:49:42,388 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-46, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:42,388 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-46 in /tmp/junit6293896813403981889/junit8934247217114872089/__consumer_offsets-46 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:42,389 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-46 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-46
2023-07-30 13:49:42,389 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-46 broker=0] Log loaded for partition __consumer_offsets-46 with initial high watermark 0
2023-07-30 13:49:42,396 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-1, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:42,397 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-1 in /tmp/junit6293896813403981889/junit8934247217114872089/__consumer_offsets-1 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:42,397 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1
2023-07-30 13:49:42,397 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-1 broker=0] Log loaded for partition __consumer_offsets-1 with initial high watermark 0
2023-07-30 13:49:42,404 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-16, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:42,405 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-16 in /tmp/junit6293896813403981889/junit8934247217114872089/__consumer_offsets-16 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:42,405 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-16 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-16
2023-07-30 13:49:42,405 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-16 broker=0] Log loaded for partition __consumer_offsets-16 with initial high watermark 0
2023-07-30 13:49:42,413 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-2, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:42,413 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-2 in /tmp/junit6293896813403981889/junit8934247217114872089/__consumer_offsets-2 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:42,413 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2
2023-07-30 13:49:42,414 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-2 broker=0] Log loaded for partition __consumer_offsets-2 with initial high watermark 0
2023-07-30 13:49:42,421 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-25, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:42,422 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-25 in /tmp/junit6293896813403981889/junit8934247217114872089/__consumer_offsets-25 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:42,422 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-25 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-25
2023-07-30 13:49:42,422 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-25 broker=0] Log loaded for partition __consumer_offsets-25 with initial high watermark 0
2023-07-30 13:49:42,430 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-40, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:42,430 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-40 in /tmp/junit6293896813403981889/junit8934247217114872089/__consumer_offsets-40 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:42,430 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-40 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-40
2023-07-30 13:49:42,431 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-40 broker=0] Log loaded for partition __consumer_offsets-40 with initial high watermark 0
2023-07-30 13:49:42,438 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-47, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:42,439 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-47 in /tmp/junit6293896813403981889/junit8934247217114872089/__consumer_offsets-47 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:42,439 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-47 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-47
2023-07-30 13:49:42,439 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-47 broker=0] Log loaded for partition __consumer_offsets-47 with initial high watermark 0
2023-07-30 13:49:42,446 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-17, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:42,447 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-17 in /tmp/junit6293896813403981889/junit8934247217114872089/__consumer_offsets-17 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:42,447 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-17 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-17
2023-07-30 13:49:42,447 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-17 broker=0] Log loaded for partition __consumer_offsets-17 with initial high watermark 0
2023-07-30 13:49:42,453 - INFO  [main:Reflections@239] - Reflections took 81 ms to scan 1 urls, producing 349 keys and 1055 values [using 20 cores]
2023-07-30 13:49:42,455 - WARN  [main:ReflectionUtils@318] - could not get type for name kotlin.jvm.internal.Lambda from any class loader
org.reflections.ReflectionsException: could not get type for name kotlin.jvm.internal.Lambda
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:91)
	at org.apache.kafka.connect.util.clusters.WorkerHandle.start(WorkerHandle.java:50)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.addWorker(EmbeddedConnectCluster.java:167)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.startConnect(EmbeddedConnectCluster.java:253)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.start(EmbeddedConnectCluster.java:137)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:130)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: kotlin.jvm.internal.Lambda
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 68 more
2023-07-30 13:49:42,455 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-32, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:42,457 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-32 in /tmp/junit6293896813403981889/junit8934247217114872089/__consumer_offsets-32 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:42,457 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-32 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-32
2023-07-30 13:49:42,457 - WARN  [main:ReflectionUtils@318] - could not get type for name kotlin.jvm.functions.Function0 from any class loader
org.reflections.ReflectionsException: could not get type for name kotlin.jvm.functions.Function0
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:91)
	at org.apache.kafka.connect.util.clusters.WorkerHandle.start(WorkerHandle.java:50)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.addWorker(EmbeddedConnectCluster.java:167)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.startConnect(EmbeddedConnectCluster.java:253)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.start(EmbeddedConnectCluster.java:137)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:130)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: kotlin.jvm.functions.Function0
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 68 more
2023-07-30 13:49:42,457 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-32 broker=0] Log loaded for partition __consumer_offsets-32 with initial high watermark 0
2023-07-30 13:49:42,459 - WARN  [main:ReflectionUtils@318] - could not get type for name java.util.spi.ToolProvider from any class loader
org.reflections.ReflectionsException: could not get type for name java.util.spi.ToolProvider
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:91)
	at org.apache.kafka.connect.util.clusters.WorkerHandle.start(WorkerHandle.java:50)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.addWorker(EmbeddedConnectCluster.java:167)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.startConnect(EmbeddedConnectCluster.java:253)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.start(EmbeddedConnectCluster.java:137)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:130)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: java.util.spi.ToolProvider
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 68 more
2023-07-30 13:49:42,462 - INFO  [main:DelegatingClassLoader@269] - Registered loader: java.net.FactoryURLClassLoader@483bf400
2023-07-30 13:49:42,462 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension'
2023-07-30 13:49:42,462 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.runtime.isolation.PluginsTest$TestConnectRestExtension'
2023-07-30 13:49:42,462 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy'
2023-07-30 13:49:42,463 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy'
2023-07-30 13:49:42,463 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy'
2023-07-30 13:49:42,463 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-37, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:42,463 - INFO  [main:DelegatingClassLoader@427] - Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension'
2023-07-30 13:49:42,463 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'TestConnectRestExtension' and 'Test' to plugin 'org.apache.kafka.connect.runtime.isolation.PluginsTest$TestConnectRestExtension'
2023-07-30 13:49:42,463 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy'
2023-07-30 13:49:42,464 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy'
2023-07-30 13:49:42,464 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy'
2023-07-30 13:49:42,464 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-37 in /tmp/junit6293896813403981889/junit8934247217114872089/__consumer_offsets-37 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:42,464 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-37 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-37
2023-07-30 13:49:42,464 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-37 broker=0] Log loaded for partition __consumer_offsets-37 with initial high watermark 0
2023-07-30 13:49:42,464 - INFO  [main:AbstractConfig@361] - DistributedConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:35687]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	config.providers = []
	config.storage.replication.factor = 1
	config.storage.topic = mm2-configs.backup.internal
	connect.protocol = sessioned
	connections.max.idle.ms = 540000
	connector.client.config.override.policy = None
	group.id = backup-mm2
	header.converter = class org.apache.kafka.connect.converters.ByteArrayConverter
	heartbeat.interval.ms = 3000
	inter.worker.key.generation.algorithm = HmacSHA256
	inter.worker.key.size = null
	inter.worker.key.ttl.ms = 3600000
	inter.worker.signature.algorithm = HmacSHA256
	inter.worker.verification.algorithms = [HmacSHA256]
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.converters.ByteArrayConverter
	listeners = null
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 60000
	offset.flush.timeout.ms = 5000
	offset.storage.partitions = 25
	offset.storage.replication.factor = 1
	offset.storage.topic = mm2-offsets.backup.internal
	plugin.path = null
	rebalance.timeout.ms = 60000
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 40000
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = localhost
	rest.port = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	scheduled.rebalance.max.delay.ms = 300000
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	status.storage.partitions = 5
	status.storage.replication.factor = 1
	status.storage.topic = mm2-status.backup.internal
	task.shutdown.graceful.timeout.ms = 5000
	topic.creation.enable = true
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.converters.ByteArrayConverter
	worker.sync.timeout.ms = 3000
	worker.unsync.backoff.ms = 300000

2023-07-30 13:49:42,465 - WARN  [main:WorkerConfig@420] - Variables cannot be used in the 'plugin.path' property, since the property is used by plugin scanning before the config providers that replace the variables are initialized. The raw value 'null' was used for plugin scanning, as opposed to the transformed value 'null', and this may cause unexpected results.
2023-07-30 13:49:42,465 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:49:42,465 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:35687]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:42,467 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,467 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:42,467 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,467 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:42,467 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,468 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,468 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:42,468 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,468 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,468 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,468 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,468 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,468 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:42,468 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:42,468 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739382468
2023-07-30 13:49:42,472 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-7, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:42,474 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-7 in /tmp/junit6293896813403981889/junit8934247217114872089/__consumer_offsets-7 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:42,474 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-7 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-7
2023-07-30 13:49:42,474 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-7 broker=0] Log loaded for partition __consumer_offsets-7 with initial high watermark 0
2023-07-30 13:49:42,476 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: XnnaS7BkQ22Tt1TiwnBd0Q
2023-07-30 13:49:42,477 - INFO  [kafka-admin-client-thread | adminclient-11:AppInfoParser@83] - App info kafka.admin.client for adminclient-11 unregistered
2023-07-30 13:49:42,478 - INFO  [kafka-admin-client-thread | adminclient-11:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:42,478 - INFO  [kafka-admin-client-thread | adminclient-11:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:42,478 - INFO  [kafka-admin-client-thread | adminclient-11:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:42,478 - INFO  [main:RestServer@132] - Added connector for http://localhost:0
2023-07-30 13:49:42,479 - INFO  [main:RestServer@204] - Initializing REST server
2023-07-30 13:49:42,479 - INFO  [main:Server@360] - jetty-9.4.30.v20200611; built: 2020-06-11T12:34:51.929Z; git: 271836e4c1f4612f12b7bb13ef5a92a927634b0d; jvm 1.8.0_275-8u275-b01-0ubuntu1~18.04-b01
2023-07-30 13:49:42,480 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-22, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:42,480 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-22 in /tmp/junit6293896813403981889/junit8934247217114872089/__consumer_offsets-22 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:42,481 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-22 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-22
2023-07-30 13:49:42,481 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-22 broker=0] Log loaded for partition __consumer_offsets-22 with initial high watermark 0
2023-07-30 13:49:42,500 - INFO  [main:AbstractConnector@331] - Started http_localhost0@181e72d3{HTTP/1.1, (http/1.1)}{localhost:33591}
2023-07-30 13:49:42,501 - INFO  [main:Server@400] - Started @4759ms
2023-07-30 13:49:42,501 - INFO  [main:RestServer@371] - Advertised URI: http://localhost:33591/
2023-07-30 13:49:42,501 - INFO  [main:RestServer@219] - REST server listening at http://localhost:33591/, advertising URL http://localhost:33591/
2023-07-30 13:49:42,501 - INFO  [main:RestServer@371] - Advertised URI: http://localhost:33591/
2023-07-30 13:49:42,501 - INFO  [main:RestServer@220] - REST admin endpoints at http://localhost:33591/
2023-07-30 13:49:42,502 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-29, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:42,502 - INFO  [main:RestServer@371] - Advertised URI: http://localhost:33591/
2023-07-30 13:49:42,502 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:49:42,503 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-29 in /tmp/junit6293896813403981889/junit8934247217114872089/__consumer_offsets-29 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:42,503 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:35687]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:42,503 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-29 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-29
2023-07-30 13:49:42,504 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-29 broker=0] Log loaded for partition __consumer_offsets-29 with initial high watermark 0
2023-07-30 13:49:42,505 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,505 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:42,506 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,506 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:42,506 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,506 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,506 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:42,506 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,506 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,507 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,507 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,507 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,507 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:42,507 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:42,507 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739382507
2023-07-30 13:49:42,509 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-44, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:42,510 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-44 in /tmp/junit6293896813403981889/junit8934247217114872089/__consumer_offsets-44 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:42,510 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-44 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-44
2023-07-30 13:49:42,510 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-44 broker=0] Log loaded for partition __consumer_offsets-44 with initial high watermark 0
2023-07-30 13:49:42,512 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: XnnaS7BkQ22Tt1TiwnBd0Q
2023-07-30 13:49:42,513 - INFO  [kafka-admin-client-thread | adminclient-12:AppInfoParser@83] - App info kafka.admin.client for adminclient-12 unregistered
2023-07-30 13:49:42,514 - INFO  [kafka-admin-client-thread | adminclient-12:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:42,514 - INFO  [kafka-admin-client-thread | adminclient-12:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:42,514 - INFO  [kafka-admin-client-thread | adminclient-12:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:42,514 - INFO  [main:NoneConnectorClientConfigOverridePolicy@45] - Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden
2023-07-30 13:49:42,514 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:49:42,515 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:35687]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:42,515 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,516 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:42,516 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,516 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:42,516 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,516 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,516 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:42,516 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,516 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,516 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,516 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,516 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,517 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:42,517 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:42,517 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739382517
2023-07-30 13:49:42,518 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-14, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:42,519 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-14 in /tmp/junit6293896813403981889/junit8934247217114872089/__consumer_offsets-14 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:42,519 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-14 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-14
2023-07-30 13:49:42,519 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-14 broker=0] Log loaded for partition __consumer_offsets-14 with initial high watermark 0
2023-07-30 13:49:42,522 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: XnnaS7BkQ22Tt1TiwnBd0Q
2023-07-30 13:49:42,522 - INFO  [kafka-admin-client-thread | adminclient-13:AppInfoParser@83] - App info kafka.admin.client for adminclient-13 unregistered
2023-07-30 13:49:42,523 - INFO  [kafka-admin-client-thread | adminclient-13:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:42,523 - INFO  [kafka-admin-client-thread | adminclient-13:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:42,523 - INFO  [kafka-admin-client-thread | adminclient-13:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:42,524 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:42,524 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:42,524 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739382524
2023-07-30 13:49:42,524 - INFO  [main:AbstractConfig@361] - JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false

2023-07-30 13:49:42,525 - INFO  [main:AbstractConfig@361] - JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false

2023-07-30 13:49:42,525 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:49:42,525 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:35687]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:42,526 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-23, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:42,526 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,526 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:42,526 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,527 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:42,527 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,527 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,527 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:42,527 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,526 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-23 in /tmp/junit6293896813403981889/junit8934247217114872089/__consumer_offsets-23 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:42,527 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,527 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,527 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,527 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,527 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-23 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-23
2023-07-30 13:49:42,527 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:42,528 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-23 broker=0] Log loaded for partition __consumer_offsets-23 with initial high watermark 0
2023-07-30 13:49:42,528 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:42,528 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739382527
2023-07-30 13:49:42,533 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: XnnaS7BkQ22Tt1TiwnBd0Q
2023-07-30 13:49:42,533 - INFO  [kafka-admin-client-thread | adminclient-14:AppInfoParser@83] - App info kafka.admin.client for adminclient-14 unregistered
2023-07-30 13:49:42,534 - INFO  [kafka-admin-client-thread | adminclient-14:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:42,534 - INFO  [kafka-admin-client-thread | adminclient-14:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:42,534 - INFO  [kafka-admin-client-thread | adminclient-14:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:42,534 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:49:42,535 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:35687]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:42,535 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-38, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:42,536 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-38 in /tmp/junit6293896813403981889/junit8934247217114872089/__consumer_offsets-38 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:42,536 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-38 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-38
2023-07-30 13:49:42,536 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-38 broker=0] Log loaded for partition __consumer_offsets-38 with initial high watermark 0
2023-07-30 13:49:42,536 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,536 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:42,537 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,537 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:42,537 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,537 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,537 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:42,537 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,537 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,537 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,537 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,537 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,537 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:42,538 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:42,538 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739382537
2023-07-30 13:49:42,543 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-8, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:42,543 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: XnnaS7BkQ22Tt1TiwnBd0Q
2023-07-30 13:49:42,544 - INFO  [kafka-admin-client-thread | adminclient-15:AppInfoParser@83] - App info kafka.admin.client for adminclient-15 unregistered
2023-07-30 13:49:42,544 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-8 in /tmp/junit6293896813403981889/junit8934247217114872089/__consumer_offsets-8 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:42,544 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-8 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-8
2023-07-30 13:49:42,544 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-8 broker=0] Log loaded for partition __consumer_offsets-8 with initial high watermark 0
2023-07-30 13:49:42,544 - INFO  [kafka-admin-client-thread | adminclient-15:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:42,545 - INFO  [kafka-admin-client-thread | adminclient-15:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:42,545 - INFO  [kafka-admin-client-thread | adminclient-15:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:42,545 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:49:42,545 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:35687]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:42,546 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,546 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:42,546 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,546 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:42,546 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,546 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,547 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:42,547 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,547 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,547 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,547 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,547 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,547 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:42,547 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:42,547 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739382547
2023-07-30 13:49:42,551 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-45, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:42,552 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-45 in /tmp/junit6293896813403981889/junit8934247217114872089/__consumer_offsets-45 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:42,552 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-45 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-45
2023-07-30 13:49:42,552 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-45 broker=0] Log loaded for partition __consumer_offsets-45 with initial high watermark 0
2023-07-30 13:49:42,553 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: XnnaS7BkQ22Tt1TiwnBd0Q
2023-07-30 13:49:42,553 - INFO  [kafka-admin-client-thread | adminclient-16:AppInfoParser@83] - App info kafka.admin.client for adminclient-16 unregistered
2023-07-30 13:49:42,554 - INFO  [kafka-admin-client-thread | adminclient-16:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:42,554 - INFO  [kafka-admin-client-thread | adminclient-16:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:42,554 - INFO  [kafka-admin-client-thread | adminclient-16:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:42,555 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:49:42,555 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:35687]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:42,556 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,556 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:42,556 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,556 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:42,556 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,556 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,556 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:42,557 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,557 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,557 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,557 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,557 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,557 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:42,557 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:42,557 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739382557
2023-07-30 13:49:42,559 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-15, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:42,560 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-15 in /tmp/junit6293896813403981889/junit8934247217114872089/__consumer_offsets-15 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:42,560 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-15 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-15
2023-07-30 13:49:42,560 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-15 broker=0] Log loaded for partition __consumer_offsets-15 with initial high watermark 0
2023-07-30 13:49:42,563 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: XnnaS7BkQ22Tt1TiwnBd0Q
2023-07-30 13:49:42,563 - INFO  [kafka-admin-client-thread | adminclient-17:AppInfoParser@83] - App info kafka.admin.client for adminclient-17 unregistered
2023-07-30 13:49:42,564 - INFO  [kafka-admin-client-thread | adminclient-17:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:42,564 - INFO  [kafka-admin-client-thread | adminclient-17:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:42,564 - INFO  [kafka-admin-client-thread | adminclient-17:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:42,565 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:42,565 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:42,565 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739382565
2023-07-30 13:49:42,565 - INFO  [main:ConnectDistributed@128] - Kafka Connect distributed worker initialization took 196ms
2023-07-30 13:49:42,566 - INFO  [main:Connect@51] - Kafka Connect starting
2023-07-30 13:49:42,566 - INFO  [main:RestServer@224] - Initializing REST resources
2023-07-30 13:49:42,566 - INFO  [DistributedHerder-connect-2-1:DistributedHerder@286] - [Worker clientId=connect-2, groupId=backup-mm2] Herder starting
2023-07-30 13:49:42,566 - INFO  [DistributedHerder-connect-2-1:Worker@195] - Worker starting
2023-07-30 13:49:42,566 - INFO  [DistributedHerder-connect-2-1:KafkaOffsetBackingStore@127] - Starting KafkaOffsetBackingStore
2023-07-30 13:49:42,566 - INFO  [DistributedHerder-connect-2-1:KafkaBasedLog@127] - Starting KafkaBasedLog with topic mm2-offsets.backup.internal
2023-07-30 13:49:42,566 - INFO  [main:RestServer@241] - Adding admin resources to main listener
2023-07-30 13:49:42,566 - INFO  [DistributedHerder-connect-2-1:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:35687]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:42,567 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-30, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:42,567 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,567 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:49:42,567 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:42,568 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,568 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:42,568 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,568 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,568 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:42,568 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:42,568 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,568 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,568 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,568 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,568 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,568 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-30 in /tmp/junit6293896813403981889/junit8934247217114872089/__consumer_offsets-30 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:42,568 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:42,568 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:42,568 - INFO  [main:DefaultSessionIdManager@334] - DefaultSessionIdManager workerName=node0
2023-07-30 13:49:42,568 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739382568
2023-07-30 13:49:42,568 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-30 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-30
2023-07-30 13:49:42,569 - INFO  [main:DefaultSessionIdManager@339] - No SessionScavenger set, using defaults
2023-07-30 13:49:42,569 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-30 broker=0] Log loaded for partition __consumer_offsets-30 with initial high watermark 0
2023-07-30 13:49:42,569 - INFO  [main:HouseKeeper@140] - node0 Scavenging every 600000ms
2023-07-30 13:49:42,576 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-0, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:42,577 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-0 in /tmp/junit6293896813403981889/junit8934247217114872089/__consumer_offsets-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:42,577 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0
2023-07-30 13:49:42,578 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-0 broker=0] Log loaded for partition __consumer_offsets-0 with initial high watermark 0
2023-07-30 13:49:42,584 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-35, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:42,585 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-35 in /tmp/junit6293896813403981889/junit8934247217114872089/__consumer_offsets-35 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:42,586 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-35 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-35
2023-07-30 13:49:42,586 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-35 broker=0] Log loaded for partition __consumer_offsets-35 with initial high watermark 0
2023-07-30 13:49:42,589 - INFO  [kafka-admin-client-thread | adminclient-18:AppInfoParser@83] - App info kafka.admin.client for adminclient-18 unregistered
2023-07-30 13:49:42,590 - INFO  [kafka-admin-client-thread | adminclient-18:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:42,590 - INFO  [kafka-admin-client-thread | adminclient-18:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:42,590 - INFO  [kafka-admin-client-thread | adminclient-18:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:42,591 - INFO  [DistributedHerder-connect-2-1:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:35687]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-5
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:49:42,592 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-5, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:42,593 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,593 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:49:42,593 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:42,593 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,593 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:42,593 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,593 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,593 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:42,593 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:42,594 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,594 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,594 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,594 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,594 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-5 in /tmp/junit6293896813403981889/junit8934247217114872089/__consumer_offsets-5 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:42,594 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,594 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-5 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-5
2023-07-30 13:49:42,594 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:42,594 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:42,594 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-5 broker=0] Log loaded for partition __consumer_offsets-5 with initial high watermark 0
2023-07-30 13:49:42,594 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739382594
2023-07-30 13:49:42,594 - INFO  [DistributedHerder-connect-2-1:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:35687]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-backup-mm2-4
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = backup-mm2
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:49:42,596 - INFO  [kafka-producer-network-thread | producer-5:Metadata@279] - [Producer clientId=producer-5] Cluster ID: XnnaS7BkQ22Tt1TiwnBd0Q
2023-07-30 13:49:42,597 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,597 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:49:42,597 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,598 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:42,598 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,598 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,598 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:42,598 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:42,598 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,598 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,598 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,598 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,598 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,598 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:42,598 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:42,599 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739382598
2023-07-30 13:49:42,601 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-20, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:42,601 - INFO  [DistributedHerder-connect-2-1:Metadata@279] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Cluster ID: XnnaS7BkQ22Tt1TiwnBd0Q
2023-07-30 13:49:42,601 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-20 in /tmp/junit6293896813403981889/junit8934247217114872089/__consumer_offsets-20 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:42,602 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-20 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-20
2023-07-30 13:49:42,602 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-20 broker=0] Log loaded for partition __consumer_offsets-20 with initial high watermark 0
2023-07-30 13:49:42,607 - INFO  [DistributedHerder-connect-2-1:KafkaConsumer@1116] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Subscribed to partition(s): mm2-offsets.backup.internal-0, mm2-offsets.backup.internal-5, mm2-offsets.backup.internal-10, mm2-offsets.backup.internal-20, mm2-offsets.backup.internal-15, mm2-offsets.backup.internal-9, mm2-offsets.backup.internal-11, mm2-offsets.backup.internal-16, mm2-offsets.backup.internal-4, mm2-offsets.backup.internal-17, mm2-offsets.backup.internal-3, mm2-offsets.backup.internal-24, mm2-offsets.backup.internal-23, mm2-offsets.backup.internal-13, mm2-offsets.backup.internal-18, mm2-offsets.backup.internal-22, mm2-offsets.backup.internal-2, mm2-offsets.backup.internal-8, mm2-offsets.backup.internal-12, mm2-offsets.backup.internal-19, mm2-offsets.backup.internal-14, mm2-offsets.backup.internal-1, mm2-offsets.backup.internal-6, mm2-offsets.backup.internal-7, mm2-offsets.backup.internal-21
2023-07-30 13:49:42,608 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-0
2023-07-30 13:49:42,608 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-5
2023-07-30 13:49:42,608 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-10
2023-07-30 13:49:42,608 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-20
2023-07-30 13:49:42,608 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-15
2023-07-30 13:49:42,608 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-9
2023-07-30 13:49:42,608 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-11
2023-07-30 13:49:42,608 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-16
2023-07-30 13:49:42,608 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-4
2023-07-30 13:49:42,608 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-17
2023-07-30 13:49:42,608 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-3
2023-07-30 13:49:42,608 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-24
2023-07-30 13:49:42,608 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-23
2023-07-30 13:49:42,608 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-13
2023-07-30 13:49:42,609 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-18
2023-07-30 13:49:42,609 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-22
2023-07-30 13:49:42,609 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-2
2023-07-30 13:49:42,609 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-8
2023-07-30 13:49:42,609 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-12
2023-07-30 13:49:42,609 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-19
2023-07-30 13:49:42,609 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-14
2023-07-30 13:49:42,609 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-1
2023-07-30 13:49:42,609 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-6
2023-07-30 13:49:42,609 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-7
2023-07-30 13:49:42,609 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-21
2023-07-30 13:49:42,610 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-27, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:42,611 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-27 in /tmp/junit6293896813403981889/junit8934247217114872089/__consumer_offsets-27 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:42,611 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-27 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-27
2023-07-30 13:49:42,611 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-27 broker=0] Log loaded for partition __consumer_offsets-27 with initial high watermark 0
Jul 30, 2023 1:49:42 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.RootResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.RootResource will be ignored. 
Jul 30, 2023 1:49:42 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource will be ignored. 
Jul 30, 2023 1:49:42 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource will be ignored. 
Jul 30, 2023 1:49:42 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource will be ignored. 
2023-07-30 13:49:42,617 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-18 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,617 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-20 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,617 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-22 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,617 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-24 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,617 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-42, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:42,618 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,618 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,618 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,618 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-6 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,618 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-8 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,618 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-10 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,618 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-12 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,618 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-42 in /tmp/junit6293896813403981889/junit8934247217114872089/__consumer_offsets-42 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:42,618 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-14 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,618 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-42 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-42
2023-07-30 13:49:42,619 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-16 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,619 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-42 broker=0] Log loaded for partition __consumer_offsets-42 with initial high watermark 0
2023-07-30 13:49:42,619 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-19 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,619 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-21 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,619 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-23 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,619 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,619 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,619 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,620 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-7 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,620 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-9 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,620 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-11 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,620 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-13 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,620 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-15 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,620 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-4, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-17 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,621 - INFO  [DistributedHerder-connect-2-1:KafkaBasedLog@161] - Finished reading KafkaBasedLog for topic mm2-offsets.backup.internal
2023-07-30 13:49:42,621 - INFO  [DistributedHerder-connect-2-1:KafkaBasedLog@163] - Started KafkaBasedLog for topic mm2-offsets.backup.internal
2023-07-30 13:49:42,621 - INFO  [DistributedHerder-connect-2-1:KafkaOffsetBackingStore@129] - Finished reading offsets topic and starting KafkaOffsetBackingStore
2023-07-30 13:49:42,622 - INFO  [DistributedHerder-connect-2-1:Worker@202] - Worker started
2023-07-30 13:49:42,622 - INFO  [DistributedHerder-connect-2-1:KafkaBasedLog@127] - Starting KafkaBasedLog with topic mm2-status.backup.internal
2023-07-30 13:49:42,623 - INFO  [DistributedHerder-connect-2-1:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:35687]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:42,625 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,625 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:49:42,625 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:42,625 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,625 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:42,625 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,625 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,625 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:42,625 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:42,626 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,626 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,626 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,626 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,626 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,626 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:42,626 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:42,626 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739382626
2023-07-30 13:49:42,627 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-12, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:42,629 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-12 in /tmp/junit6293896813403981889/junit8934247217114872089/__consumer_offsets-12 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:42,629 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-12 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-12
2023-07-30 13:49:42,629 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-12 broker=0] Log loaded for partition __consumer_offsets-12 with initial high watermark 0
2023-07-30 13:49:42,634 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-21, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:42,635 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-21 in /tmp/junit6293896813403981889/junit8934247217114872089/__consumer_offsets-21 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:42,635 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-21 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-21
2023-07-30 13:49:42,635 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-21 broker=0] Log loaded for partition __consumer_offsets-21 with initial high watermark 0
2023-07-30 13:49:42,636 - INFO  [kafka-admin-client-thread | adminclient-19:AppInfoParser@83] - App info kafka.admin.client for adminclient-19 unregistered
2023-07-30 13:49:42,637 - INFO  [kafka-admin-client-thread | adminclient-19:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:42,637 - INFO  [kafka-admin-client-thread | adminclient-19:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:42,637 - INFO  [kafka-admin-client-thread | adminclient-19:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:42,638 - INFO  [DistributedHerder-connect-2-1:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:35687]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-6
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:49:42,639 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,640 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:49:42,640 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:42,640 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,640 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:42,640 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,640 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,641 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:42,641 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:42,641 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,641 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,641 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,642 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,642 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,642 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:42,642 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:42,642 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739382642
2023-07-30 13:49:42,643 - INFO  [DistributedHerder-connect-2-1:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:35687]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-backup-mm2-5
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = backup-mm2
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:49:42,644 - INFO  [kafka-producer-network-thread | producer-6:Metadata@279] - [Producer clientId=producer-6] Cluster ID: XnnaS7BkQ22Tt1TiwnBd0Q
2023-07-30 13:49:42,644 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-36, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:42,644 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,644 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:49:42,644 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,644 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:42,645 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,645 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,645 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:42,645 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:42,645 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,645 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,645 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,645 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,645 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,645 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:42,645 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:42,645 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739382645
2023-07-30 13:49:42,645 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-36 in /tmp/junit6293896813403981889/junit8934247217114872089/__consumer_offsets-36 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:42,645 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-36 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-36
2023-07-30 13:49:42,645 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-36 broker=0] Log loaded for partition __consumer_offsets-36 with initial high watermark 0
2023-07-30 13:49:42,647 - INFO  [DistributedHerder-connect-2-1:Metadata@279] - [Consumer clientId=consumer-backup-mm2-5, groupId=backup-mm2] Cluster ID: XnnaS7BkQ22Tt1TiwnBd0Q
2023-07-30 13:49:42,650 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-6, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:42,651 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-6 in /tmp/junit6293896813403981889/junit8934247217114872089/__consumer_offsets-6 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:42,651 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-6 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-6
2023-07-30 13:49:42,651 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-6 broker=0] Log loaded for partition __consumer_offsets-6 with initial high watermark 0
2023-07-30 13:49:42,653 - INFO  [DistributedHerder-connect-2-1:KafkaConsumer@1116] - [Consumer clientId=consumer-backup-mm2-5, groupId=backup-mm2] Subscribed to partition(s): mm2-status.backup.internal-0, mm2-status.backup.internal-4, mm2-status.backup.internal-1, mm2-status.backup.internal-2, mm2-status.backup.internal-3
2023-07-30 13:49:42,653 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-5, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-status.backup.internal-0
2023-07-30 13:49:42,653 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-5, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-status.backup.internal-4
2023-07-30 13:49:42,653 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-5, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-status.backup.internal-1
2023-07-30 13:49:42,653 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-5, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-status.backup.internal-2
2023-07-30 13:49:42,653 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-5, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-status.backup.internal-3
2023-07-30 13:49:42,658 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-5, groupId=backup-mm2] Resetting offset for partition mm2-status.backup.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,659 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-5, groupId=backup-mm2] Resetting offset for partition mm2-status.backup.internal-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,659 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-5, groupId=backup-mm2] Resetting offset for partition mm2-status.backup.internal-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,659 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-5, groupId=backup-mm2] Resetting offset for partition mm2-status.backup.internal-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,659 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-43, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:42,659 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-5, groupId=backup-mm2] Resetting offset for partition mm2-status.backup.internal-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,660 - INFO  [DistributedHerder-connect-2-1:KafkaBasedLog@161] - Finished reading KafkaBasedLog for topic mm2-status.backup.internal
2023-07-30 13:49:42,660 - INFO  [DistributedHerder-connect-2-1:KafkaBasedLog@163] - Started KafkaBasedLog for topic mm2-status.backup.internal
2023-07-30 13:49:42,661 - INFO  [DistributedHerder-connect-2-1:KafkaConfigBackingStore@265] - Starting KafkaConfigBackingStore
2023-07-30 13:49:42,661 - INFO  [DistributedHerder-connect-2-1:KafkaBasedLog@127] - Starting KafkaBasedLog with topic mm2-configs.backup.internal
2023-07-30 13:49:42,661 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-43 in /tmp/junit6293896813403981889/junit8934247217114872089/__consumer_offsets-43 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:42,661 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-43 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-43
2023-07-30 13:49:42,661 - INFO  [DistributedHerder-connect-2-1:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:35687]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:42,662 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-43 broker=0] Log loaded for partition __consumer_offsets-43 with initial high watermark 0
2023-07-30 13:49:42,663 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,664 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:49:42,664 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:42,664 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,664 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:42,664 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,664 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,664 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:42,664 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:42,664 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,664 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,664 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,664 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,664 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,664 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:42,664 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:42,665 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739382664
2023-07-30 13:49:42,669 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-13, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:42,670 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-13 in /tmp/junit6293896813403981889/junit8934247217114872089/__consumer_offsets-13 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:42,670 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-13 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-13
2023-07-30 13:49:42,670 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-13 broker=0] Log loaded for partition __consumer_offsets-13 with initial high watermark 0
Jul 30, 2023 1:49:42 PM org.glassfish.jersey.internal.Errors logErrors
WARNING: The following warnings have been detected: WARNING: The (sub)resource method listLoggers in org.apache.kafka.connect.runtime.rest.resources.LoggingResource contains empty path annotation.
WARNING: The (sub)resource method createConnector in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectors in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectorPlugins in org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource contains empty path annotation.
WARNING: The (sub)resource method serverInfo in org.apache.kafka.connect.runtime.rest.resources.RootResource contains empty path annotation.

2023-07-30 13:49:42,674 - INFO  [main:ContextHandler@849] - Started o.e.j.s.ServletContextHandler@1fac1d5c{/,null,AVAILABLE}
2023-07-30 13:49:42,674 - INFO  [main:RestServer@319] - REST resources initialized; server is started and ready to handle requests
2023-07-30 13:49:42,674 - INFO  [main:Connect@57] - Kafka Connect started
2023-07-30 13:49:42,675 - INFO  [main:EmbeddedConnectCluster@169] - Started worker WorkerHandle{workerName='connect-worker-1'workerURL='http://localhost:33591/'}
2023-07-30 13:49:42,675 - INFO  [main:ConnectDistributed@90] - Scanning for plugin classes. This might take a moment ...
2023-07-30 13:49:42,676 - INFO  [kafka-admin-client-thread | adminclient-20:AppInfoParser@83] - App info kafka.admin.client for adminclient-20 unregistered
2023-07-30 13:49:42,677 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=__consumer_offsets-28, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:42,677 - INFO  [kafka-admin-client-thread | adminclient-20:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:42,677 - INFO  [kafka-admin-client-thread | adminclient-20:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:42,677 - INFO  [kafka-admin-client-thread | adminclient-20:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:42,678 - INFO  [DistributedHerder-connect-2-1:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:35687]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-7
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:49:42,678 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition __consumer_offsets-28 in /tmp/junit6293896813403981889/junit8934247217114872089/__consumer_offsets-28 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:42,678 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-28 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-28
2023-07-30 13:49:42,678 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition __consumer_offsets-28 broker=0] Log loaded for partition __consumer_offsets-28 with initial high watermark 0
2023-07-30 13:49:42,679 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,679 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:49:42,680 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:42,680 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,680 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:42,680 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,680 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,680 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:42,680 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:42,680 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,680 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,680 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,680 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,680 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,681 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:42,681 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:42,681 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739382680
2023-07-30 13:49:42,681 - INFO  [DistributedHerder-connect-2-1:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:35687]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-backup-mm2-6
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = backup-mm2
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:49:42,682 - INFO  [kafka-producer-network-thread | producer-7:Metadata@279] - [Producer clientId=producer-7] Cluster ID: XnnaS7BkQ22Tt1TiwnBd0Q
2023-07-30 13:49:42,682 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,683 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:49:42,683 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,683 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:42,683 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,683 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,683 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:42,683 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:42,683 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,683 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,683 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,683 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,683 - WARN  [DistributedHerder-connect-2-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,683 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:42,683 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:42,683 - INFO  [DistributedHerder-connect-2-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739382683
2023-07-30 13:49:42,683 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3
2023-07-30 13:49:42,685 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-18
2023-07-30 13:49:42,685 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-41
2023-07-30 13:49:42,685 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-10
2023-07-30 13:49:42,685 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-33
2023-07-30 13:49:42,685 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-48
2023-07-30 13:49:42,685 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-19
2023-07-30 13:49:42,685 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-34
2023-07-30 13:49:42,685 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4
2023-07-30 13:49:42,685 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-11
2023-07-30 13:49:42,685 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-26
2023-07-30 13:49:42,685 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-49
2023-07-30 13:49:42,685 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-39
2023-07-30 13:49:42,685 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-9
2023-07-30 13:49:42,685 - INFO  [DistributedHerder-connect-2-1:Metadata@279] - [Consumer clientId=consumer-backup-mm2-6, groupId=backup-mm2] Cluster ID: XnnaS7BkQ22Tt1TiwnBd0Q
2023-07-30 13:49:42,685 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-24
2023-07-30 13:49:42,686 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-31
2023-07-30 13:49:42,686 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-46
2023-07-30 13:49:42,686 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1
2023-07-30 13:49:42,686 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-16
2023-07-30 13:49:42,686 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2
2023-07-30 13:49:42,686 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-25
2023-07-30 13:49:42,686 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-40
2023-07-30 13:49:42,686 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-47
2023-07-30 13:49:42,686 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-17
2023-07-30 13:49:42,687 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-32
2023-07-30 13:49:42,687 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-37
2023-07-30 13:49:42,687 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-7
2023-07-30 13:49:42,687 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-22
2023-07-30 13:49:42,687 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-29
2023-07-30 13:49:42,687 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-44
2023-07-30 13:49:42,687 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-14
2023-07-30 13:49:42,687 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-23
2023-07-30 13:49:42,687 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-38
2023-07-30 13:49:42,687 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-8
2023-07-30 13:49:42,687 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-45
2023-07-30 13:49:42,687 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-15
2023-07-30 13:49:42,687 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-30
2023-07-30 13:49:42,687 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0
2023-07-30 13:49:42,687 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-35
2023-07-30 13:49:42,687 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-5
2023-07-30 13:49:42,687 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-20
2023-07-30 13:49:42,687 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-27
2023-07-30 13:49:42,687 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-42
2023-07-30 13:49:42,688 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-12
2023-07-30 13:49:42,688 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-21
2023-07-30 13:49:42,688 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-36
2023-07-30 13:49:42,688 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-6
2023-07-30 13:49:42,688 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-43
2023-07-30 13:49:42,688 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-13
2023-07-30 13:49:42,688 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-28
2023-07-30 13:49:42,689 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 5 milliseconds, of which 2 milliseconds was spent in the scheduler.
2023-07-30 13:49:42,689 - INFO  [DistributedHerder-connect-2-1:KafkaConsumer@1116] - [Consumer clientId=consumer-backup-mm2-6, groupId=backup-mm2] Subscribed to partition(s): mm2-configs.backup.internal-0
2023-07-30 13:49:42,689 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-6, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-configs.backup.internal-0
2023-07-30 13:49:42,690 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-18 in 4 milliseconds, of which 4 milliseconds was spent in the scheduler.
2023-07-30 13:49:42,690 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-41 in 5 milliseconds, of which 5 milliseconds was spent in the scheduler.
2023-07-30 13:49:42,690 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-10 in 5 milliseconds, of which 5 milliseconds was spent in the scheduler.
2023-07-30 13:49:42,690 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-33 in 5 milliseconds, of which 5 milliseconds was spent in the scheduler.
2023-07-30 13:49:42,690 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-48 in 5 milliseconds, of which 5 milliseconds was spent in the scheduler.
2023-07-30 13:49:42,690 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-19 in 5 milliseconds, of which 5 milliseconds was spent in the scheduler.
2023-07-30 13:49:42,691 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-34 in 6 milliseconds, of which 5 milliseconds was spent in the scheduler.
2023-07-30 13:49:42,691 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler.
2023-07-30 13:49:42,691 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-11 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler.
2023-07-30 13:49:42,691 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-26 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler.
2023-07-30 13:49:42,691 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-49 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler.
2023-07-30 13:49:42,691 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-39 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler.
2023-07-30 13:49:42,692 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-9 in 7 milliseconds, of which 6 milliseconds was spent in the scheduler.
2023-07-30 13:49:42,692 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-24 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler.
2023-07-30 13:49:42,692 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-31 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler.
2023-07-30 13:49:42,692 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-46 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler.
2023-07-30 13:49:42,692 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler.
2023-07-30 13:49:42,692 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-16 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler.
2023-07-30 13:49:42,692 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler.
2023-07-30 13:49:42,692 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-25 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler.
2023-07-30 13:49:42,692 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-40 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler.
2023-07-30 13:49:42,693 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-47 in 7 milliseconds, of which 6 milliseconds was spent in the scheduler.
2023-07-30 13:49:42,693 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-17 in 7 milliseconds, of which 7 milliseconds was spent in the scheduler.
2023-07-30 13:49:42,693 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-32 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler.
2023-07-30 13:49:42,693 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-37 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler.
2023-07-30 13:49:42,693 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-7 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler.
2023-07-30 13:49:42,693 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-22 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler.
2023-07-30 13:49:42,693 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-29 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler.
2023-07-30 13:49:42,693 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-44 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler.
2023-07-30 13:49:42,694 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-14 in 7 milliseconds, of which 6 milliseconds was spent in the scheduler.
2023-07-30 13:49:42,694 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-23 in 7 milliseconds, of which 7 milliseconds was spent in the scheduler.
2023-07-30 13:49:42,694 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-38 in 7 milliseconds, of which 7 milliseconds was spent in the scheduler.
2023-07-30 13:49:42,694 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-8 in 7 milliseconds, of which 7 milliseconds was spent in the scheduler.
2023-07-30 13:49:42,694 - INFO  [DistributedHerder-connect-2-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-6, groupId=backup-mm2] Resetting offset for partition mm2-configs.backup.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,694 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-45 in 7 milliseconds, of which 7 milliseconds was spent in the scheduler.
2023-07-30 13:49:42,694 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-15 in 7 milliseconds, of which 7 milliseconds was spent in the scheduler.
2023-07-30 13:49:42,694 - INFO  [DistributedHerder-connect-2-1:KafkaBasedLog@161] - Finished reading KafkaBasedLog for topic mm2-configs.backup.internal
2023-07-30 13:49:42,694 - INFO  [DistributedHerder-connect-2-1:KafkaBasedLog@163] - Started KafkaBasedLog for topic mm2-configs.backup.internal
2023-07-30 13:49:42,695 - INFO  [DistributedHerder-connect-2-1:KafkaConfigBackingStore@280] - Started KafkaConfigBackingStore
2023-07-30 13:49:42,694 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-30 in 7 milliseconds, of which 7 milliseconds was spent in the scheduler.
2023-07-30 13:49:42,695 - INFO  [DistributedHerder-connect-2-1:DistributedHerder@290] - [Worker clientId=connect-2, groupId=backup-mm2] Herder started
2023-07-30 13:49:42,696 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 9 milliseconds, of which 8 milliseconds was spent in the scheduler.
2023-07-30 13:49:42,697 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-35 in 10 milliseconds, of which 9 milliseconds was spent in the scheduler.
2023-07-30 13:49:42,697 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-5 in 10 milliseconds, of which 10 milliseconds was spent in the scheduler.
2023-07-30 13:49:42,697 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-20 in 10 milliseconds, of which 10 milliseconds was spent in the scheduler.
2023-07-30 13:49:42,697 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-27 in 10 milliseconds, of which 10 milliseconds was spent in the scheduler.
2023-07-30 13:49:42,697 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-42 in 9 milliseconds, of which 9 milliseconds was spent in the scheduler.
2023-07-30 13:49:42,697 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-12 in 9 milliseconds, of which 9 milliseconds was spent in the scheduler.
2023-07-30 13:49:42,697 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-21 in 9 milliseconds, of which 9 milliseconds was spent in the scheduler.
2023-07-30 13:49:42,698 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-36 in 10 milliseconds, of which 10 milliseconds was spent in the scheduler.
2023-07-30 13:49:42,698 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-6 in 10 milliseconds, of which 10 milliseconds was spent in the scheduler.
2023-07-30 13:49:42,698 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-43 in 10 milliseconds, of which 10 milliseconds was spent in the scheduler.
2023-07-30 13:49:42,698 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-13 in 10 milliseconds, of which 10 milliseconds was spent in the scheduler.
2023-07-30 13:49:42,698 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-28 in 10 milliseconds, of which 10 milliseconds was spent in the scheduler.
2023-07-30 13:49:42,700 - INFO  [DistributedHerder-connect-2-1:Metadata@279] - [Worker clientId=connect-2, groupId=backup-mm2] Cluster ID: XnnaS7BkQ22Tt1TiwnBd0Q
2023-07-30 13:49:42,701 - INFO  [DistributedHerder-connect-2-1:AbstractCoordinator$FindCoordinatorResponseHandler@841] - [Worker clientId=connect-2, groupId=backup-mm2] Discovered group coordinator localhost:35687 (id: 2147483647 rack: null)
2023-07-30 13:49:42,703 - INFO  [DistributedHerder-connect-2-1:WorkerCoordinator@225] - [Worker clientId=connect-2, groupId=backup-mm2] Rebalance started
2023-07-30 13:49:42,703 - INFO  [DistributedHerder-connect-2-1:AbstractCoordinator@534] - [Worker clientId=connect-2, groupId=backup-mm2] (Re-)joining group
2023-07-30 13:49:42,723 - INFO  [DistributedHerder-connect-2-1:AbstractCoordinator@468] - [Worker clientId=connect-2, groupId=backup-mm2] Rebalance failed.
org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group.
2023-07-30 13:49:42,724 - INFO  [DistributedHerder-connect-2-1:AbstractCoordinator@534] - [Worker clientId=connect-2, groupId=backup-mm2] (Re-)joining group
2023-07-30 13:49:42,731 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group backup-mm2 in state PreparingRebalance with old generation 0 (__consumer_offsets-5) (reason: Adding new member connect-2-12409f17-1c6b-4299-8d44-e2b23de6867f with group instance id None)
2023-07-30 13:49:42,737 - INFO  [executor-Rebalance:Logging@66] - [GroupCoordinator 0]: Stabilized group backup-mm2 generation 1 (__consumer_offsets-5)
2023-07-30 13:49:42,740 - INFO  [DistributedHerder-connect-2-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-2, groupId=backup-mm2] Successfully joined group with generation Generation{generationId=1, memberId='connect-2-12409f17-1c6b-4299-8d44-e2b23de6867f', protocol='sessioned'}
2023-07-30 13:49:42,758 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group backup-mm2 for generation 1
2023-07-30 13:49:42,773 - INFO  [main:Reflections@239] - Reflections took 95 ms to scan 1 urls, producing 349 keys and 1055 values [using 20 cores]
2023-07-30 13:49:42,774 - WARN  [main:ReflectionUtils@318] - could not get type for name kotlin.jvm.internal.Lambda from any class loader
org.reflections.ReflectionsException: could not get type for name kotlin.jvm.internal.Lambda
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:91)
	at org.apache.kafka.connect.util.clusters.WorkerHandle.start(WorkerHandle.java:50)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.addWorker(EmbeddedConnectCluster.java:167)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.startConnect(EmbeddedConnectCluster.java:253)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.start(EmbeddedConnectCluster.java:137)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:130)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: kotlin.jvm.internal.Lambda
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 68 more
2023-07-30 13:49:42,776 - WARN  [main:ReflectionUtils@318] - could not get type for name kotlin.jvm.functions.Function0 from any class loader
org.reflections.ReflectionsException: could not get type for name kotlin.jvm.functions.Function0
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:91)
	at org.apache.kafka.connect.util.clusters.WorkerHandle.start(WorkerHandle.java:50)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.addWorker(EmbeddedConnectCluster.java:167)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.startConnect(EmbeddedConnectCluster.java:253)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.start(EmbeddedConnectCluster.java:137)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:130)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: kotlin.jvm.functions.Function0
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 68 more
2023-07-30 13:49:42,778 - WARN  [main:ReflectionUtils@318] - could not get type for name java.util.spi.ToolProvider from any class loader
org.reflections.ReflectionsException: could not get type for name java.util.spi.ToolProvider
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:91)
	at org.apache.kafka.connect.util.clusters.WorkerHandle.start(WorkerHandle.java:50)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.addWorker(EmbeddedConnectCluster.java:167)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.startConnect(EmbeddedConnectCluster.java:253)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.start(EmbeddedConnectCluster.java:137)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:130)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: java.util.spi.ToolProvider
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 68 more
2023-07-30 13:49:42,781 - INFO  [main:DelegatingClassLoader@269] - Registered loader: java.net.FactoryURLClassLoader@483bf400
2023-07-30 13:49:42,781 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension'
2023-07-30 13:49:42,781 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.runtime.isolation.PluginsTest$TestConnectRestExtension'
2023-07-30 13:49:42,781 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy'
2023-07-30 13:49:42,782 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy'
2023-07-30 13:49:42,782 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy'
2023-07-30 13:49:42,782 - INFO  [main:DelegatingClassLoader@427] - Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension'
2023-07-30 13:49:42,782 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'TestConnectRestExtension' and 'Test' to plugin 'org.apache.kafka.connect.runtime.isolation.PluginsTest$TestConnectRestExtension'
2023-07-30 13:49:42,783 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy'
2023-07-30 13:49:42,783 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy'
2023-07-30 13:49:42,783 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy'
2023-07-30 13:49:42,783 - INFO  [main:AbstractConfig@361] - DistributedConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:35687]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	config.providers = []
	config.storage.replication.factor = 1
	config.storage.topic = mm2-configs.backup.internal
	connect.protocol = sessioned
	connections.max.idle.ms = 540000
	connector.client.config.override.policy = None
	group.id = backup-mm2
	header.converter = class org.apache.kafka.connect.converters.ByteArrayConverter
	heartbeat.interval.ms = 3000
	inter.worker.key.generation.algorithm = HmacSHA256
	inter.worker.key.size = null
	inter.worker.key.ttl.ms = 3600000
	inter.worker.signature.algorithm = HmacSHA256
	inter.worker.verification.algorithms = [HmacSHA256]
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.converters.ByteArrayConverter
	listeners = null
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 60000
	offset.flush.timeout.ms = 5000
	offset.storage.partitions = 25
	offset.storage.replication.factor = 1
	offset.storage.topic = mm2-offsets.backup.internal
	plugin.path = null
	rebalance.timeout.ms = 60000
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 40000
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = localhost
	rest.port = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	scheduled.rebalance.max.delay.ms = 300000
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	status.storage.partitions = 5
	status.storage.replication.factor = 1
	status.storage.topic = mm2-status.backup.internal
	task.shutdown.graceful.timeout.ms = 5000
	topic.creation.enable = true
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.converters.ByteArrayConverter
	worker.sync.timeout.ms = 3000
	worker.unsync.backoff.ms = 300000

2023-07-30 13:49:42,784 - WARN  [main:WorkerConfig@420] - Variables cannot be used in the 'plugin.path' property, since the property is used by plugin scanning before the config providers that replace the variables are initialized. The raw value 'null' was used for plugin scanning, as opposed to the transformed value 'null', and this may cause unexpected results.
2023-07-30 13:49:42,784 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:49:42,784 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:35687]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:42,785 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,785 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:42,785 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,785 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:42,786 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,786 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,786 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:42,786 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,786 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,786 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,786 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,786 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,786 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:42,786 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:42,786 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739382786
2023-07-30 13:49:42,791 - INFO  [DistributedHerder-connect-1-1:AbstractCoordinator$FindCoordinatorResponseHandler@841] - [Worker clientId=connect-1, groupId=backup-mm2] Discovered group coordinator localhost:35687 (id: 2147483647 rack: null)
2023-07-30 13:49:42,792 - INFO  [DistributedHerder-connect-1-1:WorkerCoordinator@225] - [Worker clientId=connect-1, groupId=backup-mm2] Rebalance started
2023-07-30 13:49:42,792 - INFO  [DistributedHerder-connect-1-1:AbstractCoordinator@534] - [Worker clientId=connect-1, groupId=backup-mm2] (Re-)joining group
2023-07-30 13:49:42,793 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: XnnaS7BkQ22Tt1TiwnBd0Q
2023-07-30 13:49:42,793 - INFO  [kafka-admin-client-thread | adminclient-21:AppInfoParser@83] - App info kafka.admin.client for adminclient-21 unregistered
2023-07-30 13:49:42,794 - INFO  [kafka-admin-client-thread | adminclient-21:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:42,794 - INFO  [kafka-admin-client-thread | adminclient-21:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:42,794 - INFO  [kafka-admin-client-thread | adminclient-21:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:42,794 - INFO  [main:RestServer@132] - Added connector for http://localhost:0
2023-07-30 13:49:42,794 - INFO  [main:RestServer@204] - Initializing REST server
2023-07-30 13:49:42,795 - INFO  [main:Server@360] - jetty-9.4.30.v20200611; built: 2020-06-11T12:34:51.929Z; git: 271836e4c1f4612f12b7bb13ef5a92a927634b0d; jvm 1.8.0_275-8u275-b01-0ubuntu1~18.04-b01
2023-07-30 13:49:42,802 - INFO  [main:AbstractConnector@331] - Started http_localhost0@28da7d11{HTTP/1.1, (http/1.1)}{localhost:37073}
2023-07-30 13:49:42,803 - INFO  [main:Server@400] - Started @5061ms
2023-07-30 13:49:42,803 - INFO  [main:RestServer@371] - Advertised URI: http://localhost:37073/
2023-07-30 13:49:42,803 - INFO  [main:RestServer@219] - REST server listening at http://localhost:37073/, advertising URL http://localhost:37073/
2023-07-30 13:49:42,803 - INFO  [main:RestServer@371] - Advertised URI: http://localhost:37073/
2023-07-30 13:49:42,804 - INFO  [main:RestServer@220] - REST admin endpoints at http://localhost:37073/
2023-07-30 13:49:42,804 - INFO  [main:RestServer@371] - Advertised URI: http://localhost:37073/
2023-07-30 13:49:42,804 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:49:42,804 - INFO  [DistributedHerder-connect-2-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-2, groupId=backup-mm2] Successfully synced group in generation Generation{generationId=1, memberId='connect-2-12409f17-1c6b-4299-8d44-e2b23de6867f', protocol='sessioned'}
2023-07-30 13:49:42,804 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:35687]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:42,804 - INFO  [DistributedHerder-connect-2-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-2, groupId=backup-mm2] Joined group at generation 1 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-2-12409f17-1c6b-4299-8d44-e2b23de6867f', leaderUrl='http://localhost:33591/', offset=-1, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:49:42,805 - INFO  [DistributedHerder-connect-1-1:AbstractCoordinator@468] - [Worker clientId=connect-1, groupId=backup-mm2] Rebalance failed.
org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group.
2023-07-30 13:49:42,805 - INFO  [DistributedHerder-connect-1-1:AbstractCoordinator@534] - [Worker clientId=connect-1, groupId=backup-mm2] (Re-)joining group
2023-07-30 13:49:42,805 - INFO  [DistributedHerder-connect-2-1:DistributedHerder@1215] - [Worker clientId=connect-2, groupId=backup-mm2] Starting connectors and tasks using config offset -1
2023-07-30 13:49:42,805 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,805 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:42,805 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,805 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:42,806 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,806 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,805 - INFO  [DistributedHerder-connect-2-1:DistributedHerder@1243] - [Worker clientId=connect-2, groupId=backup-mm2] Finished starting connectors and tasks
2023-07-30 13:49:42,806 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:42,807 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,807 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,807 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,807 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,807 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,806 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group backup-mm2 in state PreparingRebalance with old generation 1 (__consumer_offsets-5) (reason: Adding new member connect-1-e0f7ca44-8c98-43fc-a034-dae0d9034b88 with group instance id None)
2023-07-30 13:49:42,807 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:42,807 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:42,807 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739382807
2023-07-30 13:49:42,813 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: XnnaS7BkQ22Tt1TiwnBd0Q
2023-07-30 13:49:42,813 - INFO  [kafka-admin-client-thread | adminclient-22:AppInfoParser@83] - App info kafka.admin.client for adminclient-22 unregistered
2023-07-30 13:49:42,814 - INFO  [kafka-admin-client-thread | adminclient-22:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:42,814 - INFO  [kafka-admin-client-thread | adminclient-22:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:42,814 - INFO  [kafka-admin-client-thread | adminclient-22:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:42,814 - INFO  [main:NoneConnectorClientConfigOverridePolicy@45] - Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden
2023-07-30 13:49:42,814 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:49:42,815 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:35687]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:42,815 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,816 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:42,816 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,816 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:42,816 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,816 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,816 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:42,816 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,817 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,817 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,817 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,817 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,817 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:42,817 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:42,817 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739382817
2023-07-30 13:49:42,823 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: XnnaS7BkQ22Tt1TiwnBd0Q
2023-07-30 13:49:42,824 - INFO  [kafka-admin-client-thread | adminclient-23:AppInfoParser@83] - App info kafka.admin.client for adminclient-23 unregistered
2023-07-30 13:49:42,824 - INFO  [kafka-admin-client-thread | adminclient-23:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:42,824 - INFO  [kafka-admin-client-thread | adminclient-23:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:42,824 - INFO  [kafka-admin-client-thread | adminclient-23:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:42,825 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:42,825 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:42,825 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739382825
2023-07-30 13:49:42,825 - INFO  [main:AbstractConfig@361] - JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false

2023-07-30 13:49:42,826 - INFO  [main:AbstractConfig@361] - JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false

2023-07-30 13:49:42,826 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:49:42,826 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:35687]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:42,827 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,827 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:42,827 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,827 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:42,827 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,827 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,827 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:42,827 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,827 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,828 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,828 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,828 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,828 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:42,828 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:42,828 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739382828
2023-07-30 13:49:42,834 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: XnnaS7BkQ22Tt1TiwnBd0Q
2023-07-30 13:49:42,834 - INFO  [kafka-admin-client-thread | adminclient-24:AppInfoParser@83] - App info kafka.admin.client for adminclient-24 unregistered
2023-07-30 13:49:42,835 - INFO  [kafka-admin-client-thread | adminclient-24:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:42,835 - INFO  [kafka-admin-client-thread | adminclient-24:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:42,835 - INFO  [kafka-admin-client-thread | adminclient-24:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:42,835 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:49:42,835 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:35687]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:42,837 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,837 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:42,837 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,837 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:42,838 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,838 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,838 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:42,838 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,838 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,838 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,839 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,839 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,839 - INFO  [KafkaBasedLog Work Thread - mm2-configs.backup.internal:DistributedHerder$ConfigUpdateListener@1577] - [Worker clientId=connect-2, groupId=backup-mm2] Session key updated
2023-07-30 13:49:42,839 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:42,839 - INFO  [KafkaBasedLog Work Thread - mm2-configs.backup.internal:DistributedHerder$ConfigUpdateListener@1577] - [Worker clientId=connect-1, groupId=backup-mm2] Session key updated
2023-07-30 13:49:42,839 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:42,840 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739382839
2023-07-30 13:49:42,845 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: XnnaS7BkQ22Tt1TiwnBd0Q
2023-07-30 13:49:42,845 - INFO  [kafka-admin-client-thread | adminclient-25:AppInfoParser@83] - App info kafka.admin.client for adminclient-25 unregistered
2023-07-30 13:49:42,846 - INFO  [kafka-admin-client-thread | adminclient-25:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:42,846 - INFO  [kafka-admin-client-thread | adminclient-25:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:42,846 - INFO  [kafka-admin-client-thread | adminclient-25:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:42,846 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:49:42,846 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:35687]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:42,848 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,848 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:42,848 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,848 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:42,848 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,848 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,848 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:42,849 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,849 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,849 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,849 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,849 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,849 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:42,849 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:42,850 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739382849
2023-07-30 13:49:42,854 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: XnnaS7BkQ22Tt1TiwnBd0Q
2023-07-30 13:49:42,854 - INFO  [kafka-admin-client-thread | adminclient-26:AppInfoParser@83] - App info kafka.admin.client for adminclient-26 unregistered
2023-07-30 13:49:42,855 - INFO  [kafka-admin-client-thread | adminclient-26:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:42,855 - INFO  [kafka-admin-client-thread | adminclient-26:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:42,855 - INFO  [kafka-admin-client-thread | adminclient-26:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:42,856 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:49:42,856 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:35687]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:42,858 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,858 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:42,858 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,858 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:42,858 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,858 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,859 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:42,859 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,859 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,859 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,859 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,859 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,859 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:42,860 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:42,860 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739382859
2023-07-30 13:49:42,864 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: XnnaS7BkQ22Tt1TiwnBd0Q
2023-07-30 13:49:42,864 - INFO  [kafka-admin-client-thread | adminclient-27:AppInfoParser@83] - App info kafka.admin.client for adminclient-27 unregistered
2023-07-30 13:49:42,865 - INFO  [kafka-admin-client-thread | adminclient-27:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:42,865 - INFO  [kafka-admin-client-thread | adminclient-27:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:42,865 - INFO  [kafka-admin-client-thread | adminclient-27:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:42,866 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:42,866 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:42,867 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739382866
2023-07-30 13:49:42,867 - INFO  [main:ConnectDistributed@128] - Kafka Connect distributed worker initialization took 192ms
2023-07-30 13:49:42,867 - INFO  [main:Connect@51] - Kafka Connect starting
2023-07-30 13:49:42,867 - INFO  [main:RestServer@224] - Initializing REST resources
2023-07-30 13:49:42,867 - INFO  [DistributedHerder-connect-3-1:DistributedHerder@286] - [Worker clientId=connect-3, groupId=backup-mm2] Herder starting
2023-07-30 13:49:42,867 - INFO  [DistributedHerder-connect-3-1:Worker@195] - Worker starting
2023-07-30 13:49:42,867 - INFO  [DistributedHerder-connect-3-1:KafkaOffsetBackingStore@127] - Starting KafkaOffsetBackingStore
2023-07-30 13:49:42,867 - INFO  [DistributedHerder-connect-3-1:KafkaBasedLog@127] - Starting KafkaBasedLog with topic mm2-offsets.backup.internal
2023-07-30 13:49:42,867 - INFO  [main:RestServer@241] - Adding admin resources to main listener
2023-07-30 13:49:42,868 - INFO  [DistributedHerder-connect-3-1:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:35687]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:42,868 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,868 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:49:42,868 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:42,868 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,868 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:42,868 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,868 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,868 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:42,869 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:42,869 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,869 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,869 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,869 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,869 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,869 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:42,869 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:42,869 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739382869
2023-07-30 13:49:42,870 - INFO  [main:DefaultSessionIdManager@334] - DefaultSessionIdManager workerName=node0
2023-07-30 13:49:42,870 - INFO  [main:DefaultSessionIdManager@339] - No SessionScavenger set, using defaults
2023-07-30 13:49:42,871 - INFO  [main:HouseKeeper@140] - node0 Scavenging every 600000ms
2023-07-30 13:49:42,879 - INFO  [kafka-admin-client-thread | adminclient-28:AppInfoParser@83] - App info kafka.admin.client for adminclient-28 unregistered
2023-07-30 13:49:42,880 - INFO  [kafka-admin-client-thread | adminclient-28:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:42,880 - INFO  [kafka-admin-client-thread | adminclient-28:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:42,880 - INFO  [kafka-admin-client-thread | adminclient-28:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:42,880 - INFO  [DistributedHerder-connect-3-1:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:35687]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-8
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:49:42,882 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,882 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:49:42,882 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:42,882 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,882 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:42,882 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,882 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,882 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:42,882 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:42,883 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,883 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,883 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,883 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,883 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,883 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:42,883 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:42,883 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739382883
2023-07-30 13:49:42,883 - INFO  [DistributedHerder-connect-3-1:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:35687]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-backup-mm2-7
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = backup-mm2
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:49:42,884 - INFO  [kafka-producer-network-thread | producer-8:Metadata@279] - [Producer clientId=producer-8] Cluster ID: XnnaS7BkQ22Tt1TiwnBd0Q
2023-07-30 13:49:42,884 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,885 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:49:42,885 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,885 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:42,885 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,885 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,885 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:42,885 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:42,885 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,885 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,885 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,885 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,885 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,885 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:42,885 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:42,885 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739382885
2023-07-30 13:49:42,888 - INFO  [DistributedHerder-connect-3-1:Metadata@279] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Cluster ID: XnnaS7BkQ22Tt1TiwnBd0Q
2023-07-30 13:49:42,893 - INFO  [DistributedHerder-connect-3-1:KafkaConsumer@1116] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Subscribed to partition(s): mm2-offsets.backup.internal-0, mm2-offsets.backup.internal-5, mm2-offsets.backup.internal-10, mm2-offsets.backup.internal-20, mm2-offsets.backup.internal-15, mm2-offsets.backup.internal-9, mm2-offsets.backup.internal-11, mm2-offsets.backup.internal-16, mm2-offsets.backup.internal-4, mm2-offsets.backup.internal-17, mm2-offsets.backup.internal-3, mm2-offsets.backup.internal-24, mm2-offsets.backup.internal-23, mm2-offsets.backup.internal-13, mm2-offsets.backup.internal-18, mm2-offsets.backup.internal-22, mm2-offsets.backup.internal-2, mm2-offsets.backup.internal-8, mm2-offsets.backup.internal-12, mm2-offsets.backup.internal-19, mm2-offsets.backup.internal-14, mm2-offsets.backup.internal-1, mm2-offsets.backup.internal-6, mm2-offsets.backup.internal-7, mm2-offsets.backup.internal-21
2023-07-30 13:49:42,893 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-0
2023-07-30 13:49:42,893 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-5
2023-07-30 13:49:42,893 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-10
2023-07-30 13:49:42,893 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-20
2023-07-30 13:49:42,893 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-15
2023-07-30 13:49:42,893 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-9
2023-07-30 13:49:42,893 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-11
2023-07-30 13:49:42,893 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-16
2023-07-30 13:49:42,893 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-4
2023-07-30 13:49:42,893 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-17
2023-07-30 13:49:42,893 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-3
2023-07-30 13:49:42,893 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-24
2023-07-30 13:49:42,894 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-23
2023-07-30 13:49:42,894 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-13
2023-07-30 13:49:42,894 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-18
2023-07-30 13:49:42,894 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-22
2023-07-30 13:49:42,894 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-2
2023-07-30 13:49:42,894 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-8
2023-07-30 13:49:42,894 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-12
2023-07-30 13:49:42,894 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-19
2023-07-30 13:49:42,894 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-14
2023-07-30 13:49:42,894 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-1
2023-07-30 13:49:42,894 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-6
2023-07-30 13:49:42,894 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-7
2023-07-30 13:49:42,894 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-offsets.backup.internal-21
2023-07-30 13:49:42,901 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-18 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,901 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-20 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,901 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-22 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,901 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-24 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,901 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,901 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,901 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,901 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-6 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,901 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-8 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,901 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-10 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,901 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-12 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,902 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-14 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,902 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-16 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,902 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-19 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,902 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-21 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,902 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-23 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,902 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,902 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,902 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,902 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-7 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,902 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-9 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,902 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-11 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,902 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-13 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,903 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-15 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,903 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-7, groupId=backup-mm2] Resetting offset for partition mm2-offsets.backup.internal-17 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,903 - INFO  [DistributedHerder-connect-3-1:KafkaBasedLog@161] - Finished reading KafkaBasedLog for topic mm2-offsets.backup.internal
2023-07-30 13:49:42,903 - INFO  [DistributedHerder-connect-3-1:KafkaBasedLog@163] - Started KafkaBasedLog for topic mm2-offsets.backup.internal
2023-07-30 13:49:42,904 - INFO  [DistributedHerder-connect-3-1:KafkaOffsetBackingStore@129] - Finished reading offsets topic and starting KafkaOffsetBackingStore
2023-07-30 13:49:42,904 - INFO  [DistributedHerder-connect-3-1:Worker@202] - Worker started
2023-07-30 13:49:42,904 - INFO  [DistributedHerder-connect-3-1:KafkaBasedLog@127] - Starting KafkaBasedLog with topic mm2-status.backup.internal
2023-07-30 13:49:42,904 - INFO  [DistributedHerder-connect-3-1:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:35687]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:42,906 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,906 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:49:42,906 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:42,907 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,907 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:42,907 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,907 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,907 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:42,907 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:42,907 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,907 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,907 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,907 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,907 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,907 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:42,907 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:42,907 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739382907
Jul 30, 2023 1:49:42 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource will be ignored. 
Jul 30, 2023 1:49:42 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.RootResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.RootResource will be ignored. 
Jul 30, 2023 1:49:42 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource will be ignored. 
Jul 30, 2023 1:49:42 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource will be ignored. 
2023-07-30 13:49:42,916 - INFO  [kafka-admin-client-thread | adminclient-29:AppInfoParser@83] - App info kafka.admin.client for adminclient-29 unregistered
2023-07-30 13:49:42,917 - INFO  [kafka-admin-client-thread | adminclient-29:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:42,917 - INFO  [kafka-admin-client-thread | adminclient-29:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:42,917 - INFO  [kafka-admin-client-thread | adminclient-29:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:42,917 - INFO  [DistributedHerder-connect-3-1:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:35687]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-9
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:49:42,918 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,919 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:49:42,919 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:42,919 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,919 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:42,919 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,920 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,920 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:42,920 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:42,920 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,920 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,920 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,920 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,920 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,920 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:42,920 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:42,920 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739382920
2023-07-30 13:49:42,920 - INFO  [DistributedHerder-connect-3-1:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:35687]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-backup-mm2-8
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = backup-mm2
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:49:42,921 - INFO  [kafka-producer-network-thread | producer-9:Metadata@279] - [Producer clientId=producer-9] Cluster ID: XnnaS7BkQ22Tt1TiwnBd0Q
2023-07-30 13:49:42,922 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,922 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:49:42,922 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,922 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:42,922 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,922 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,922 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:42,922 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:42,922 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,922 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,922 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,922 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,922 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,922 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:42,922 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:42,922 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739382922
2023-07-30 13:49:42,924 - INFO  [DistributedHerder-connect-3-1:Metadata@279] - [Consumer clientId=consumer-backup-mm2-8, groupId=backup-mm2] Cluster ID: XnnaS7BkQ22Tt1TiwnBd0Q
2023-07-30 13:49:42,927 - INFO  [DistributedHerder-connect-3-1:KafkaConsumer@1116] - [Consumer clientId=consumer-backup-mm2-8, groupId=backup-mm2] Subscribed to partition(s): mm2-status.backup.internal-0, mm2-status.backup.internal-4, mm2-status.backup.internal-1, mm2-status.backup.internal-2, mm2-status.backup.internal-3
2023-07-30 13:49:42,927 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-8, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-status.backup.internal-0
2023-07-30 13:49:42,927 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-8, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-status.backup.internal-4
2023-07-30 13:49:42,927 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-8, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-status.backup.internal-1
2023-07-30 13:49:42,927 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-8, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-status.backup.internal-2
2023-07-30 13:49:42,927 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-8, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-status.backup.internal-3
2023-07-30 13:49:42,931 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-8, groupId=backup-mm2] Resetting offset for partition mm2-status.backup.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,931 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-8, groupId=backup-mm2] Resetting offset for partition mm2-status.backup.internal-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,931 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-8, groupId=backup-mm2] Resetting offset for partition mm2-status.backup.internal-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,931 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-8, groupId=backup-mm2] Resetting offset for partition mm2-status.backup.internal-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,931 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-8, groupId=backup-mm2] Resetting offset for partition mm2-status.backup.internal-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,931 - INFO  [DistributedHerder-connect-3-1:KafkaBasedLog@161] - Finished reading KafkaBasedLog for topic mm2-status.backup.internal
2023-07-30 13:49:42,932 - INFO  [DistributedHerder-connect-3-1:KafkaBasedLog@163] - Started KafkaBasedLog for topic mm2-status.backup.internal
2023-07-30 13:49:42,932 - INFO  [DistributedHerder-connect-3-1:KafkaConfigBackingStore@265] - Starting KafkaConfigBackingStore
2023-07-30 13:49:42,932 - INFO  [DistributedHerder-connect-3-1:KafkaBasedLog@127] - Starting KafkaBasedLog with topic mm2-configs.backup.internal
2023-07-30 13:49:42,933 - INFO  [DistributedHerder-connect-3-1:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:35687]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:42,934 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,934 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:49:42,935 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:42,935 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,935 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:42,935 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,935 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,935 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:42,935 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:42,935 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,935 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,935 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,935 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,935 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,935 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:42,935 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:42,935 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739382935
2023-07-30 13:49:42,944 - INFO  [kafka-admin-client-thread | adminclient-30:AppInfoParser@83] - App info kafka.admin.client for adminclient-30 unregistered
2023-07-30 13:49:42,945 - INFO  [kafka-admin-client-thread | adminclient-30:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:42,945 - INFO  [kafka-admin-client-thread | adminclient-30:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:42,945 - INFO  [kafka-admin-client-thread | adminclient-30:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:42,945 - INFO  [DistributedHerder-connect-3-1:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:35687]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-10
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:49:42,947 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,947 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:49:42,947 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:42,947 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,949 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:42,949 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,949 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,949 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:42,949 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:42,949 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,949 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,949 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,950 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,950 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,950 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:42,950 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:42,950 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739382950
2023-07-30 13:49:42,950 - INFO  [DistributedHerder-connect-3-1:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:35687]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-backup-mm2-9
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = backup-mm2
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:49:42,951 - INFO  [kafka-producer-network-thread | producer-10:Metadata@279] - [Producer clientId=producer-10] Cluster ID: XnnaS7BkQ22Tt1TiwnBd0Q
2023-07-30 13:49:42,952 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,952 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:49:42,952 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,952 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:42,952 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,952 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,952 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:42,952 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:42,952 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,952 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:42,952 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:42,952 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,953 - WARN  [DistributedHerder-connect-3-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:42,953 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:42,953 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:42,953 - INFO  [DistributedHerder-connect-3-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739382953
Jul 30, 2023 1:49:42 PM org.glassfish.jersey.internal.Errors logErrors
WARNING: The following warnings have been detected: WARNING: The (sub)resource method listLoggers in org.apache.kafka.connect.runtime.rest.resources.LoggingResource contains empty path annotation.
WARNING: The (sub)resource method createConnector in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectors in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectorPlugins in org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource contains empty path annotation.
WARNING: The (sub)resource method serverInfo in org.apache.kafka.connect.runtime.rest.resources.RootResource contains empty path annotation.

2023-07-30 13:49:42,954 - INFO  [main:ContextHandler@849] - Started o.e.j.s.ServletContextHandler@6f94fb9d{/,null,AVAILABLE}
2023-07-30 13:49:42,955 - INFO  [main:RestServer@319] - REST resources initialized; server is started and ready to handle requests
2023-07-30 13:49:42,955 - INFO  [main:Connect@57] - Kafka Connect started
2023-07-30 13:49:42,955 - INFO  [DistributedHerder-connect-3-1:Metadata@279] - [Consumer clientId=consumer-backup-mm2-9, groupId=backup-mm2] Cluster ID: XnnaS7BkQ22Tt1TiwnBd0Q
2023-07-30 13:49:42,955 - INFO  [main:EmbeddedConnectCluster@169] - Started worker WorkerHandle{workerName='connect-worker-2'workerURL='http://localhost:37073/'}
2023-07-30 13:49:42,957 - INFO  [DistributedHerder-connect-3-1:KafkaConsumer@1116] - [Consumer clientId=consumer-backup-mm2-9, groupId=backup-mm2] Subscribed to partition(s): mm2-configs.backup.internal-0
2023-07-30 13:49:42,958 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@618] - [Consumer clientId=consumer-backup-mm2-9, groupId=backup-mm2] Seeking to EARLIEST offset of partition mm2-configs.backup.internal-0
2023-07-30 13:49:42,961 - INFO  [DistributedHerder-connect-3-1:SubscriptionState@396] - [Consumer clientId=consumer-backup-mm2-9, groupId=backup-mm2] Resetting offset for partition mm2-configs.backup.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:42,965 - INFO  [DistributedHerder-connect-3-1:KafkaBasedLog@161] - Finished reading KafkaBasedLog for topic mm2-configs.backup.internal
2023-07-30 13:49:42,965 - INFO  [DistributedHerder-connect-3-1:KafkaBasedLog@163] - Started KafkaBasedLog for topic mm2-configs.backup.internal
2023-07-30 13:49:42,966 - INFO  [DistributedHerder-connect-3-1:KafkaConfigBackingStore@280] - Started KafkaConfigBackingStore
2023-07-30 13:49:42,966 - INFO  [DistributedHerder-connect-3-1:DistributedHerder@290] - [Worker clientId=connect-3, groupId=backup-mm2] Herder started
2023-07-30 13:49:42,970 - INFO  [DistributedHerder-connect-3-1:Metadata@279] - [Worker clientId=connect-3, groupId=backup-mm2] Cluster ID: XnnaS7BkQ22Tt1TiwnBd0Q
2023-07-30 13:49:42,970 - INFO  [DistributedHerder-connect-3-1:AbstractCoordinator$FindCoordinatorResponseHandler@841] - [Worker clientId=connect-3, groupId=backup-mm2] Discovered group coordinator localhost:35687 (id: 2147483647 rack: null)
2023-07-30 13:49:42,970 - INFO  [DistributedHerder-connect-3-1:WorkerCoordinator@225] - [Worker clientId=connect-3, groupId=backup-mm2] Rebalance started
2023-07-30 13:49:42,971 - INFO  [DistributedHerder-connect-3-1:AbstractCoordinator@534] - [Worker clientId=connect-3, groupId=backup-mm2] (Re-)joining group
2023-07-30 13:49:42,973 - INFO  [DistributedHerder-connect-3-1:AbstractCoordinator@468] - [Worker clientId=connect-3, groupId=backup-mm2] Rebalance failed.
org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group.
2023-07-30 13:49:42,973 - INFO  [DistributedHerder-connect-3-1:AbstractCoordinator@534] - [Worker clientId=connect-3, groupId=backup-mm2] (Re-)joining group
2023-07-30 13:49:43,230 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:43083/ is {"version":"unknown","commit":"unknown","kafka_cluster_id":"XnnaS7BkQ22Tt1TiwnBd0Q"}
2023-07-30 13:49:43,244 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:33591/ is {"version":"unknown","commit":"unknown","kafka_cluster_id":"XnnaS7BkQ22Tt1TiwnBd0Q"}
2023-07-30 13:49:43,254 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:37073/ is {"version":"unknown","commit":"unknown","kafka_cluster_id":"XnnaS7BkQ22Tt1TiwnBd0Q"}
2023-07-30 13:49:43,257 - INFO  [main:FileTxnSnapLog@115] - zookeeper.snapshot.trust.empty : false
2023-07-30 13:49:43,257 - INFO  [main:ZKDatabase@117] - zookeeper.snapshotSizeFactor = 0.33
2023-07-30 13:49:43,258 - INFO  [main:ZooKeeperServer@938] - minSessionTimeout set to 1600
2023-07-30 13:49:43,258 - INFO  [main:ZooKeeperServer@947] - maxSessionTimeout set to 16000
2023-07-30 13:49:43,258 - INFO  [main:ZooKeeperServer@166] - Created server with tickTime 800 minSessionTimeout 1600 maxSessionTimeout 16000 datadir /tmp/kafka-5061408539614014686/version-2 snapdir /tmp/kafka-434762771172158815/version-2
2023-07-30 13:49:43,258 - INFO  [main:NIOServerCnxnFactory@673] - Configuring NIO connection handler with 10s sessionless connection timeout, 3 selector thread(s), 40 worker threads, and 64 kB direct buffers.
2023-07-30 13:49:43,258 - INFO  [main:NIOServerCnxnFactory@686] - binding to port /127.0.0.1:0
2023-07-30 13:49:43,259 - INFO  [main:FileTxnSnapLog@404] - Snapshotting: 0x0 to /tmp/kafka-434762771172158815/version-2/snapshot.0
2023-07-30 13:49:43,260 - INFO  [main:FileTxnSnapLog@404] - Snapshotting: 0x0 to /tmp/kafka-434762771172158815/version-2/snapshot.0
2023-07-30 13:49:43,262 - INFO  [main:AbstractConfig@361] - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.7-IV2
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/junit7660765717748375067/junit866559250785637651
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.7-IV2
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:40297
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

2023-07-30 13:49:43,263 - INFO  [main:Logging@66] - starting
2023-07-30 13:49:43,263 - INFO  [main:Logging@66] - Connecting to zookeeper on 127.0.0.1:40297
2023-07-30 13:49:43,264 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:40297.
2023-07-30 13:49:43,264 - INFO  [main:ZooKeeper@868] - Initiating client connection, connectString=127.0.0.1:40297 sessionTimeout=18000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@3ae126d1
2023-07-30 13:49:43,264 - INFO  [main:ClientCnxnSocket@237] - jute.maxbuffer value is 4194304 Bytes
2023-07-30 13:49:43,264 - INFO  [main:ClientCnxn@1653] - zookeeper.request.timeout value is 0. feature enabled=
2023-07-30 13:49:43,265 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Waiting until connected.
2023-07-30 13:49:43,265 - INFO  [main-SendThread(127.0.0.1:40297):ClientCnxn$SendThread@1112] - Opening socket connection to server localhost/127.0.0.1:40297. Will not attempt to authenticate using SASL (unknown error)
2023-07-30 13:49:43,266 - INFO  [main-SendThread(127.0.0.1:40297):ClientCnxn$SendThread@959] - Socket connection established, initiating session, client: /127.0.0.1:35372, server: localhost/127.0.0.1:40297
2023-07-30 13:49:43,267 - INFO  [SyncThread:0:FileTxnLog@218] - Creating new log file: log.1
2023-07-30 13:49:43,268 - INFO  [main-SendThread(127.0.0.1:40297):ClientCnxn$SendThread@1394] - Session establishment complete on server localhost/127.0.0.1:40297, sessionid = 0x1087f31a64a0000, negotiated timeout = 16000
2023-07-30 13:49:43,268 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Connected.
2023-07-30 13:49:43,295 - INFO  [feature-zk-node-event-process-thread:Logging@66] - [feature-zk-node-event-process-thread]: Starting
2023-07-30 13:49:43,296 - INFO  [feature-zk-node-event-process-thread:Logging@66] - Feature ZK node at path: /feature does not exist
2023-07-30 13:49:43,296 - INFO  [feature-zk-node-event-process-thread:FinalizedFeatureCache$@42] - Cleared cache
2023-07-30 13:49:43,299 - INFO  [main:Logging@66] - Cluster ID = l67hsn_BQke0DtmYXtLNog
2023-07-30 13:49:43,299 - WARN  [main:Logging@70] - No meta.properties file under dir /tmp/junit7660765717748375067/junit866559250785637651/meta.properties
2023-07-30 13:49:43,302 - INFO  [main:AbstractConfig@361] - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.7-IV2
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/junit7660765717748375067/junit866559250785637651
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.7-IV2
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:40297
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

2023-07-30 13:49:43,304 - INFO  [main:AbstractConfig@361] - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.7-IV2
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/junit7660765717748375067/junit866559250785637651
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.7-IV2
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:40297
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

2023-07-30 13:49:43,327 - INFO  [ThrottledChannelReaper-Fetch:Logging@66] - [ThrottledChannelReaper-Fetch]: Starting
2023-07-30 13:49:43,327 - INFO  [ThrottledChannelReaper-Produce:Logging@66] - [ThrottledChannelReaper-Produce]: Starting
2023-07-30 13:49:43,328 - INFO  [ThrottledChannelReaper-Request:Logging@66] - [ThrottledChannelReaper-Request]: Starting
2023-07-30 13:49:43,328 - INFO  [ThrottledChannelReaper-ControllerMutation:Logging@66] - [ThrottledChannelReaper-ControllerMutation]: Starting
2023-07-30 13:49:43,330 - INFO  [main:Logging@66] - Loading logs from log dirs ArraySeq(/tmp/junit7660765717748375067/junit866559250785637651)
2023-07-30 13:49:43,330 - INFO  [main:Logging@66] - Attempting recovery for all logs in /tmp/junit7660765717748375067/junit866559250785637651 since no clean shutdown file was found
2023-07-30 13:49:43,330 - INFO  [main:Logging@66] - Loaded 0 logs in 0ms.
2023-07-30 13:49:43,331 - INFO  [main:Logging@66] - Starting log cleanup with a period of 300000 ms.
2023-07-30 13:49:43,331 - INFO  [main:Logging@66] - Starting log flusher with a default period of 9223372036854775807 ms.
2023-07-30 13:49:43,371 - INFO  [main:Logging@66] - Created ConnectionAcceptRate sensor, quotaLimit=2147483647
2023-07-30 13:49:43,372 - INFO  [main:Logging@66] - Created ConnectionAcceptRate-PLAINTEXT sensor, quotaLimit=2147483647
2023-07-30 13:49:43,372 - INFO  [main:Logging@66] - Updated PLAINTEXT max connection creation rate to 2147483647
2023-07-30 13:49:43,373 - INFO  [main:Logging@66] - Awaiting socket connections on localhost:45395.
2023-07-30 13:49:43,375 - INFO  [main:Logging@66] - [SocketServer brokerId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT)
2023-07-30 13:49:43,377 - INFO  [ExpirationReaper-0-Produce:Logging@66] - [ExpirationReaper-0-Produce]: Starting
2023-07-30 13:49:43,377 - INFO  [ExpirationReaper-0-Fetch:Logging@66] - [ExpirationReaper-0-Fetch]: Starting
2023-07-30 13:49:43,378 - INFO  [ExpirationReaper-0-DeleteRecords:Logging@66] - [ExpirationReaper-0-DeleteRecords]: Starting
2023-07-30 13:49:43,378 - INFO  [ExpirationReaper-0-ElectLeader:Logging@66] - [ExpirationReaper-0-ElectLeader]: Starting
2023-07-30 13:49:43,379 - INFO  [LogDirFailureHandler:Logging@66] - [LogDirFailureHandler]: Starting
2023-07-30 13:49:43,379 - INFO  [broker-0-to-controller-send-thread:Logging@66] - [broker-0-to-controller-send-thread]: Starting
2023-07-30 13:49:43,380 - INFO  [main:Logging@66] - Creating /brokers/ids/0 (is it secure? false)
2023-07-30 13:49:43,382 - INFO  [main:Logging@66] - Stat of the created znode at /brokers/ids/0 is: 24,24,1690739383381,1690739383381,1,0,0,74449245071605760,204,0,24

2023-07-30 13:49:43,382 - INFO  [main:Logging@66] - Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://localhost:45395, czxid (broker epoch): 24
2023-07-30 13:49:43,385 - INFO  [ExpirationReaper-0-topic:Logging@66] - [ExpirationReaper-0-topic]: Starting
2023-07-30 13:49:43,386 - INFO  [ExpirationReaper-0-Heartbeat:Logging@66] - [ExpirationReaper-0-Heartbeat]: Starting
2023-07-30 13:49:43,386 - INFO  [ExpirationReaper-0-Rebalance:Logging@66] - [ExpirationReaper-0-Rebalance]: Starting
2023-07-30 13:49:43,387 - INFO  [main:Logging@66] - [GroupCoordinator 0]: Starting up.
2023-07-30 13:49:43,387 - INFO  [main:Logging@66] - [GroupCoordinator 0]: Startup complete.
2023-07-30 13:49:43,387 - INFO  [controller-event-thread:Logging@66] - Successfully created /controller_epoch with initial epoch 0
2023-07-30 13:49:43,389 - INFO  [main:Logging@66] - [ProducerId Manager 0]: Acquired new producerId block (brokerId:0,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1
2023-07-30 13:49:43,390 - INFO  [main:Logging@66] - [TransactionCoordinator id=0] Starting up.
2023-07-30 13:49:43,390 - INFO  [main:Logging@66] - [TransactionCoordinator id=0] Startup complete.
2023-07-30 13:49:43,391 - INFO  [TxnMarkerSenderThread-0:Logging@66] - [Transaction Marker Channel Manager 0]: Starting
2023-07-30 13:49:43,392 - INFO  [ExpirationReaper-0-AlterAcls:Logging@66] - [ExpirationReaper-0-AlterAcls]: Starting
2023-07-30 13:49:43,395 - INFO  [/config/changes-event-process-thread:Logging@66] - [/config/changes-event-process-thread]: Starting
2023-07-30 13:49:43,397 - INFO  [main:Logging@66] - [SocketServer brokerId=0] Starting socket server acceptors and processors
2023-07-30 13:49:43,399 - INFO  [main:Logging@66] - [SocketServer brokerId=0] Started data-plane acceptor and processor(s) for endpoint : ListenerName(PLAINTEXT)
2023-07-30 13:49:43,399 - INFO  [main:Logging@66] - [SocketServer brokerId=0] Started socket server acceptors and processors
2023-07-30 13:49:43,400 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:43,400 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:43,400 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739378724
2023-07-30 13:49:43,400 - WARN  [main:AppInfoParser@68] - Error registering AppInfo mbean
javax.management.InstanceAlreadyExistsException: kafka.server:type=app-info,id=0
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:64)
	at kafka.server.KafkaServer.startup(KafkaServer.scala:389)
	at kafka.utils.TestUtils$.createServer(TestUtils.scala:160)
	at kafka.utils.TestUtils$.createServer(TestUtils.scala:151)
	at kafka.utils.TestUtils.createServer(TestUtils.scala)
	at org.apache.kafka.connect.util.clusters.EmbeddedKafkaCluster.start(EmbeddedKafkaCluster.java:156)
	at org.apache.kafka.connect.util.clusters.EmbeddedKafkaCluster.start(EmbeddedKafkaCluster.java:134)
	at org.apache.kafka.connect.util.clusters.EmbeddedKafkaCluster.before(EmbeddedKafkaCluster.java:111)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.start(EmbeddedConnectCluster.java:136)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
2023-07-30 13:49:43,401 - INFO  [main:Logging@66] - [KafkaServer id=0] started
2023-07-30 13:49:43,401 - INFO  [main:AbstractConfig@361] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:45395]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-11
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:49:43,405 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:43,405 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:43,405 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739383404
2023-07-30 13:49:43,406 - INFO  [main:EmbeddedConnectCluster@235] - Starting Connect cluster 'backup-connect-cluster' with 3 workers
2023-07-30 13:49:43,406 - INFO  [main:ConnectDistributed@90] - Scanning for plugin classes. This might take a moment ...
2023-07-30 13:49:43,407 - INFO  [kafka-producer-network-thread | producer-11:Metadata@279] - [Producer clientId=producer-11] Cluster ID: l67hsn_BQke0DtmYXtLNog
2023-07-30 13:49:43,480 - INFO  [broker-0-to-controller-send-thread:Logging@66] - [broker-0-to-controller-send-thread]: Recorded new controller, from now on will use broker 0
2023-07-30 13:49:43,491 - INFO  [main:Reflections@239] - Reflections took 82 ms to scan 1 urls, producing 349 keys and 1055 values [using 20 cores]
2023-07-30 13:49:43,493 - WARN  [main:ReflectionUtils@318] - could not get type for name kotlin.jvm.internal.Lambda from any class loader
org.reflections.ReflectionsException: could not get type for name kotlin.jvm.internal.Lambda
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:91)
	at org.apache.kafka.connect.util.clusters.WorkerHandle.start(WorkerHandle.java:50)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.addWorker(EmbeddedConnectCluster.java:167)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.startConnect(EmbeddedConnectCluster.java:253)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.start(EmbeddedConnectCluster.java:137)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: kotlin.jvm.internal.Lambda
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 68 more
2023-07-30 13:49:43,495 - WARN  [main:ReflectionUtils@318] - could not get type for name kotlin.jvm.functions.Function0 from any class loader
org.reflections.ReflectionsException: could not get type for name kotlin.jvm.functions.Function0
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:91)
	at org.apache.kafka.connect.util.clusters.WorkerHandle.start(WorkerHandle.java:50)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.addWorker(EmbeddedConnectCluster.java:167)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.startConnect(EmbeddedConnectCluster.java:253)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.start(EmbeddedConnectCluster.java:137)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: kotlin.jvm.functions.Function0
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 68 more
2023-07-30 13:49:43,496 - WARN  [main:ReflectionUtils@318] - could not get type for name java.util.spi.ToolProvider from any class loader
org.reflections.ReflectionsException: could not get type for name java.util.spi.ToolProvider
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:91)
	at org.apache.kafka.connect.util.clusters.WorkerHandle.start(WorkerHandle.java:50)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.addWorker(EmbeddedConnectCluster.java:167)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.startConnect(EmbeddedConnectCluster.java:253)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.start(EmbeddedConnectCluster.java:137)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: java.util.spi.ToolProvider
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 68 more
2023-07-30 13:49:43,499 - INFO  [main:DelegatingClassLoader@269] - Registered loader: java.net.FactoryURLClassLoader@483bf400
2023-07-30 13:49:43,499 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension'
2023-07-30 13:49:43,499 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.runtime.isolation.PluginsTest$TestConnectRestExtension'
2023-07-30 13:49:43,499 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy'
2023-07-30 13:49:43,499 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy'
2023-07-30 13:49:43,500 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy'
2023-07-30 13:49:43,500 - INFO  [main:DelegatingClassLoader@427] - Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension'
2023-07-30 13:49:43,500 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'TestConnectRestExtension' and 'Test' to plugin 'org.apache.kafka.connect.runtime.isolation.PluginsTest$TestConnectRestExtension'
2023-07-30 13:49:43,500 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy'
2023-07-30 13:49:43,500 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy'
2023-07-30 13:49:43,501 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy'
2023-07-30 13:49:43,501 - INFO  [main:AbstractConfig@361] - DistributedConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:45395]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	config.providers = []
	config.storage.replication.factor = 1
	config.storage.topic = mm2-configs.primary.internal
	connect.protocol = sessioned
	connections.max.idle.ms = 540000
	connector.client.config.override.policy = None
	group.id = primary-mm2
	header.converter = class org.apache.kafka.connect.converters.ByteArrayConverter
	heartbeat.interval.ms = 3000
	inter.worker.key.generation.algorithm = HmacSHA256
	inter.worker.key.size = null
	inter.worker.key.ttl.ms = 3600000
	inter.worker.signature.algorithm = HmacSHA256
	inter.worker.verification.algorithms = [HmacSHA256]
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.converters.ByteArrayConverter
	listeners = null
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 60000
	offset.flush.timeout.ms = 5000
	offset.storage.partitions = 25
	offset.storage.replication.factor = 1
	offset.storage.topic = mm2-offsets.primary.internal
	plugin.path = null
	rebalance.timeout.ms = 60000
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 40000
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = localhost
	rest.port = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	scheduled.rebalance.max.delay.ms = 300000
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	status.storage.partitions = 5
	status.storage.replication.factor = 1
	status.storage.topic = mm2-status.primary.internal
	task.shutdown.graceful.timeout.ms = 5000
	topic.creation.enable = true
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.converters.ByteArrayConverter
	worker.sync.timeout.ms = 3000
	worker.unsync.backoff.ms = 300000

2023-07-30 13:49:43,501 - WARN  [main:WorkerConfig@420] - Variables cannot be used in the 'plugin.path' property, since the property is used by plugin scanning before the config providers that replace the variables are initialized. The raw value 'null' was used for plugin scanning, as opposed to the transformed value 'null', and this may cause unexpected results.
2023-07-30 13:49:43,502 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:49:43,502 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:45395]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:43,503 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,503 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:43,503 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,503 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:43,503 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,503 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,504 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:43,504 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,504 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,504 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,504 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,504 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,504 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:43,504 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:43,504 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739383504
2023-07-30 13:49:43,510 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: l67hsn_BQke0DtmYXtLNog
2023-07-30 13:49:43,510 - INFO  [kafka-admin-client-thread | adminclient-31:AppInfoParser@83] - App info kafka.admin.client for adminclient-31 unregistered
2023-07-30 13:49:43,511 - INFO  [kafka-admin-client-thread | adminclient-31:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:43,511 - INFO  [kafka-admin-client-thread | adminclient-31:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:43,511 - INFO  [kafka-admin-client-thread | adminclient-31:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:43,511 - INFO  [main:RestServer@132] - Added connector for http://localhost:0
2023-07-30 13:49:43,511 - INFO  [main:RestServer@204] - Initializing REST server
2023-07-30 13:49:43,511 - INFO  [main:Server@360] - jetty-9.4.30.v20200611; built: 2020-06-11T12:34:51.929Z; git: 271836e4c1f4612f12b7bb13ef5a92a927634b0d; jvm 1.8.0_275-8u275-b01-0ubuntu1~18.04-b01
2023-07-30 13:49:43,519 - INFO  [main:AbstractConnector@331] - Started http_localhost0@630b6190{HTTP/1.1, (http/1.1)}{localhost:43163}
2023-07-30 13:49:43,519 - INFO  [main:Server@400] - Started @5777ms
2023-07-30 13:49:43,519 - INFO  [main:RestServer@371] - Advertised URI: http://localhost:43163/
2023-07-30 13:49:43,519 - INFO  [main:RestServer@219] - REST server listening at http://localhost:43163/, advertising URL http://localhost:43163/
2023-07-30 13:49:43,519 - INFO  [main:RestServer@371] - Advertised URI: http://localhost:43163/
2023-07-30 13:49:43,519 - INFO  [main:RestServer@220] - REST admin endpoints at http://localhost:43163/
2023-07-30 13:49:43,519 - INFO  [main:RestServer@371] - Advertised URI: http://localhost:43163/
2023-07-30 13:49:43,520 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:49:43,520 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:45395]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:43,521 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,521 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:43,521 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,521 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:43,521 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,521 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,521 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:43,521 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,521 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,521 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,521 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,521 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,522 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:43,522 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:43,522 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739383522
2023-07-30 13:49:43,526 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: l67hsn_BQke0DtmYXtLNog
2023-07-30 13:49:43,526 - INFO  [kafka-admin-client-thread | adminclient-32:AppInfoParser@83] - App info kafka.admin.client for adminclient-32 unregistered
2023-07-30 13:49:43,527 - INFO  [kafka-admin-client-thread | adminclient-32:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:43,527 - INFO  [kafka-admin-client-thread | adminclient-32:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:43,527 - INFO  [kafka-admin-client-thread | adminclient-32:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:43,527 - INFO  [main:NoneConnectorClientConfigOverridePolicy@45] - Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden
2023-07-30 13:49:43,527 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:49:43,528 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:45395]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:43,529 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,529 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:43,529 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,529 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:43,529 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,530 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,530 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:43,530 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,530 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,530 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,530 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,530 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,530 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:43,531 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:43,531 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739383530
2023-07-30 13:49:43,535 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: l67hsn_BQke0DtmYXtLNog
2023-07-30 13:49:43,536 - INFO  [kafka-admin-client-thread | adminclient-33:AppInfoParser@83] - App info kafka.admin.client for adminclient-33 unregistered
2023-07-30 13:49:43,536 - INFO  [kafka-admin-client-thread | adminclient-33:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:43,536 - INFO  [kafka-admin-client-thread | adminclient-33:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:43,536 - INFO  [kafka-admin-client-thread | adminclient-33:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:43,537 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:43,537 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:43,537 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739383537
2023-07-30 13:49:43,538 - INFO  [main:AbstractConfig@361] - JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false

2023-07-30 13:49:43,538 - INFO  [main:AbstractConfig@361] - JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false

2023-07-30 13:49:43,538 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:49:43,538 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:45395]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:43,539 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,539 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:43,539 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,539 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:43,539 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,539 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,540 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:43,540 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,540 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,540 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,540 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,540 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,540 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:43,540 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:43,540 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739383540
2023-07-30 13:49:43,546 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: l67hsn_BQke0DtmYXtLNog
2023-07-30 13:49:43,546 - INFO  [kafka-admin-client-thread | adminclient-34:AppInfoParser@83] - App info kafka.admin.client for adminclient-34 unregistered
2023-07-30 13:49:43,547 - INFO  [kafka-admin-client-thread | adminclient-34:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:43,547 - INFO  [kafka-admin-client-thread | adminclient-34:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:43,547 - INFO  [kafka-admin-client-thread | adminclient-34:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:43,547 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:49:43,547 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:45395]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:43,548 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,548 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:43,548 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,548 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:43,548 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,549 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,549 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:43,549 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,549 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,549 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,549 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,549 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,549 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:43,549 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:43,549 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739383549
2023-07-30 13:49:43,554 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: l67hsn_BQke0DtmYXtLNog
2023-07-30 13:49:43,554 - INFO  [kafka-admin-client-thread | adminclient-35:AppInfoParser@83] - App info kafka.admin.client for adminclient-35 unregistered
2023-07-30 13:49:43,555 - INFO  [kafka-admin-client-thread | adminclient-35:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:43,555 - INFO  [kafka-admin-client-thread | adminclient-35:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:43,555 - INFO  [kafka-admin-client-thread | adminclient-35:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:43,555 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:49:43,556 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:45395]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:43,556 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,556 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:43,556 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,557 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:43,557 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,557 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,557 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:43,557 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,557 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,557 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,557 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,557 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,557 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:43,557 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:43,557 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739383557
2023-07-30 13:49:43,561 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: l67hsn_BQke0DtmYXtLNog
2023-07-30 13:49:43,562 - INFO  [kafka-admin-client-thread | adminclient-36:AppInfoParser@83] - App info kafka.admin.client for adminclient-36 unregistered
2023-07-30 13:49:43,562 - INFO  [kafka-admin-client-thread | adminclient-36:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:43,562 - INFO  [kafka-admin-client-thread | adminclient-36:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:43,562 - INFO  [kafka-admin-client-thread | adminclient-36:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:43,563 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:49:43,563 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:45395]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:43,564 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,564 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:43,564 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,564 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:43,564 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,564 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,564 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:43,564 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,565 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,565 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,565 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,565 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,565 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:43,565 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:43,565 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739383565
2023-07-30 13:49:43,569 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: l67hsn_BQke0DtmYXtLNog
2023-07-30 13:49:43,570 - INFO  [kafka-admin-client-thread | adminclient-37:AppInfoParser@83] - App info kafka.admin.client for adminclient-37 unregistered
2023-07-30 13:49:43,571 - INFO  [kafka-admin-client-thread | adminclient-37:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:43,571 - INFO  [kafka-admin-client-thread | adminclient-37:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:43,571 - INFO  [kafka-admin-client-thread | adminclient-37:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:43,572 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:43,572 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:43,572 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739383571
2023-07-30 13:49:43,572 - INFO  [main:ConnectDistributed@128] - Kafka Connect distributed worker initialization took 166ms
2023-07-30 13:49:43,572 - INFO  [main:Connect@51] - Kafka Connect starting
2023-07-30 13:49:43,572 - INFO  [main:RestServer@224] - Initializing REST resources
2023-07-30 13:49:43,572 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@286] - [Worker clientId=connect-4, groupId=primary-mm2] Herder starting
2023-07-30 13:49:43,573 - INFO  [DistributedHerder-connect-4-1:Worker@195] - Worker starting
2023-07-30 13:49:43,573 - INFO  [DistributedHerder-connect-4-1:KafkaOffsetBackingStore@127] - Starting KafkaOffsetBackingStore
2023-07-30 13:49:43,573 - INFO  [DistributedHerder-connect-4-1:KafkaBasedLog@127] - Starting KafkaBasedLog with topic mm2-offsets.primary.internal
2023-07-30 13:49:43,573 - INFO  [main:RestServer@241] - Adding admin resources to main listener
2023-07-30 13:49:43,573 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:45395]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:43,573 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,574 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:49:43,574 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:43,574 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,574 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:43,574 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,574 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,574 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:43,574 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:43,574 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,574 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,574 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,574 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,574 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,574 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:43,574 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:43,574 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739383574
2023-07-30 13:49:43,575 - INFO  [main:DefaultSessionIdManager@334] - DefaultSessionIdManager workerName=node0
2023-07-30 13:49:43,576 - INFO  [main:DefaultSessionIdManager@339] - No SessionScavenger set, using defaults
2023-07-30 13:49:43,576 - INFO  [main:HouseKeeper@140] - node0 Scavenging every 660000ms
2023-07-30 13:49:43,581 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Creating topic mm2-offsets.primary.internal with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0), 5 -> ArrayBuffer(0), 6 -> ArrayBuffer(0), 7 -> ArrayBuffer(0), 8 -> ArrayBuffer(0), 9 -> ArrayBuffer(0), 10 -> ArrayBuffer(0), 11 -> ArrayBuffer(0), 12 -> ArrayBuffer(0), 13 -> ArrayBuffer(0), 14 -> ArrayBuffer(0), 15 -> ArrayBuffer(0), 16 -> ArrayBuffer(0), 17 -> ArrayBuffer(0), 18 -> ArrayBuffer(0), 19 -> ArrayBuffer(0), 20 -> ArrayBuffer(0), 21 -> ArrayBuffer(0), 22 -> ArrayBuffer(0), 23 -> ArrayBuffer(0), 24 -> ArrayBuffer(0))
2023-07-30 13:49:43,612 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(mm2-offsets.primary.internal-4, mm2-offsets.primary.internal-22, mm2-offsets.primary.internal-8, mm2-offsets.primary.internal-9, mm2-offsets.primary.internal-16, mm2-offsets.primary.internal-24, mm2-offsets.primary.internal-19, mm2-offsets.primary.internal-21, mm2-offsets.primary.internal-15, mm2-offsets.primary.internal-3, mm2-offsets.primary.internal-12, mm2-offsets.primary.internal-5, mm2-offsets.primary.internal-0, mm2-offsets.primary.internal-11, mm2-offsets.primary.internal-23, mm2-offsets.primary.internal-6, mm2-offsets.primary.internal-10, mm2-offsets.primary.internal-17, mm2-offsets.primary.internal-13, mm2-offsets.primary.internal-1, mm2-offsets.primary.internal-14, mm2-offsets.primary.internal-2, mm2-offsets.primary.internal-18, mm2-offsets.primary.internal-20, mm2-offsets.primary.internal-7)
2023-07-30 13:49:43,615 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-17, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:43,617 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-17 in /tmp/junit7660765717748375067/junit866559250785637651/mm2-offsets.primary.internal-17 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:43,617 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-17 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-17
2023-07-30 13:49:43,617 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-17 broker=0] Log loaded for partition mm2-offsets.primary.internal-17 with initial high watermark 0
2023-07-30 13:49:43,620 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-2, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:43,621 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-2 in /tmp/junit7660765717748375067/junit866559250785637651/mm2-offsets.primary.internal-2 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:43,621 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-2 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-2
2023-07-30 13:49:43,621 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-2 broker=0] Log loaded for partition mm2-offsets.primary.internal-2 with initial high watermark 0
Jul 30, 2023 1:49:43 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource will be ignored. 
Jul 30, 2023 1:49:43 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource will be ignored. 
Jul 30, 2023 1:49:43 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.RootResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.RootResource will be ignored. 
Jul 30, 2023 1:49:43 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource will be ignored. 
2023-07-30 13:49:43,628 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-21, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:43,629 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-21 in /tmp/junit7660765717748375067/junit866559250785637651/mm2-offsets.primary.internal-21 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:43,629 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-21 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-21
2023-07-30 13:49:43,629 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-21 broker=0] Log loaded for partition mm2-offsets.primary.internal-21 with initial high watermark 0
2023-07-30 13:49:43,636 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-6, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:43,637 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-6 in /tmp/junit7660765717748375067/junit866559250785637651/mm2-offsets.primary.internal-6 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:43,637 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-6 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-6
2023-07-30 13:49:43,639 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-6 broker=0] Log loaded for partition mm2-offsets.primary.internal-6 with initial high watermark 0
2023-07-30 13:49:43,645 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-10, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:43,646 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-10 in /tmp/junit7660765717748375067/junit866559250785637651/mm2-offsets.primary.internal-10 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:43,646 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-10 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-10
2023-07-30 13:49:43,646 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-10 broker=0] Log loaded for partition mm2-offsets.primary.internal-10 with initial high watermark 0
Jul 30, 2023 1:49:43 PM org.glassfish.jersey.internal.Errors logErrors
WARNING: The following warnings have been detected: WARNING: The (sub)resource method listLoggers in org.apache.kafka.connect.runtime.rest.resources.LoggingResource contains empty path annotation.
WARNING: The (sub)resource method createConnector in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectors in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectorPlugins in org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource contains empty path annotation.
WARNING: The (sub)resource method serverInfo in org.apache.kafka.connect.runtime.rest.resources.RootResource contains empty path annotation.

2023-07-30 13:49:43,652 - INFO  [main:ContextHandler@849] - Started o.e.j.s.ServletContextHandler@4667c4c1{/,null,AVAILABLE}
2023-07-30 13:49:43,652 - INFO  [main:RestServer@319] - REST resources initialized; server is started and ready to handle requests
2023-07-30 13:49:43,652 - INFO  [main:Connect@57] - Kafka Connect started
2023-07-30 13:49:43,652 - INFO  [main:EmbeddedConnectCluster@169] - Started worker WorkerHandle{workerName='connect-worker-0'workerURL='http://localhost:43163/'}
2023-07-30 13:49:43,652 - INFO  [main:ConnectDistributed@90] - Scanning for plugin classes. This might take a moment ...
2023-07-30 13:49:43,653 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-14, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:43,654 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-14 in /tmp/junit7660765717748375067/junit866559250785637651/mm2-offsets.primary.internal-14 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:43,654 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-14 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-14
2023-07-30 13:49:43,654 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-14 broker=0] Log loaded for partition mm2-offsets.primary.internal-14 with initial high watermark 0
2023-07-30 13:49:43,661 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-16, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:43,662 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-16 in /tmp/junit7660765717748375067/junit866559250785637651/mm2-offsets.primary.internal-16 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:43,662 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-16 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-16
2023-07-30 13:49:43,662 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-16 broker=0] Log loaded for partition mm2-offsets.primary.internal-16 with initial high watermark 0
2023-07-30 13:49:43,670 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-1, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:43,671 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-1 in /tmp/junit7660765717748375067/junit866559250785637651/mm2-offsets.primary.internal-1 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:43,671 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-1 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-1
2023-07-30 13:49:43,671 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-1 broker=0] Log loaded for partition mm2-offsets.primary.internal-1 with initial high watermark 0
2023-07-30 13:49:43,678 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-20, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:43,678 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-20 in /tmp/junit7660765717748375067/junit866559250785637651/mm2-offsets.primary.internal-20 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:43,678 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-20 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-20
2023-07-30 13:49:43,679 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-20 broker=0] Log loaded for partition mm2-offsets.primary.internal-20 with initial high watermark 0
2023-07-30 13:49:43,686 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-5, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:43,687 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-5 in /tmp/junit7660765717748375067/junit866559250785637651/mm2-offsets.primary.internal-5 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:43,687 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-5 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-5
2023-07-30 13:49:43,687 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-5 broker=0] Log loaded for partition mm2-offsets.primary.internal-5 with initial high watermark 0
2023-07-30 13:49:43,695 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-24, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:43,695 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-24 in /tmp/junit7660765717748375067/junit866559250785637651/mm2-offsets.primary.internal-24 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:43,695 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-24 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-24
2023-07-30 13:49:43,695 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-24 broker=0] Log loaded for partition mm2-offsets.primary.internal-24 with initial high watermark 0
2023-07-30 13:49:43,703 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-9, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:43,704 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-9 in /tmp/junit7660765717748375067/junit866559250785637651/mm2-offsets.primary.internal-9 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:43,704 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-9 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-9
2023-07-30 13:49:43,704 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-9 broker=0] Log loaded for partition mm2-offsets.primary.internal-9 with initial high watermark 0
2023-07-30 13:49:43,711 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-13, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:43,712 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-13 in /tmp/junit7660765717748375067/junit866559250785637651/mm2-offsets.primary.internal-13 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:43,712 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-13 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-13
2023-07-30 13:49:43,712 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-13 broker=0] Log loaded for partition mm2-offsets.primary.internal-13 with initial high watermark 0
2023-07-30 13:49:43,719 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-15, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:43,720 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-15 in /tmp/junit7660765717748375067/junit866559250785637651/mm2-offsets.primary.internal-15 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:43,720 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-15 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-15
2023-07-30 13:49:43,720 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-15 broker=0] Log loaded for partition mm2-offsets.primary.internal-15 with initial high watermark 0
2023-07-30 13:49:43,728 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-0, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:43,728 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-0 in /tmp/junit7660765717748375067/junit866559250785637651/mm2-offsets.primary.internal-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:43,728 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-0 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-0
2023-07-30 13:49:43,728 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-0 broker=0] Log loaded for partition mm2-offsets.primary.internal-0 with initial high watermark 0
2023-07-30 13:49:43,731 - INFO  [main:Reflections@239] - Reflections took 77 ms to scan 1 urls, producing 349 keys and 1055 values [using 20 cores]
2023-07-30 13:49:43,732 - WARN  [main:ReflectionUtils@318] - could not get type for name kotlin.jvm.internal.Lambda from any class loader
org.reflections.ReflectionsException: could not get type for name kotlin.jvm.internal.Lambda
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:91)
	at org.apache.kafka.connect.util.clusters.WorkerHandle.start(WorkerHandle.java:50)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.addWorker(EmbeddedConnectCluster.java:167)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.startConnect(EmbeddedConnectCluster.java:253)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.start(EmbeddedConnectCluster.java:137)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: kotlin.jvm.internal.Lambda
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 68 more
2023-07-30 13:49:43,734 - WARN  [main:ReflectionUtils@318] - could not get type for name kotlin.jvm.functions.Function0 from any class loader
org.reflections.ReflectionsException: could not get type for name kotlin.jvm.functions.Function0
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:91)
	at org.apache.kafka.connect.util.clusters.WorkerHandle.start(WorkerHandle.java:50)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.addWorker(EmbeddedConnectCluster.java:167)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.startConnect(EmbeddedConnectCluster.java:253)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.start(EmbeddedConnectCluster.java:137)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: kotlin.jvm.functions.Function0
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 68 more
2023-07-30 13:49:43,735 - WARN  [main:ReflectionUtils@318] - could not get type for name java.util.spi.ToolProvider from any class loader
org.reflections.ReflectionsException: could not get type for name java.util.spi.ToolProvider
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:91)
	at org.apache.kafka.connect.util.clusters.WorkerHandle.start(WorkerHandle.java:50)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.addWorker(EmbeddedConnectCluster.java:167)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.startConnect(EmbeddedConnectCluster.java:253)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.start(EmbeddedConnectCluster.java:137)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: java.util.spi.ToolProvider
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 68 more
2023-07-30 13:49:43,736 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-19, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:43,737 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-19 in /tmp/junit7660765717748375067/junit866559250785637651/mm2-offsets.primary.internal-19 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:43,737 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-19 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-19
2023-07-30 13:49:43,737 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-19 broker=0] Log loaded for partition mm2-offsets.primary.internal-19 with initial high watermark 0
2023-07-30 13:49:43,738 - INFO  [main:DelegatingClassLoader@269] - Registered loader: java.net.FactoryURLClassLoader@483bf400
2023-07-30 13:49:43,738 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension'
2023-07-30 13:49:43,738 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.runtime.isolation.PluginsTest$TestConnectRestExtension'
2023-07-30 13:49:43,738 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy'
2023-07-30 13:49:43,738 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy'
2023-07-30 13:49:43,738 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy'
2023-07-30 13:49:43,739 - INFO  [main:DelegatingClassLoader@427] - Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension'
2023-07-30 13:49:43,739 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'TestConnectRestExtension' and 'Test' to plugin 'org.apache.kafka.connect.runtime.isolation.PluginsTest$TestConnectRestExtension'
2023-07-30 13:49:43,739 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy'
2023-07-30 13:49:43,739 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy'
2023-07-30 13:49:43,739 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy'
2023-07-30 13:49:43,740 - INFO  [main:AbstractConfig@361] - DistributedConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:45395]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	config.providers = []
	config.storage.replication.factor = 1
	config.storage.topic = mm2-configs.primary.internal
	connect.protocol = sessioned
	connections.max.idle.ms = 540000
	connector.client.config.override.policy = None
	group.id = primary-mm2
	header.converter = class org.apache.kafka.connect.converters.ByteArrayConverter
	heartbeat.interval.ms = 3000
	inter.worker.key.generation.algorithm = HmacSHA256
	inter.worker.key.size = null
	inter.worker.key.ttl.ms = 3600000
	inter.worker.signature.algorithm = HmacSHA256
	inter.worker.verification.algorithms = [HmacSHA256]
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.converters.ByteArrayConverter
	listeners = null
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 60000
	offset.flush.timeout.ms = 5000
	offset.storage.partitions = 25
	offset.storage.replication.factor = 1
	offset.storage.topic = mm2-offsets.primary.internal
	plugin.path = null
	rebalance.timeout.ms = 60000
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 40000
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = localhost
	rest.port = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	scheduled.rebalance.max.delay.ms = 300000
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	status.storage.partitions = 5
	status.storage.replication.factor = 1
	status.storage.topic = mm2-status.primary.internal
	task.shutdown.graceful.timeout.ms = 5000
	topic.creation.enable = true
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.converters.ByteArrayConverter
	worker.sync.timeout.ms = 3000
	worker.unsync.backoff.ms = 300000

2023-07-30 13:49:43,740 - WARN  [main:WorkerConfig@420] - Variables cannot be used in the 'plugin.path' property, since the property is used by plugin scanning before the config providers that replace the variables are initialized. The raw value 'null' was used for plugin scanning, as opposed to the transformed value 'null', and this may cause unexpected results.
2023-07-30 13:49:43,740 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:49:43,740 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:45395]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:43,741 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,741 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:43,741 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,741 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:43,741 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,741 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,741 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:43,742 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,742 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,742 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,742 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,742 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,742 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:43,742 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:43,742 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739383742
2023-07-30 13:49:43,745 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-4, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:43,745 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-4 in /tmp/junit7660765717748375067/junit866559250785637651/mm2-offsets.primary.internal-4 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:43,745 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-4 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-4
2023-07-30 13:49:43,745 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-4 broker=0] Log loaded for partition mm2-offsets.primary.internal-4 with initial high watermark 0
2023-07-30 13:49:43,746 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: l67hsn_BQke0DtmYXtLNog
2023-07-30 13:49:43,746 - INFO  [kafka-admin-client-thread | adminclient-39:AppInfoParser@83] - App info kafka.admin.client for adminclient-39 unregistered
2023-07-30 13:49:43,747 - INFO  [kafka-admin-client-thread | adminclient-39:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:43,747 - INFO  [kafka-admin-client-thread | adminclient-39:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:43,747 - INFO  [kafka-admin-client-thread | adminclient-39:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:43,747 - INFO  [main:RestServer@132] - Added connector for http://localhost:0
2023-07-30 13:49:43,747 - INFO  [main:RestServer@204] - Initializing REST server
2023-07-30 13:49:43,747 - INFO  [main:Server@360] - jetty-9.4.30.v20200611; built: 2020-06-11T12:34:51.929Z; git: 271836e4c1f4612f12b7bb13ef5a92a927634b0d; jvm 1.8.0_275-8u275-b01-0ubuntu1~18.04-b01
2023-07-30 13:49:43,754 - INFO  [main:AbstractConnector@331] - Started http_localhost0@25d0cb3a{HTTP/1.1, (http/1.1)}{localhost:39987}
2023-07-30 13:49:43,754 - INFO  [main:Server@400] - Started @6012ms
2023-07-30 13:49:43,754 - INFO  [main:RestServer@371] - Advertised URI: http://localhost:39987/
2023-07-30 13:49:43,754 - INFO  [main:RestServer@219] - REST server listening at http://localhost:39987/, advertising URL http://localhost:39987/
2023-07-30 13:49:43,755 - INFO  [main:RestServer@371] - Advertised URI: http://localhost:39987/
2023-07-30 13:49:43,755 - INFO  [main:RestServer@220] - REST admin endpoints at http://localhost:39987/
2023-07-30 13:49:43,755 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-23, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:43,755 - INFO  [main:RestServer@371] - Advertised URI: http://localhost:39987/
2023-07-30 13:49:43,755 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:49:43,755 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:45395]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:43,755 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-23 in /tmp/junit7660765717748375067/junit866559250785637651/mm2-offsets.primary.internal-23 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:43,755 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-23 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-23
2023-07-30 13:49:43,755 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-23 broker=0] Log loaded for partition mm2-offsets.primary.internal-23 with initial high watermark 0
2023-07-30 13:49:43,756 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,756 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:43,756 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,756 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:43,756 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,756 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,756 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:43,756 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,757 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,757 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,757 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,757 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,757 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:43,757 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:43,757 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739383757
2023-07-30 13:49:43,761 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: l67hsn_BQke0DtmYXtLNog
2023-07-30 13:49:43,761 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-8, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:43,761 - INFO  [kafka-admin-client-thread | adminclient-40:AppInfoParser@83] - App info kafka.admin.client for adminclient-40 unregistered
2023-07-30 13:49:43,761 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-8 in /tmp/junit7660765717748375067/junit866559250785637651/mm2-offsets.primary.internal-8 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:43,761 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-8 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-8
2023-07-30 13:49:43,761 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-8 broker=0] Log loaded for partition mm2-offsets.primary.internal-8 with initial high watermark 0
2023-07-30 13:49:43,762 - INFO  [kafka-admin-client-thread | adminclient-40:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:43,762 - INFO  [kafka-admin-client-thread | adminclient-40:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:43,762 - INFO  [kafka-admin-client-thread | adminclient-40:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:43,762 - INFO  [main:NoneConnectorClientConfigOverridePolicy@45] - Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden
2023-07-30 13:49:43,762 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:49:43,762 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:45395]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:43,763 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,763 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:43,763 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,763 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:43,763 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,763 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,763 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:43,764 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,764 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,764 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,764 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,764 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,764 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:43,764 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:43,764 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739383764
2023-07-30 13:49:43,768 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: l67hsn_BQke0DtmYXtLNog
2023-07-30 13:49:43,768 - INFO  [kafka-admin-client-thread | adminclient-41:AppInfoParser@83] - App info kafka.admin.client for adminclient-41 unregistered
2023-07-30 13:49:43,769 - INFO  [kafka-admin-client-thread | adminclient-41:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:43,769 - INFO  [kafka-admin-client-thread | adminclient-41:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:43,769 - INFO  [kafka-admin-client-thread | adminclient-41:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:43,769 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:43,769 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:43,769 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-12, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:43,769 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739383769
2023-07-30 13:49:43,770 - INFO  [main:AbstractConfig@361] - JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false

2023-07-30 13:49:43,770 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-12 in /tmp/junit7660765717748375067/junit866559250785637651/mm2-offsets.primary.internal-12 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:43,770 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-12 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-12
2023-07-30 13:49:43,770 - INFO  [main:AbstractConfig@361] - JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false

2023-07-30 13:49:43,770 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-12 broker=0] Log loaded for partition mm2-offsets.primary.internal-12 with initial high watermark 0
2023-07-30 13:49:43,770 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:49:43,770 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:45395]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:43,771 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,771 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:43,771 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,771 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:43,771 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,771 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,772 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:43,772 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,772 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,772 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,772 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,772 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,772 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:43,772 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:43,772 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739383772
2023-07-30 13:49:43,777 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: l67hsn_BQke0DtmYXtLNog
2023-07-30 13:49:43,777 - INFO  [kafka-admin-client-thread | adminclient-42:AppInfoParser@83] - App info kafka.admin.client for adminclient-42 unregistered
2023-07-30 13:49:43,778 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-18, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:43,778 - INFO  [kafka-admin-client-thread | adminclient-42:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:43,778 - INFO  [kafka-admin-client-thread | adminclient-42:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:43,778 - INFO  [kafka-admin-client-thread | adminclient-42:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:43,778 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:49:43,778 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-18 in /tmp/junit7660765717748375067/junit866559250785637651/mm2-offsets.primary.internal-18 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:43,778 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:45395]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:43,779 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-18 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-18
2023-07-30 13:49:43,779 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-18 broker=0] Log loaded for partition mm2-offsets.primary.internal-18 with initial high watermark 0
2023-07-30 13:49:43,779 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,779 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:43,780 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,780 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:43,780 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,780 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,780 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:43,780 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,780 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,780 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,780 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,780 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,780 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:43,780 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:43,780 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739383780
2023-07-30 13:49:43,784 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: l67hsn_BQke0DtmYXtLNog
2023-07-30 13:49:43,784 - INFO  [kafka-admin-client-thread | adminclient-43:AppInfoParser@83] - App info kafka.admin.client for adminclient-43 unregistered
2023-07-30 13:49:43,785 - INFO  [kafka-admin-client-thread | adminclient-43:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:43,785 - INFO  [kafka-admin-client-thread | adminclient-43:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:43,785 - INFO  [kafka-admin-client-thread | adminclient-43:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:43,785 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:49:43,785 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:45395]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:43,786 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-3, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:43,786 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,786 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:43,787 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,787 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-3 in /tmp/junit7660765717748375067/junit866559250785637651/mm2-offsets.primary.internal-3 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:43,787 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:43,787 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,787 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,787 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:43,787 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,787 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,787 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,788 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,788 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,787 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-3 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-3
2023-07-30 13:49:43,788 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:43,788 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:43,788 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-3 broker=0] Log loaded for partition mm2-offsets.primary.internal-3 with initial high watermark 0
2023-07-30 13:49:43,788 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739383788
2023-07-30 13:49:43,791 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: l67hsn_BQke0DtmYXtLNog
2023-07-30 13:49:43,792 - INFO  [kafka-admin-client-thread | adminclient-44:AppInfoParser@83] - App info kafka.admin.client for adminclient-44 unregistered
2023-07-30 13:49:43,792 - INFO  [kafka-admin-client-thread | adminclient-44:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:43,792 - INFO  [kafka-admin-client-thread | adminclient-44:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:43,792 - INFO  [kafka-admin-client-thread | adminclient-44:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:43,793 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:49:43,793 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:45395]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:43,794 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,794 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:43,794 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,794 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:43,794 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,794 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,794 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:43,794 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,794 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,794 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,794 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,795 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,794 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-22, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:43,795 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:43,795 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:43,795 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739383795
2023-07-30 13:49:43,795 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-22 in /tmp/junit7660765717748375067/junit866559250785637651/mm2-offsets.primary.internal-22 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:43,795 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-22 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-22
2023-07-30 13:49:43,796 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-22 broker=0] Log loaded for partition mm2-offsets.primary.internal-22 with initial high watermark 0
2023-07-30 13:49:43,798 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: l67hsn_BQke0DtmYXtLNog
2023-07-30 13:49:43,799 - INFO  [kafka-admin-client-thread | adminclient-45:AppInfoParser@83] - App info kafka.admin.client for adminclient-45 unregistered
2023-07-30 13:49:43,799 - INFO  [kafka-admin-client-thread | adminclient-45:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:43,799 - INFO  [kafka-admin-client-thread | adminclient-45:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:43,799 - INFO  [kafka-admin-client-thread | adminclient-45:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:43,800 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:43,800 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:43,800 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739383800
2023-07-30 13:49:43,801 - INFO  [main:ConnectDistributed@128] - Kafka Connect distributed worker initialization took 148ms
2023-07-30 13:49:43,801 - INFO  [main:Connect@51] - Kafka Connect starting
2023-07-30 13:49:43,801 - INFO  [main:RestServer@224] - Initializing REST resources
2023-07-30 13:49:43,801 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@286] - [Worker clientId=connect-5, groupId=primary-mm2] Herder starting
2023-07-30 13:49:43,801 - INFO  [DistributedHerder-connect-5-1:Worker@195] - Worker starting
2023-07-30 13:49:43,801 - INFO  [DistributedHerder-connect-5-1:KafkaOffsetBackingStore@127] - Starting KafkaOffsetBackingStore
2023-07-30 13:49:43,801 - INFO  [DistributedHerder-connect-5-1:KafkaBasedLog@127] - Starting KafkaBasedLog with topic mm2-offsets.primary.internal
2023-07-30 13:49:43,801 - INFO  [main:RestServer@241] - Adding admin resources to main listener
2023-07-30 13:49:43,801 - INFO  [DistributedHerder-connect-5-1:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:45395]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:43,802 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,802 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:49:43,802 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:43,802 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,802 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:43,802 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,802 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,802 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:43,803 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:43,803 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,803 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,803 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,803 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,803 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,802 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-7, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:43,803 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:43,803 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:43,803 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739383803
2023-07-30 13:49:43,804 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-7 in /tmp/junit7660765717748375067/junit866559250785637651/mm2-offsets.primary.internal-7 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:43,804 - INFO  [main:DefaultSessionIdManager@334] - DefaultSessionIdManager workerName=node0
2023-07-30 13:49:43,804 - INFO  [main:DefaultSessionIdManager@339] - No SessionScavenger set, using defaults
2023-07-30 13:49:43,804 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-7 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-7
2023-07-30 13:49:43,804 - INFO  [main:HouseKeeper@140] - node0 Scavenging every 660000ms
2023-07-30 13:49:43,804 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-7 broker=0] Log loaded for partition mm2-offsets.primary.internal-7 with initial high watermark 0
2023-07-30 13:49:43,808 - INFO  [DistributedHerder-connect-5-1:TopicAdmin@400] - Unable to use admin client to verify the cleanup policy of 'mm2-offsets.primary.internal' topic is 'compact', either because the broker is an older version or because the Kafka principal used for Connect internal topics does not have the required permission to describe topic configurations.
2023-07-30 13:49:43,808 - INFO  [kafka-admin-client-thread | adminclient-46:AppInfoParser@83] - App info kafka.admin.client for adminclient-46 unregistered
2023-07-30 13:49:43,808 - INFO  [kafka-admin-client-thread | adminclient-46:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:43,808 - INFO  [kafka-admin-client-thread | adminclient-46:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:43,809 - INFO  [kafka-admin-client-thread | adminclient-46:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:43,809 - INFO  [DistributedHerder-connect-5-1:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:45395]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-12
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:49:43,810 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,811 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:49:43,811 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:43,811 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,811 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:43,811 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,811 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,811 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:43,812 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:43,812 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,812 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,812 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,812 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,812 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,812 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=mm2-offsets.primary.internal-11, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:43,812 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:43,812 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:43,812 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739383812
2023-07-30 13:49:43,813 - INFO  [kafka-producer-network-thread | producer-12:Metadata@279] - [Producer clientId=producer-12] Cluster ID: l67hsn_BQke0DtmYXtLNog
2023-07-30 13:49:43,813 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition mm2-offsets.primary.internal-11 in /tmp/junit7660765717748375067/junit866559250785637651/mm2-offsets.primary.internal-11 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:43,813 - INFO  [DistributedHerder-connect-5-1:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:45395]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-primary-mm2-10
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = primary-mm2
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:49:43,813 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-11 broker=0] No checkpointed highwatermark is found for partition mm2-offsets.primary.internal-11
2023-07-30 13:49:43,813 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition mm2-offsets.primary.internal-11 broker=0] Log loaded for partition mm2-offsets.primary.internal-11 with initial high watermark 0
2023-07-30 13:49:43,814 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,814 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:49:43,814 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,814 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:43,814 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,814 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,814 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:43,814 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:43,814 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,815 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,815 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,815 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,815 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,815 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:43,815 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:43,815 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739383815
2023-07-30 13:49:43,816 - INFO  [DistributedHerder-connect-5-1:Metadata@279] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Cluster ID: l67hsn_BQke0DtmYXtLNog
2023-07-30 13:49:43,822 - INFO  [DistributedHerder-connect-4-1:TopicAdmin@284] - Created topic (name=mm2-offsets.primary.internal, numPartitions=25, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at localhost:45395
2023-07-30 13:49:43,822 - INFO  [kafka-admin-client-thread | adminclient-38:AppInfoParser@83] - App info kafka.admin.client for adminclient-38 unregistered
2023-07-30 13:49:43,823 - INFO  [kafka-admin-client-thread | adminclient-38:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:43,823 - INFO  [kafka-admin-client-thread | adminclient-38:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:43,823 - INFO  [kafka-admin-client-thread | adminclient-38:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:43,824 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:45395]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-13
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:49:43,825 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,826 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:49:43,826 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:43,826 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,826 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:43,826 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,826 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,826 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:43,826 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:43,826 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,826 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,826 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,826 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,826 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,826 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:43,826 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:43,826 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739383826
2023-07-30 13:49:43,827 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:45395]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-primary-mm2-11
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = primary-mm2
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:49:43,831 - INFO  [kafka-producer-network-thread | producer-13:Metadata@279] - [Producer clientId=producer-13] Cluster ID: l67hsn_BQke0DtmYXtLNog
2023-07-30 13:49:43,831 - INFO  [DistributedHerder-connect-5-1:KafkaConsumer@1116] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Subscribed to partition(s): mm2-offsets.primary.internal-0, mm2-offsets.primary.internal-5, mm2-offsets.primary.internal-10, mm2-offsets.primary.internal-20, mm2-offsets.primary.internal-15, mm2-offsets.primary.internal-9, mm2-offsets.primary.internal-11, mm2-offsets.primary.internal-16, mm2-offsets.primary.internal-4, mm2-offsets.primary.internal-17, mm2-offsets.primary.internal-3, mm2-offsets.primary.internal-24, mm2-offsets.primary.internal-23, mm2-offsets.primary.internal-13, mm2-offsets.primary.internal-18, mm2-offsets.primary.internal-22, mm2-offsets.primary.internal-2, mm2-offsets.primary.internal-8, mm2-offsets.primary.internal-12, mm2-offsets.primary.internal-19, mm2-offsets.primary.internal-14, mm2-offsets.primary.internal-1, mm2-offsets.primary.internal-6, mm2-offsets.primary.internal-7, mm2-offsets.primary.internal-21
2023-07-30 13:49:43,831 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-0
2023-07-30 13:49:43,832 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-5
2023-07-30 13:49:43,832 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-10
2023-07-30 13:49:43,832 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-20
2023-07-30 13:49:43,832 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-15
2023-07-30 13:49:43,832 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-9
2023-07-30 13:49:43,832 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-11
2023-07-30 13:49:43,832 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-16
2023-07-30 13:49:43,832 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-4
2023-07-30 13:49:43,832 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-17
2023-07-30 13:49:43,832 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-3
2023-07-30 13:49:43,832 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-24
2023-07-30 13:49:43,832 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-23
2023-07-30 13:49:43,832 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-13
2023-07-30 13:49:43,832 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-18
2023-07-30 13:49:43,832 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-22
2023-07-30 13:49:43,832 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-2
2023-07-30 13:49:43,832 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-8
2023-07-30 13:49:43,832 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-12
2023-07-30 13:49:43,832 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,832 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:49:43,833 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,833 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:43,833 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,833 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,833 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:43,833 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:43,833 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,833 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,833 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,833 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,833 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,832 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-19
2023-07-30 13:49:43,833 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:43,833 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:43,833 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-14
2023-07-30 13:49:43,833 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739383833
2023-07-30 13:49:43,833 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-1
2023-07-30 13:49:43,833 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-6
2023-07-30 13:49:43,833 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-7
2023-07-30 13:49:43,833 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-21
2023-07-30 13:49:43,835 - INFO  [DistributedHerder-connect-4-1:Metadata@279] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Cluster ID: l67hsn_BQke0DtmYXtLNog
2023-07-30 13:49:43,838 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,838 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,838 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-23 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,838 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-21 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,838 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-18 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,838 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-16 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,838 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-14 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,838 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-12 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,838 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-10 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,838 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-8 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,838 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-6 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,838 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,839 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,839 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,839 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-24 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,839 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-22 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,839 - INFO  [DistributedHerder-connect-4-1:KafkaConsumer@1116] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Subscribed to partition(s): mm2-offsets.primary.internal-0, mm2-offsets.primary.internal-5, mm2-offsets.primary.internal-10, mm2-offsets.primary.internal-20, mm2-offsets.primary.internal-15, mm2-offsets.primary.internal-9, mm2-offsets.primary.internal-11, mm2-offsets.primary.internal-16, mm2-offsets.primary.internal-4, mm2-offsets.primary.internal-17, mm2-offsets.primary.internal-3, mm2-offsets.primary.internal-24, mm2-offsets.primary.internal-23, mm2-offsets.primary.internal-13, mm2-offsets.primary.internal-18, mm2-offsets.primary.internal-22, mm2-offsets.primary.internal-2, mm2-offsets.primary.internal-8, mm2-offsets.primary.internal-12, mm2-offsets.primary.internal-19, mm2-offsets.primary.internal-14, mm2-offsets.primary.internal-1, mm2-offsets.primary.internal-6, mm2-offsets.primary.internal-7, mm2-offsets.primary.internal-21
2023-07-30 13:49:43,839 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-20 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,839 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-0
2023-07-30 13:49:43,839 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-19 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,839 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-5
2023-07-30 13:49:43,839 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-17 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,839 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-10
2023-07-30 13:49:43,839 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-15 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,839 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-20
2023-07-30 13:49:43,839 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-13 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,839 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-15
2023-07-30 13:49:43,839 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-11 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,839 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-9
2023-07-30 13:49:43,840 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-9 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,840 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-11
2023-07-30 13:49:43,840 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-7 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,840 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-16
2023-07-30 13:49:43,840 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,840 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-4
2023-07-30 13:49:43,840 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-17
2023-07-30 13:49:43,840 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-3
2023-07-30 13:49:43,840 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-24
2023-07-30 13:49:43,840 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-23
2023-07-30 13:49:43,840 - INFO  [DistributedHerder-connect-5-1:KafkaBasedLog@161] - Finished reading KafkaBasedLog for topic mm2-offsets.primary.internal
2023-07-30 13:49:43,840 - INFO  [DistributedHerder-connect-5-1:KafkaBasedLog@163] - Started KafkaBasedLog for topic mm2-offsets.primary.internal
2023-07-30 13:49:43,840 - INFO  [DistributedHerder-connect-5-1:KafkaOffsetBackingStore@129] - Finished reading offsets topic and starting KafkaOffsetBackingStore
2023-07-30 13:49:43,840 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-13
2023-07-30 13:49:43,841 - INFO  [DistributedHerder-connect-5-1:Worker@202] - Worker started
2023-07-30 13:49:43,842 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-18
2023-07-30 13:49:43,842 - INFO  [DistributedHerder-connect-5-1:KafkaBasedLog@127] - Starting KafkaBasedLog with topic mm2-status.primary.internal
2023-07-30 13:49:43,842 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-22
2023-07-30 13:49:43,842 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-2
2023-07-30 13:49:43,842 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-8
2023-07-30 13:49:43,842 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-12
2023-07-30 13:49:43,842 - INFO  [DistributedHerder-connect-5-1:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:45395]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:43,842 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-19
2023-07-30 13:49:43,842 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-14
2023-07-30 13:49:43,843 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-1
2023-07-30 13:49:43,843 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-6
2023-07-30 13:49:43,843 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-7
2023-07-30 13:49:43,843 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-21
2023-07-30 13:49:43,845 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,845 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:49:43,845 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:43,845 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,846 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:43,846 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,846 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,846 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:43,846 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:43,846 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,846 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,846 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,846 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,846 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,846 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:43,846 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:43,846 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739383846
Jul 30, 2023 1:49:43 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource will be ignored. 
Jul 30, 2023 1:49:43 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.RootResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.RootResource will be ignored. 
Jul 30, 2023 1:49:43 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource will be ignored. 
Jul 30, 2023 1:49:43 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource will be ignored. 
2023-07-30 13:49:43,852 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,852 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,852 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-23 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,852 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-21 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,852 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-18 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,852 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-16 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,852 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-14 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,852 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-12 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,852 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-10 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,852 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-8 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,852 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-6 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,852 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,853 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,853 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,853 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-24 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,853 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-22 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,853 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-20 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,853 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-19 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,853 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-17 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,853 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-15 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,853 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-13 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,853 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-11 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,853 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-9 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,853 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-7 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,853 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Creating topic mm2-status.primary.internal with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0))
2023-07-30 13:49:43,853 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,854 - INFO  [DistributedHerder-connect-4-1:KafkaBasedLog@161] - Finished reading KafkaBasedLog for topic mm2-offsets.primary.internal
2023-07-30 13:49:43,854 - INFO  [DistributedHerder-connect-4-1:KafkaBasedLog@163] - Started KafkaBasedLog for topic mm2-offsets.primary.internal
2023-07-30 13:49:43,854 - INFO  [DistributedHerder-connect-4-1:KafkaOffsetBackingStore@129] - Finished reading offsets topic and starting KafkaOffsetBackingStore
2023-07-30 13:49:43,855 - INFO  [DistributedHerder-connect-4-1:Worker@202] - Worker started
2023-07-30 13:49:43,855 - INFO  [DistributedHerder-connect-4-1:KafkaBasedLog@127] - Starting KafkaBasedLog with topic mm2-status.primary.internal
2023-07-30 13:49:43,855 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:45395]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:43,858 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,858 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:49:43,858 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:43,858 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,858 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:43,858 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,858 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,858 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:43,858 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:43,858 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,858 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,858 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,858 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,858 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,858 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:43,858 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:43,858 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739383858
2023-07-30 13:49:43,863 - INFO  [DistributedHerder-connect-4-1:TopicAdmin@400] - Unable to use admin client to verify the cleanup policy of 'mm2-status.primary.internal' topic is 'compact', either because the broker is an older version or because the Kafka principal used for Connect internal topics does not have the required permission to describe topic configurations.
2023-07-30 13:49:43,863 - INFO  [kafka-admin-client-thread | adminclient-48:AppInfoParser@83] - App info kafka.admin.client for adminclient-48 unregistered
2023-07-30 13:49:43,864 - INFO  [kafka-admin-client-thread | adminclient-48:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:43,864 - INFO  [kafka-admin-client-thread | adminclient-48:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:43,864 - INFO  [kafka-admin-client-thread | adminclient-48:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:43,865 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:45395]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-14
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:49:43,869 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,870 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:49:43,870 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:43,870 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,870 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:43,870 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,870 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,870 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:43,870 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:43,870 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,870 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,870 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,870 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,870 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,871 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(mm2-status.primary.internal-4, mm2-status.primary.internal-1, mm2-status.primary.internal-3, mm2-status.primary.internal-0, mm2-status.primary.internal-2)
2023-07-30 13:49:43,871 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:43,871 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:43,871 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739383871
2023-07-30 13:49:43,872 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:45395]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-primary-mm2-12
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = primary-mm2
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:49:43,872 - INFO  [kafka-producer-network-thread | producer-14:Metadata@279] - [Producer clientId=producer-14] Cluster ID: l67hsn_BQke0DtmYXtLNog
2023-07-30 13:49:43,873 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=mm2-status.primary.internal-1, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:43,874 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,874 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:49:43,874 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,874 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:43,874 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,874 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,874 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:43,874 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:43,874 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,874 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,874 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,874 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,874 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,874 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition mm2-status.primary.internal-1 in /tmp/junit7660765717748375067/junit866559250785637651/mm2-status.primary.internal-1 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:43,874 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:43,874 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:43,875 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739383874
2023-07-30 13:49:43,875 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition mm2-status.primary.internal-1 broker=0] No checkpointed highwatermark is found for partition mm2-status.primary.internal-1
2023-07-30 13:49:43,875 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition mm2-status.primary.internal-1 broker=0] Log loaded for partition mm2-status.primary.internal-1 with initial high watermark 0
2023-07-30 13:49:43,876 - INFO  [DistributedHerder-connect-4-1:Metadata@279] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Cluster ID: l67hsn_BQke0DtmYXtLNog
2023-07-30 13:49:43,878 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=mm2-status.primary.internal-2, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:43,879 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition mm2-status.primary.internal-2 in /tmp/junit7660765717748375067/junit866559250785637651/mm2-status.primary.internal-2 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:43,879 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition mm2-status.primary.internal-2 broker=0] No checkpointed highwatermark is found for partition mm2-status.primary.internal-2
2023-07-30 13:49:43,879 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition mm2-status.primary.internal-2 broker=0] Log loaded for partition mm2-status.primary.internal-2 with initial high watermark 0
Jul 30, 2023 1:49:43 PM org.glassfish.jersey.internal.Errors logErrors
WARNING: The following warnings have been detected: WARNING: The (sub)resource method listLoggers in org.apache.kafka.connect.runtime.rest.resources.LoggingResource contains empty path annotation.
WARNING: The (sub)resource method createConnector in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectors in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectorPlugins in org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource contains empty path annotation.
WARNING: The (sub)resource method serverInfo in org.apache.kafka.connect.runtime.rest.resources.RootResource contains empty path annotation.

2023-07-30 13:49:43,881 - INFO  [main:ContextHandler@849] - Started o.e.j.s.ServletContextHandler@2fd64b11{/,null,AVAILABLE}
2023-07-30 13:49:43,882 - INFO  [main:RestServer@319] - REST resources initialized; server is started and ready to handle requests
2023-07-30 13:49:43,882 - INFO  [main:Connect@57] - Kafka Connect started
2023-07-30 13:49:43,882 - INFO  [main:EmbeddedConnectCluster@169] - Started worker WorkerHandle{workerName='connect-worker-1'workerURL='http://localhost:39987/'}
2023-07-30 13:49:43,882 - INFO  [main:ConnectDistributed@90] - Scanning for plugin classes. This might take a moment ...
2023-07-30 13:49:43,886 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=mm2-status.primary.internal-0, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:43,886 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition mm2-status.primary.internal-0 in /tmp/junit7660765717748375067/junit866559250785637651/mm2-status.primary.internal-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:43,886 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition mm2-status.primary.internal-0 broker=0] No checkpointed highwatermark is found for partition mm2-status.primary.internal-0
2023-07-30 13:49:43,886 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition mm2-status.primary.internal-0 broker=0] Log loaded for partition mm2-status.primary.internal-0 with initial high watermark 0
2023-07-30 13:49:43,894 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=mm2-status.primary.internal-3, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:43,895 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition mm2-status.primary.internal-3 in /tmp/junit7660765717748375067/junit866559250785637651/mm2-status.primary.internal-3 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:43,895 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition mm2-status.primary.internal-3 broker=0] No checkpointed highwatermark is found for partition mm2-status.primary.internal-3
2023-07-30 13:49:43,895 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition mm2-status.primary.internal-3 broker=0] Log loaded for partition mm2-status.primary.internal-3 with initial high watermark 0
2023-07-30 13:49:43,902 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=mm2-status.primary.internal-4, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:43,903 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition mm2-status.primary.internal-4 in /tmp/junit7660765717748375067/junit866559250785637651/mm2-status.primary.internal-4 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:43,903 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition mm2-status.primary.internal-4 broker=0] No checkpointed highwatermark is found for partition mm2-status.primary.internal-4
2023-07-30 13:49:43,903 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition mm2-status.primary.internal-4 broker=0] Log loaded for partition mm2-status.primary.internal-4 with initial high watermark 0
2023-07-30 13:49:43,912 - INFO  [DistributedHerder-connect-5-1:TopicAdmin@284] - Created topic (name=mm2-status.primary.internal, numPartitions=5, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at localhost:45395
2023-07-30 13:49:43,914 - INFO  [kafka-admin-client-thread | adminclient-47:AppInfoParser@83] - App info kafka.admin.client for adminclient-47 unregistered
2023-07-30 13:49:43,915 - INFO  [kafka-admin-client-thread | adminclient-47:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:43,915 - INFO  [kafka-admin-client-thread | adminclient-47:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:43,915 - INFO  [kafka-admin-client-thread | adminclient-47:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:43,916 - INFO  [DistributedHerder-connect-5-1:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:45395]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-15
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:49:43,917 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,918 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:49:43,918 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:43,918 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,918 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:43,918 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,918 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,918 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:43,918 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:43,918 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,918 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,918 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,918 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,918 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,918 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:43,918 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:43,918 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739383918
2023-07-30 13:49:43,918 - INFO  [DistributedHerder-connect-5-1:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:45395]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-primary-mm2-13
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = primary-mm2
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:49:43,919 - INFO  [kafka-producer-network-thread | producer-15:Metadata@279] - [Producer clientId=producer-15] Cluster ID: l67hsn_BQke0DtmYXtLNog
2023-07-30 13:49:43,920 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,920 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:49:43,920 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,920 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:43,920 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,920 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,920 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:43,920 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:43,920 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,920 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,920 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,920 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,920 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,920 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:43,920 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:43,920 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739383920
2023-07-30 13:49:43,921 - INFO  [DistributedHerder-connect-5-1:Metadata@279] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Cluster ID: l67hsn_BQke0DtmYXtLNog
2023-07-30 13:49:43,924 - INFO  [DistributedHerder-connect-5-1:KafkaConsumer@1116] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Subscribed to partition(s): mm2-status.primary.internal-0, mm2-status.primary.internal-1, mm2-status.primary.internal-4, mm2-status.primary.internal-2, mm2-status.primary.internal-3
2023-07-30 13:49:43,924 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-status.primary.internal-0
2023-07-30 13:49:43,924 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-status.primary.internal-1
2023-07-30 13:49:43,924 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-status.primary.internal-4
2023-07-30 13:49:43,924 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-status.primary.internal-2
2023-07-30 13:49:43,924 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-status.primary.internal-3
2023-07-30 13:49:43,927 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Resetting offset for partition mm2-status.primary.internal-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,927 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Resetting offset for partition mm2-status.primary.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,927 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Resetting offset for partition mm2-status.primary.internal-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,927 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Resetting offset for partition mm2-status.primary.internal-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,927 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Resetting offset for partition mm2-status.primary.internal-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,927 - INFO  [DistributedHerder-connect-5-1:KafkaBasedLog@161] - Finished reading KafkaBasedLog for topic mm2-status.primary.internal
2023-07-30 13:49:43,927 - INFO  [DistributedHerder-connect-5-1:KafkaBasedLog@163] - Started KafkaBasedLog for topic mm2-status.primary.internal
2023-07-30 13:49:43,928 - INFO  [DistributedHerder-connect-5-1:KafkaConfigBackingStore@265] - Starting KafkaConfigBackingStore
2023-07-30 13:49:43,928 - INFO  [DistributedHerder-connect-5-1:KafkaBasedLog@127] - Starting KafkaBasedLog with topic mm2-configs.primary.internal
2023-07-30 13:49:43,929 - INFO  [DistributedHerder-connect-5-1:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:45395]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:43,931 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,932 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:49:43,932 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:43,932 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,932 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:43,932 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,932 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,932 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:43,932 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:43,932 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,932 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,932 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,932 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,932 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,932 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:43,932 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:43,932 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739383932
2023-07-30 13:49:43,936 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Creating topic mm2-configs.primary.internal with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0))
2023-07-30 13:49:43,941 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(mm2-configs.primary.internal-0)
2023-07-30 13:49:43,943 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=mm2-configs.primary.internal-0, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:43,943 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition mm2-configs.primary.internal-0 in /tmp/junit7660765717748375067/junit866559250785637651/mm2-configs.primary.internal-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:43,944 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition mm2-configs.primary.internal-0 broker=0] No checkpointed highwatermark is found for partition mm2-configs.primary.internal-0
2023-07-30 13:49:43,944 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition mm2-configs.primary.internal-0 broker=0] Log loaded for partition mm2-configs.primary.internal-0 with initial high watermark 0
2023-07-30 13:49:43,947 - INFO  [DistributedHerder-connect-5-1:TopicAdmin@284] - Created topic (name=mm2-configs.primary.internal, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at localhost:45395
2023-07-30 13:49:43,947 - INFO  [kafka-admin-client-thread | adminclient-49:AppInfoParser@83] - App info kafka.admin.client for adminclient-49 unregistered
2023-07-30 13:49:43,948 - INFO  [kafka-admin-client-thread | adminclient-49:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:43,948 - INFO  [kafka-admin-client-thread | adminclient-49:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:43,948 - INFO  [kafka-admin-client-thread | adminclient-49:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:43,948 - INFO  [DistributedHerder-connect-5-1:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:45395]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-16
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:49:43,950 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,950 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:49:43,950 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:43,950 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,950 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:43,950 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,950 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,951 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:43,951 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:43,951 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,951 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,951 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,951 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,951 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,951 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:43,951 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:43,951 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739383951
2023-07-30 13:49:43,951 - INFO  [DistributedHerder-connect-5-1:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:45395]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-primary-mm2-14
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = primary-mm2
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:49:43,952 - INFO  [kafka-producer-network-thread | producer-16:Metadata@279] - [Producer clientId=producer-16] Cluster ID: l67hsn_BQke0DtmYXtLNog
2023-07-30 13:49:43,952 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,952 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:49:43,952 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,952 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:43,952 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,952 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,953 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:43,953 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:43,953 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,953 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,953 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,953 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,953 - WARN  [DistributedHerder-connect-5-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,953 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:43,953 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:43,953 - INFO  [DistributedHerder-connect-5-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739383953
2023-07-30 13:49:43,954 - INFO  [DistributedHerder-connect-5-1:Metadata@279] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Cluster ID: l67hsn_BQke0DtmYXtLNog
2023-07-30 13:49:43,957 - INFO  [DistributedHerder-connect-5-1:KafkaConsumer@1116] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Subscribed to partition(s): mm2-configs.primary.internal-0
2023-07-30 13:49:43,957 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-configs.primary.internal-0
2023-07-30 13:49:43,959 - INFO  [DistributedHerder-connect-5-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Resetting offset for partition mm2-configs.primary.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,959 - INFO  [DistributedHerder-connect-5-1:KafkaBasedLog@161] - Finished reading KafkaBasedLog for topic mm2-configs.primary.internal
2023-07-30 13:49:43,959 - INFO  [DistributedHerder-connect-5-1:KafkaBasedLog@163] - Started KafkaBasedLog for topic mm2-configs.primary.internal
2023-07-30 13:49:43,959 - INFO  [DistributedHerder-connect-5-1:KafkaConfigBackingStore@280] - Started KafkaConfigBackingStore
2023-07-30 13:49:43,960 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@290] - [Worker clientId=connect-5, groupId=primary-mm2] Herder started
2023-07-30 13:49:43,964 - INFO  [DistributedHerder-connect-5-1:Metadata@279] - [Worker clientId=connect-5, groupId=primary-mm2] Cluster ID: l67hsn_BQke0DtmYXtLNog
2023-07-30 13:49:43,967 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Creating topic __consumer_offsets with configuration {segment.bytes=104857600, compression.type=producer, cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0), 5 -> ArrayBuffer(0), 6 -> ArrayBuffer(0), 7 -> ArrayBuffer(0), 8 -> ArrayBuffer(0), 9 -> ArrayBuffer(0), 10 -> ArrayBuffer(0), 11 -> ArrayBuffer(0), 12 -> ArrayBuffer(0), 13 -> ArrayBuffer(0), 14 -> ArrayBuffer(0), 15 -> ArrayBuffer(0), 16 -> ArrayBuffer(0), 17 -> ArrayBuffer(0), 18 -> ArrayBuffer(0), 19 -> ArrayBuffer(0), 20 -> ArrayBuffer(0), 21 -> ArrayBuffer(0), 22 -> ArrayBuffer(0), 23 -> ArrayBuffer(0), 24 -> ArrayBuffer(0), 25 -> ArrayBuffer(0), 26 -> ArrayBuffer(0), 27 -> ArrayBuffer(0), 28 -> ArrayBuffer(0), 29 -> ArrayBuffer(0), 30 -> ArrayBuffer(0), 31 -> ArrayBuffer(0), 32 -> ArrayBuffer(0), 33 -> ArrayBuffer(0), 34 -> ArrayBuffer(0), 35 -> ArrayBuffer(0), 36 -> ArrayBuffer(0), 37 -> ArrayBuffer(0), 38 -> ArrayBuffer(0), 39 -> ArrayBuffer(0), 40 -> ArrayBuffer(0), 41 -> ArrayBuffer(0), 42 -> ArrayBuffer(0), 43 -> ArrayBuffer(0), 44 -> ArrayBuffer(0), 45 -> ArrayBuffer(0), 46 -> ArrayBuffer(0), 47 -> ArrayBuffer(0), 48 -> ArrayBuffer(0), 49 -> ArrayBuffer(0))
2023-07-30 13:49:43,969 - INFO  [DistributedHerder-connect-4-1:KafkaConsumer@1116] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Subscribed to partition(s): mm2-status.primary.internal-0, mm2-status.primary.internal-1, mm2-status.primary.internal-4, mm2-status.primary.internal-2, mm2-status.primary.internal-3
2023-07-30 13:49:43,969 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-status.primary.internal-0
2023-07-30 13:49:43,969 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-status.primary.internal-1
2023-07-30 13:49:43,969 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-status.primary.internal-4
2023-07-30 13:49:43,969 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-status.primary.internal-2
2023-07-30 13:49:43,969 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-status.primary.internal-3
2023-07-30 13:49:43,969 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [KafkaApi-0] Auto creation of topic __consumer_offsets with 50 partitions and replication factor 1 is successful
2023-07-30 13:49:43,971 - INFO  [main:Reflections@239] - Reflections took 88 ms to scan 1 urls, producing 349 keys and 1055 values [using 20 cores]
2023-07-30 13:49:43,971 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Resetting offset for partition mm2-status.primary.internal-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,971 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Resetting offset for partition mm2-status.primary.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,971 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Resetting offset for partition mm2-status.primary.internal-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,972 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Resetting offset for partition mm2-status.primary.internal-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,972 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Resetting offset for partition mm2-status.primary.internal-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:43,972 - INFO  [DistributedHerder-connect-4-1:KafkaBasedLog@161] - Finished reading KafkaBasedLog for topic mm2-status.primary.internal
2023-07-30 13:49:43,972 - INFO  [DistributedHerder-connect-4-1:KafkaBasedLog@163] - Started KafkaBasedLog for topic mm2-status.primary.internal
2023-07-30 13:49:43,972 - INFO  [DistributedHerder-connect-4-1:KafkaConfigBackingStore@265] - Starting KafkaConfigBackingStore
2023-07-30 13:49:43,973 - INFO  [DistributedHerder-connect-4-1:KafkaBasedLog@127] - Starting KafkaBasedLog with topic mm2-configs.primary.internal
2023-07-30 13:49:43,974 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:45395]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:43,975 - WARN  [main:ReflectionUtils@318] - could not get type for name kotlin.jvm.internal.Lambda from any class loader
org.reflections.ReflectionsException: could not get type for name kotlin.jvm.internal.Lambda
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:91)
	at org.apache.kafka.connect.util.clusters.WorkerHandle.start(WorkerHandle.java:50)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.addWorker(EmbeddedConnectCluster.java:167)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.startConnect(EmbeddedConnectCluster.java:253)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.start(EmbeddedConnectCluster.java:137)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: kotlin.jvm.internal.Lambda
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 68 more
2023-07-30 13:49:43,976 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,976 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:49:43,976 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:43,976 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,976 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:43,976 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,976 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,976 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:43,977 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:43,977 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,977 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,977 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,977 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,977 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,977 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:43,977 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:43,977 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739383977
2023-07-30 13:49:43,977 - WARN  [main:ReflectionUtils@318] - could not get type for name kotlin.jvm.functions.Function0 from any class loader
org.reflections.ReflectionsException: could not get type for name kotlin.jvm.functions.Function0
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:91)
	at org.apache.kafka.connect.util.clusters.WorkerHandle.start(WorkerHandle.java:50)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.addWorker(EmbeddedConnectCluster.java:167)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.startConnect(EmbeddedConnectCluster.java:253)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.start(EmbeddedConnectCluster.java:137)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: kotlin.jvm.functions.Function0
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 68 more
2023-07-30 13:49:43,979 - WARN  [main:ReflectionUtils@318] - could not get type for name java.util.spi.ToolProvider from any class loader
org.reflections.ReflectionsException: could not get type for name java.util.spi.ToolProvider
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.cli.ConnectDistributed.startConnect(ConnectDistributed.java:91)
	at org.apache.kafka.connect.util.clusters.WorkerHandle.start(WorkerHandle.java:50)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.addWorker(EmbeddedConnectCluster.java:167)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.startConnect(EmbeddedConnectCluster.java:253)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.start(EmbeddedConnectCluster.java:137)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: java.util.spi.ToolProvider
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 68 more
2023-07-30 13:49:43,983 - INFO  [main:DelegatingClassLoader@269] - Registered loader: java.net.FactoryURLClassLoader@483bf400
2023-07-30 13:49:43,984 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension'
2023-07-30 13:49:43,984 - INFO  [kafka-admin-client-thread | adminclient-50:AppInfoParser@83] - App info kafka.admin.client for adminclient-50 unregistered
2023-07-30 13:49:43,984 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.runtime.isolation.PluginsTest$TestConnectRestExtension'
2023-07-30 13:49:43,984 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy'
2023-07-30 13:49:43,984 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy'
2023-07-30 13:49:43,984 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy'
2023-07-30 13:49:43,985 - INFO  [kafka-admin-client-thread | adminclient-50:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:43,985 - INFO  [kafka-admin-client-thread | adminclient-50:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:43,985 - INFO  [kafka-admin-client-thread | adminclient-50:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:43,985 - INFO  [main:DelegatingClassLoader@427] - Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension'
2023-07-30 13:49:43,985 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'TestConnectRestExtension' and 'Test' to plugin 'org.apache.kafka.connect.runtime.isolation.PluginsTest$TestConnectRestExtension'
2023-07-30 13:49:43,985 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:45395]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-17
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:49:43,985 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy'
2023-07-30 13:49:43,986 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy'
2023-07-30 13:49:43,986 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy'
2023-07-30 13:49:43,986 - INFO  [main:AbstractConfig@361] - DistributedConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:45395]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	config.providers = []
	config.storage.replication.factor = 1
	config.storage.topic = mm2-configs.primary.internal
	connect.protocol = sessioned
	connections.max.idle.ms = 540000
	connector.client.config.override.policy = None
	group.id = primary-mm2
	header.converter = class org.apache.kafka.connect.converters.ByteArrayConverter
	heartbeat.interval.ms = 3000
	inter.worker.key.generation.algorithm = HmacSHA256
	inter.worker.key.size = null
	inter.worker.key.ttl.ms = 3600000
	inter.worker.signature.algorithm = HmacSHA256
	inter.worker.verification.algorithms = [HmacSHA256]
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.converters.ByteArrayConverter
	listeners = null
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 60000
	offset.flush.timeout.ms = 5000
	offset.storage.partitions = 25
	offset.storage.replication.factor = 1
	offset.storage.topic = mm2-offsets.primary.internal
	plugin.path = null
	rebalance.timeout.ms = 60000
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 40000
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = localhost
	rest.port = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	scheduled.rebalance.max.delay.ms = 300000
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	status.storage.partitions = 5
	status.storage.replication.factor = 1
	status.storage.topic = mm2-status.primary.internal
	task.shutdown.graceful.timeout.ms = 5000
	topic.creation.enable = true
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.converters.ByteArrayConverter
	worker.sync.timeout.ms = 3000
	worker.unsync.backoff.ms = 300000

2023-07-30 13:49:43,987 - WARN  [main:WorkerConfig@420] - Variables cannot be used in the 'plugin.path' property, since the property is used by plugin scanning before the config providers that replace the variables are initialized. The raw value 'null' was used for plugin scanning, as opposed to the transformed value 'null', and this may cause unexpected results.
2023-07-30 13:49:43,987 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:49:43,987 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:45395]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:43,988 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,989 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:49:43,989 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:43,989 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,989 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:43,989 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,989 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,989 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:43,989 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:43,989 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,989 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,989 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,989 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,989 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:43,989 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,990 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,990 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,990 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:43,990 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:43,990 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:43,990 - INFO  [kafka-producer-network-thread | producer-17:Metadata@279] - [Producer clientId=producer-17] Cluster ID: l67hsn_BQke0DtmYXtLNog
2023-07-30 13:49:43,990 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,990 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739383990
2023-07-30 13:49:43,990 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,991 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:43,991 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,991 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,991 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,991 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,991 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:45395]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-primary-mm2-15
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = primary-mm2
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:49:43,991 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,992 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:43,992 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:43,992 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739383992
2023-07-30 13:49:43,994 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,994 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:49:43,994 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,994 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:43,994 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,994 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,994 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:43,994 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:43,994 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,994 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:43,995 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:43,995 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,995 - WARN  [DistributedHerder-connect-4-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:43,995 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:43,995 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:43,995 - INFO  [DistributedHerder-connect-4-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739383995
2023-07-30 13:49:43,995 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: l67hsn_BQke0DtmYXtLNog
2023-07-30 13:49:43,996 - INFO  [kafka-admin-client-thread | adminclient-51:AppInfoParser@83] - App info kafka.admin.client for adminclient-51 unregistered
2023-07-30 13:49:43,996 - INFO  [DistributedHerder-connect-4-1:Metadata@279] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Cluster ID: l67hsn_BQke0DtmYXtLNog
2023-07-30 13:49:43,997 - INFO  [kafka-admin-client-thread | adminclient-51:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:43,997 - INFO  [kafka-admin-client-thread | adminclient-51:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:43,997 - INFO  [kafka-admin-client-thread | adminclient-51:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:43,997 - INFO  [main:RestServer@132] - Added connector for http://localhost:0
2023-07-30 13:49:43,997 - INFO  [main:RestServer@204] - Initializing REST server
2023-07-30 13:49:43,998 - INFO  [main:Server@360] - jetty-9.4.30.v20200611; built: 2020-06-11T12:34:51.929Z; git: 271836e4c1f4612f12b7bb13ef5a92a927634b0d; jvm 1.8.0_275-8u275-b01-0ubuntu1~18.04-b01
2023-07-30 13:49:43,999 - INFO  [DistributedHerder-connect-4-1:KafkaConsumer@1116] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Subscribed to partition(s): mm2-configs.primary.internal-0
2023-07-30 13:49:43,999 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-configs.primary.internal-0
2023-07-30 13:49:44,003 - INFO  [DistributedHerder-connect-4-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Resetting offset for partition mm2-configs.primary.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:44,005 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-37, __consumer_offsets-38, __consumer_offsets-13, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40)
2023-07-30 13:49:44,005 - INFO  [DistributedHerder-connect-4-1:KafkaBasedLog@161] - Finished reading KafkaBasedLog for topic mm2-configs.primary.internal
2023-07-30 13:49:44,006 - INFO  [DistributedHerder-connect-4-1:KafkaBasedLog@163] - Started KafkaBasedLog for topic mm2-configs.primary.internal
2023-07-30 13:49:44,006 - INFO  [DistributedHerder-connect-4-1:KafkaConfigBackingStore@280] - Started KafkaConfigBackingStore
2023-07-30 13:49:44,007 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@290] - [Worker clientId=connect-4, groupId=primary-mm2] Herder started
2023-07-30 13:49:44,011 - INFO  [main:AbstractConnector@331] - Started http_localhost0@1984212d{HTTP/1.1, (http/1.1)}{localhost:41733}
2023-07-30 13:49:44,011 - INFO  [main:Server@400] - Started @6269ms
2023-07-30 13:49:44,011 - INFO  [main:RestServer@371] - Advertised URI: http://localhost:41733/
2023-07-30 13:49:44,011 - INFO  [main:RestServer@219] - REST server listening at http://localhost:41733/, advertising URL http://localhost:41733/
2023-07-30 13:49:44,011 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-3, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,011 - INFO  [main:RestServer@371] - Advertised URI: http://localhost:41733/
2023-07-30 13:49:44,011 - INFO  [DistributedHerder-connect-4-1:Metadata@279] - [Worker clientId=connect-4, groupId=primary-mm2] Cluster ID: l67hsn_BQke0DtmYXtLNog
2023-07-30 13:49:44,012 - INFO  [main:RestServer@220] - REST admin endpoints at http://localhost:41733/
2023-07-30 13:49:44,012 - INFO  [main:RestServer@371] - Advertised URI: http://localhost:41733/
2023-07-30 13:49:44,012 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:49:44,012 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-3 in /tmp/junit7660765717748375067/junit866559250785637651/__consumer_offsets-3 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,012 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:45395]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:44,013 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3
2023-07-30 13:49:44,013 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-3 broker=0] Log loaded for partition __consumer_offsets-3 with initial high watermark 0
2023-07-30 13:49:44,013 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:44,013 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:44,013 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:44,014 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:44,014 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:44,014 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:44,014 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:44,014 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:44,014 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:44,014 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:44,014 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:44,014 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:44,014 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:44,014 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:44,015 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739384014
2023-07-30 13:49:44,015 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-18, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,016 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-18 in /tmp/junit7660765717748375067/junit866559250785637651/__consumer_offsets-18 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,016 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-18 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-18
2023-07-30 13:49:44,016 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-18 broker=0] Log loaded for partition __consumer_offsets-18 with initial high watermark 0
2023-07-30 13:49:44,017 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: l67hsn_BQke0DtmYXtLNog
2023-07-30 13:49:44,018 - INFO  [kafka-admin-client-thread | adminclient-52:AppInfoParser@83] - App info kafka.admin.client for adminclient-52 unregistered
2023-07-30 13:49:44,018 - INFO  [kafka-admin-client-thread | adminclient-52:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:44,018 - INFO  [kafka-admin-client-thread | adminclient-52:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:44,018 - INFO  [kafka-admin-client-thread | adminclient-52:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:44,019 - INFO  [main:NoneConnectorClientConfigOverridePolicy@45] - Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden
2023-07-30 13:49:44,019 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:49:44,019 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:45395]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:44,020 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:44,020 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:44,020 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:44,020 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:44,020 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:44,020 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:44,020 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:44,020 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:44,020 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:44,020 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:44,020 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:44,020 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:44,021 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:44,021 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:44,021 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739384021
2023-07-30 13:49:44,024 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: l67hsn_BQke0DtmYXtLNog
2023-07-30 13:49:44,024 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-41, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,024 - INFO  [kafka-admin-client-thread | adminclient-53:AppInfoParser@83] - App info kafka.admin.client for adminclient-53 unregistered
2023-07-30 13:49:44,025 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-41 in /tmp/junit7660765717748375067/junit866559250785637651/__consumer_offsets-41 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,025 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-41 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-41
2023-07-30 13:49:44,025 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-41 broker=0] Log loaded for partition __consumer_offsets-41 with initial high watermark 0
2023-07-30 13:49:44,025 - INFO  [kafka-admin-client-thread | adminclient-53:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:44,025 - INFO  [kafka-admin-client-thread | adminclient-53:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:44,025 - INFO  [kafka-admin-client-thread | adminclient-53:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:44,025 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:44,026 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:44,026 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739384025
2023-07-30 13:49:44,026 - INFO  [main:AbstractConfig@361] - JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false

2023-07-30 13:49:44,026 - INFO  [main:AbstractConfig@361] - JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false

2023-07-30 13:49:44,026 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:49:44,027 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:45395]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:44,027 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:44,027 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:44,027 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:44,027 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:44,028 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:44,028 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:44,028 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:44,028 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:44,028 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:44,028 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:44,028 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:44,028 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:44,028 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:44,028 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:44,028 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739384028
2023-07-30 13:49:44,031 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: l67hsn_BQke0DtmYXtLNog
2023-07-30 13:49:44,031 - INFO  [kafka-admin-client-thread | adminclient-54:AppInfoParser@83] - App info kafka.admin.client for adminclient-54 unregistered
2023-07-30 13:49:44,032 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-10, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,032 - INFO  [kafka-admin-client-thread | adminclient-54:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:44,032 - INFO  [kafka-admin-client-thread | adminclient-54:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:44,032 - INFO  [kafka-admin-client-thread | adminclient-54:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:44,032 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:49:44,033 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:45395]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:44,033 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-10 in /tmp/junit7660765717748375067/junit866559250785637651/__consumer_offsets-10 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,033 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-10 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-10
2023-07-30 13:49:44,033 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-10 broker=0] Log loaded for partition __consumer_offsets-10 with initial high watermark 0
2023-07-30 13:49:44,033 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:44,033 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:44,034 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:44,034 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:44,034 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:44,034 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:44,034 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:44,034 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:44,034 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:44,034 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:44,034 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:44,034 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:44,034 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:44,035 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:44,035 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739384034
2023-07-30 13:49:44,037 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: l67hsn_BQke0DtmYXtLNog
2023-07-30 13:49:44,037 - INFO  [kafka-admin-client-thread | adminclient-55:AppInfoParser@83] - App info kafka.admin.client for adminclient-55 unregistered
2023-07-30 13:49:44,038 - INFO  [kafka-admin-client-thread | adminclient-55:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:44,038 - INFO  [kafka-admin-client-thread | adminclient-55:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:44,038 - INFO  [kafka-admin-client-thread | adminclient-55:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:44,038 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:49:44,038 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:45395]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:44,039 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:44,039 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:44,039 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:44,039 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:44,039 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:44,039 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:44,040 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:44,040 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:44,040 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:44,040 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:44,040 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:44,040 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:44,040 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:44,040 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:44,040 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739384040
2023-07-30 13:49:44,040 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-33, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,041 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-33 in /tmp/junit7660765717748375067/junit866559250785637651/__consumer_offsets-33 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,041 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-33 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-33
2023-07-30 13:49:44,041 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-33 broker=0] Log loaded for partition __consumer_offsets-33 with initial high watermark 0
2023-07-30 13:49:44,043 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: l67hsn_BQke0DtmYXtLNog
2023-07-30 13:49:44,043 - INFO  [kafka-admin-client-thread | adminclient-56:AppInfoParser@83] - App info kafka.admin.client for adminclient-56 unregistered
2023-07-30 13:49:44,044 - INFO  [kafka-admin-client-thread | adminclient-56:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:44,044 - INFO  [kafka-admin-client-thread | adminclient-56:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:44,044 - INFO  [kafka-admin-client-thread | adminclient-56:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:44,044 - INFO  [main:ConnectUtils@49] - Creating Kafka admin client
2023-07-30 13:49:44,044 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:45395]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:44,045 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:44,045 - WARN  [main:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:44,045 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:44,045 - WARN  [main:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:44,046 - WARN  [main:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:44,046 - WARN  [main:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:44,046 - WARN  [main:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:44,046 - WARN  [main:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:44,046 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:44,046 - WARN  [main:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:44,046 - WARN  [main:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:44,046 - WARN  [main:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:44,046 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:44,046 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:44,046 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739384046
2023-07-30 13:49:44,049 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-48, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,049 - INFO  [main:ConnectUtils@65] - Kafka cluster ID: l67hsn_BQke0DtmYXtLNog
2023-07-30 13:49:44,049 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-48 in /tmp/junit7660765717748375067/junit866559250785637651/__consumer_offsets-48 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,049 - INFO  [kafka-admin-client-thread | adminclient-57:AppInfoParser@83] - App info kafka.admin.client for adminclient-57 unregistered
2023-07-30 13:49:44,049 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-48 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-48
2023-07-30 13:49:44,049 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-48 broker=0] Log loaded for partition __consumer_offsets-48 with initial high watermark 0
2023-07-30 13:49:44,050 - INFO  [kafka-admin-client-thread | adminclient-57:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:44,050 - INFO  [kafka-admin-client-thread | adminclient-57:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:44,050 - INFO  [kafka-admin-client-thread | adminclient-57:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:44,051 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:44,051 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:44,051 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739384051
2023-07-30 13:49:44,051 - INFO  [main:ConnectDistributed@128] - Kafka Connect distributed worker initialization took 169ms
2023-07-30 13:49:44,051 - INFO  [main:Connect@51] - Kafka Connect starting
2023-07-30 13:49:44,052 - INFO  [main:RestServer@224] - Initializing REST resources
2023-07-30 13:49:44,052 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@286] - [Worker clientId=connect-6, groupId=primary-mm2] Herder starting
2023-07-30 13:49:44,052 - INFO  [DistributedHerder-connect-6-1:Worker@195] - Worker starting
2023-07-30 13:49:44,052 - INFO  [DistributedHerder-connect-6-1:KafkaOffsetBackingStore@127] - Starting KafkaOffsetBackingStore
2023-07-30 13:49:44,052 - INFO  [DistributedHerder-connect-6-1:KafkaBasedLog@127] - Starting KafkaBasedLog with topic mm2-offsets.primary.internal
2023-07-30 13:49:44,052 - INFO  [main:RestServer@241] - Adding admin resources to main listener
2023-07-30 13:49:44,052 - INFO  [DistributedHerder-connect-6-1:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:45395]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:44,053 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:44,053 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:49:44,053 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:44,053 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:44,053 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:44,053 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:44,053 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:44,053 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:44,053 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:44,053 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:44,053 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:44,053 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:44,053 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:44,053 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:44,053 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:44,053 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:44,053 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739384053
2023-07-30 13:49:44,054 - INFO  [main:DefaultSessionIdManager@334] - DefaultSessionIdManager workerName=node0
2023-07-30 13:49:44,054 - INFO  [main:DefaultSessionIdManager@339] - No SessionScavenger set, using defaults
2023-07-30 13:49:44,054 - INFO  [main:HouseKeeper@140] - node0 Scavenging every 660000ms
2023-07-30 13:49:44,057 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-19, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,057 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-19 in /tmp/junit7660765717748375067/junit866559250785637651/__consumer_offsets-19 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,057 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-19 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-19
2023-07-30 13:49:44,057 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-19 broker=0] Log loaded for partition __consumer_offsets-19 with initial high watermark 0
2023-07-30 13:49:44,058 - INFO  [kafka-admin-client-thread | adminclient-58:AppInfoParser@83] - App info kafka.admin.client for adminclient-58 unregistered
2023-07-30 13:49:44,058 - INFO  [kafka-admin-client-thread | adminclient-58:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:44,058 - INFO  [kafka-admin-client-thread | adminclient-58:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:44,058 - INFO  [kafka-admin-client-thread | adminclient-58:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:44,059 - INFO  [DistributedHerder-connect-6-1:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:45395]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-18
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:49:44,060 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:44,061 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:49:44,061 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:44,061 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:44,061 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:44,061 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:44,061 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:44,061 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:44,062 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:44,062 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:44,062 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:44,062 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:44,062 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:44,062 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:44,062 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:44,062 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:44,062 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739384062
2023-07-30 13:49:44,062 - INFO  [DistributedHerder-connect-6-1:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:45395]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-primary-mm2-16
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = primary-mm2
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:49:44,063 - INFO  [kafka-producer-network-thread | producer-18:Metadata@279] - [Producer clientId=producer-18] Cluster ID: l67hsn_BQke0DtmYXtLNog
2023-07-30 13:49:44,065 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:44,065 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:49:44,065 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:44,065 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:44,065 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:44,065 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:44,065 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:44,065 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:44,065 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:44,065 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:44,065 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:44,065 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:44,065 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:44,065 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:44,065 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:44,065 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739384065
2023-07-30 13:49:44,065 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-34, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,066 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-34 in /tmp/junit7660765717748375067/junit866559250785637651/__consumer_offsets-34 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,066 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-34 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-34
2023-07-30 13:49:44,066 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-34 broker=0] Log loaded for partition __consumer_offsets-34 with initial high watermark 0
2023-07-30 13:49:44,067 - INFO  [DistributedHerder-connect-6-1:Metadata@279] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Cluster ID: l67hsn_BQke0DtmYXtLNog
2023-07-30 13:49:44,069 - INFO  [DistributedHerder-connect-6-1:KafkaConsumer@1116] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Subscribed to partition(s): mm2-offsets.primary.internal-0, mm2-offsets.primary.internal-5, mm2-offsets.primary.internal-10, mm2-offsets.primary.internal-20, mm2-offsets.primary.internal-15, mm2-offsets.primary.internal-9, mm2-offsets.primary.internal-11, mm2-offsets.primary.internal-16, mm2-offsets.primary.internal-4, mm2-offsets.primary.internal-17, mm2-offsets.primary.internal-3, mm2-offsets.primary.internal-24, mm2-offsets.primary.internal-23, mm2-offsets.primary.internal-13, mm2-offsets.primary.internal-18, mm2-offsets.primary.internal-22, mm2-offsets.primary.internal-2, mm2-offsets.primary.internal-8, mm2-offsets.primary.internal-12, mm2-offsets.primary.internal-19, mm2-offsets.primary.internal-14, mm2-offsets.primary.internal-1, mm2-offsets.primary.internal-6, mm2-offsets.primary.internal-7, mm2-offsets.primary.internal-21
2023-07-30 13:49:44,070 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-0
2023-07-30 13:49:44,070 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-5
2023-07-30 13:49:44,070 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-10
2023-07-30 13:49:44,070 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-20
2023-07-30 13:49:44,070 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-15
2023-07-30 13:49:44,070 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-9
2023-07-30 13:49:44,070 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-11
2023-07-30 13:49:44,070 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-16
2023-07-30 13:49:44,070 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-4
2023-07-30 13:49:44,070 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-17
2023-07-30 13:49:44,070 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-3
2023-07-30 13:49:44,070 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-24
2023-07-30 13:49:44,070 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-23
2023-07-30 13:49:44,070 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-13
2023-07-30 13:49:44,070 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-18
2023-07-30 13:49:44,070 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-22
2023-07-30 13:49:44,070 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-2
2023-07-30 13:49:44,070 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-8
2023-07-30 13:49:44,070 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-12
2023-07-30 13:49:44,070 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-19
2023-07-30 13:49:44,070 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-14
2023-07-30 13:49:44,070 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-1
2023-07-30 13:49:44,071 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-6
2023-07-30 13:49:44,071 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-7
2023-07-30 13:49:44,071 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-offsets.primary.internal-21
2023-07-30 13:49:44,074 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-4, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,074 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:44,075 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:44,075 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-4 in /tmp/junit7660765717748375067/junit866559250785637651/__consumer_offsets-4 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,075 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-23 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:44,075 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4
2023-07-30 13:49:44,075 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-21 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:44,075 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-4 broker=0] Log loaded for partition __consumer_offsets-4 with initial high watermark 0
2023-07-30 13:49:44,075 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-18 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:44,075 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-16 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:44,075 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-14 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:44,075 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-12 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:44,075 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-10 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:44,076 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-8 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:44,076 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-6 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:44,076 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:44,076 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:44,076 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:44,076 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-24 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:44,076 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-22 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:44,076 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-20 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:44,076 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-19 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:44,076 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-17 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:44,076 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-15 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:44,076 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-13 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:44,076 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-11 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:44,076 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-9 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:44,077 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-7 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:44,077 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Resetting offset for partition mm2-offsets.primary.internal-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:44,077 - INFO  [DistributedHerder-connect-6-1:KafkaBasedLog@161] - Finished reading KafkaBasedLog for topic mm2-offsets.primary.internal
2023-07-30 13:49:44,077 - INFO  [DistributedHerder-connect-6-1:KafkaBasedLog@163] - Started KafkaBasedLog for topic mm2-offsets.primary.internal
2023-07-30 13:49:44,077 - INFO  [DistributedHerder-connect-6-1:KafkaOffsetBackingStore@129] - Finished reading offsets topic and starting KafkaOffsetBackingStore
2023-07-30 13:49:44,078 - INFO  [DistributedHerder-connect-6-1:Worker@202] - Worker started
2023-07-30 13:49:44,078 - INFO  [DistributedHerder-connect-6-1:KafkaBasedLog@127] - Starting KafkaBasedLog with topic mm2-status.primary.internal
2023-07-30 13:49:44,078 - INFO  [DistributedHerder-connect-6-1:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:45395]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:44,081 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:44,081 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:49:44,081 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:44,081 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:44,081 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:44,081 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:44,081 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:44,081 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:44,081 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:44,081 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:44,081 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:44,081 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:44,082 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:44,082 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:44,082 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:44,082 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:44,082 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739384082
2023-07-30 13:49:44,082 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-11, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,083 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-11 in /tmp/junit7660765717748375067/junit866559250785637651/__consumer_offsets-11 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,083 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-11 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-11
2023-07-30 13:49:44,083 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-11 broker=0] Log loaded for partition __consumer_offsets-11 with initial high watermark 0
2023-07-30 13:49:44,088 - INFO  [kafka-admin-client-thread | adminclient-59:AppInfoParser@83] - App info kafka.admin.client for adminclient-59 unregistered
2023-07-30 13:49:44,088 - INFO  [kafka-admin-client-thread | adminclient-59:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:44,088 - INFO  [kafka-admin-client-thread | adminclient-59:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:44,088 - INFO  [kafka-admin-client-thread | adminclient-59:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:44,089 - INFO  [DistributedHerder-connect-6-1:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:45395]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-19
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:49:44,090 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-26, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,090 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:44,091 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
Jul 30, 2023 1:49:44 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource will be ignored. 
2023-07-30 13:49:44,091 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:44,091 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:44,091 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:44,091 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:44,091 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:44,091 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
Jul 30, 2023 1:49:44 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource will be ignored. 
2023-07-30 13:49:44,091 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:44,091 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:44,091 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
Jul 30, 2023 1:49:44 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource will be ignored. 
2023-07-30 13:49:44,091 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:44,091 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-26 in /tmp/junit7660765717748375067/junit866559250785637651/__consumer_offsets-26 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
Jul 30, 2023 1:49:44 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.RootResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.RootResource will be ignored. 
2023-07-30 13:49:44,091 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:44,092 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:44,091 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-26 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-26
2023-07-30 13:49:44,092 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:44,092 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-26 broker=0] Log loaded for partition __consumer_offsets-26 with initial high watermark 0
2023-07-30 13:49:44,092 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:44,092 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739384092
2023-07-30 13:49:44,092 - INFO  [kafka-producer-network-thread | producer-19:Metadata@279] - [Producer clientId=producer-19] Cluster ID: l67hsn_BQke0DtmYXtLNog
2023-07-30 13:49:44,092 - INFO  [DistributedHerder-connect-6-1:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:45395]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-primary-mm2-17
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = primary-mm2
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:49:44,094 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:44,094 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:49:44,094 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:44,094 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:44,094 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:44,094 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:44,094 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:44,094 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:44,094 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:44,094 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:44,094 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:44,094 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:44,094 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:44,094 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:44,094 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:44,094 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739384094
2023-07-30 13:49:44,096 - INFO  [DistributedHerder-connect-6-1:Metadata@279] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Cluster ID: l67hsn_BQke0DtmYXtLNog
2023-07-30 13:49:44,099 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-49, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,100 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-49 in /tmp/junit7660765717748375067/junit866559250785637651/__consumer_offsets-49 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,100 - INFO  [DistributedHerder-connect-6-1:KafkaConsumer@1116] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Subscribed to partition(s): mm2-status.primary.internal-0, mm2-status.primary.internal-1, mm2-status.primary.internal-4, mm2-status.primary.internal-2, mm2-status.primary.internal-3
2023-07-30 13:49:44,100 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-49 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-49
2023-07-30 13:49:44,100 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-status.primary.internal-0
2023-07-30 13:49:44,100 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-49 broker=0] Log loaded for partition __consumer_offsets-49 with initial high watermark 0
2023-07-30 13:49:44,100 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-status.primary.internal-1
2023-07-30 13:49:44,100 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-status.primary.internal-4
2023-07-30 13:49:44,100 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-status.primary.internal-2
2023-07-30 13:49:44,100 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-status.primary.internal-3
2023-07-30 13:49:44,104 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Resetting offset for partition mm2-status.primary.internal-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:44,104 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Resetting offset for partition mm2-status.primary.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:44,104 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Resetting offset for partition mm2-status.primary.internal-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:44,104 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Resetting offset for partition mm2-status.primary.internal-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:44,105 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Resetting offset for partition mm2-status.primary.internal-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:44,105 - INFO  [DistributedHerder-connect-6-1:KafkaBasedLog@161] - Finished reading KafkaBasedLog for topic mm2-status.primary.internal
2023-07-30 13:49:44,105 - INFO  [DistributedHerder-connect-6-1:KafkaBasedLog@163] - Started KafkaBasedLog for topic mm2-status.primary.internal
2023-07-30 13:49:44,106 - INFO  [DistributedHerder-connect-6-1:KafkaConfigBackingStore@265] - Starting KafkaConfigBackingStore
2023-07-30 13:49:44,107 - INFO  [DistributedHerder-connect-6-1:KafkaBasedLog@127] - Starting KafkaBasedLog with topic mm2-configs.primary.internal
2023-07-30 13:49:44,107 - INFO  [DistributedHerder-connect-6-1:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:45395]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:44,110 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:44,110 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:49:44,110 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:44,110 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:44,110 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:44,110 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:44,110 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:44,110 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:44,110 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:44,110 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:44,110 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:44,110 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:44,110 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:44,110 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:44,110 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-39, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,110 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:44,110 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:44,110 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739384110
2023-07-30 13:49:44,111 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-39 in /tmp/junit7660765717748375067/junit866559250785637651/__consumer_offsets-39 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,111 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-39 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-39
2023-07-30 13:49:44,111 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-39 broker=0] Log loaded for partition __consumer_offsets-39 with initial high watermark 0
Jul 30, 2023 1:49:44 PM org.glassfish.jersey.internal.Errors logErrors
WARNING: The following warnings have been detected: WARNING: The (sub)resource method listLoggers in org.apache.kafka.connect.runtime.rest.resources.LoggingResource contains empty path annotation.
WARNING: The (sub)resource method createConnector in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectors in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
WARNING: The (sub)resource method listConnectorPlugins in org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource contains empty path annotation.
WARNING: The (sub)resource method serverInfo in org.apache.kafka.connect.runtime.rest.resources.RootResource contains empty path annotation.

2023-07-30 13:49:44,115 - INFO  [main:ContextHandler@849] - Started o.e.j.s.ServletContextHandler@36f7d7b{/,null,AVAILABLE}
2023-07-30 13:49:44,115 - INFO  [main:RestServer@319] - REST resources initialized; server is started and ready to handle requests
2023-07-30 13:49:44,115 - INFO  [main:Connect@57] - Kafka Connect started
2023-07-30 13:49:44,115 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-9, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,115 - INFO  [main:EmbeddedConnectCluster@169] - Started worker WorkerHandle{workerName='connect-worker-2'workerURL='http://localhost:41733/'}
2023-07-30 13:49:44,116 - INFO  [kafka-admin-client-thread | adminclient-60:AppInfoParser@83] - App info kafka.admin.client for adminclient-60 unregistered
2023-07-30 13:49:44,116 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-9 in /tmp/junit7660765717748375067/junit866559250785637651/__consumer_offsets-9 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,116 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-9 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-9
2023-07-30 13:49:44,116 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-9 broker=0] Log loaded for partition __consumer_offsets-9 with initial high watermark 0
2023-07-30 13:49:44,117 - INFO  [kafka-admin-client-thread | adminclient-60:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:44,117 - INFO  [kafka-admin-client-thread | adminclient-60:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:44,117 - INFO  [kafka-admin-client-thread | adminclient-60:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:44,117 - INFO  [DistributedHerder-connect-6-1:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:45395]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-20
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:49:44,119 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:44,120 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:49:44,120 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'group.id' was supplied but isn't a known config.
2023-07-30 13:49:44,120 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:44,120 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:44,120 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:44,120 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:44,120 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:44,120 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:44,120 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:44,121 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:44,121 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:44,121 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:44,121 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:44,121 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:44,121 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:44,121 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739384121
2023-07-30 13:49:44,121 - INFO  [DistributedHerder-connect-6-1:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:45395]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-primary-mm2-18
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = primary-mm2
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:49:44,121 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:43083/ is {"version":"unknown","commit":"unknown","kafka_cluster_id":"XnnaS7BkQ22Tt1TiwnBd0Q"}
2023-07-30 13:49:44,122 - INFO  [kafka-producer-network-thread | producer-20:Metadata@279] - [Producer clientId=producer-20] Cluster ID: l67hsn_BQke0DtmYXtLNog
2023-07-30 13:49:44,127 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'config.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:44,127 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:49:44,127 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'status.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:44,127 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'rest.host.name' was supplied but isn't a known config.
2023-07-30 13:49:44,127 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'header.converter' was supplied but isn't a known config.
2023-07-30 13:49:44,127 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'config.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:44,127 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'rest.port' was supplied but isn't a known config.
2023-07-30 13:49:44,128 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:44,128 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'status.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:44,128 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'offset.storage.replication.factor' was supplied but isn't a known config.
2023-07-30 13:49:44,128 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'offset.storage.topic' was supplied but isn't a known config.
2023-07-30 13:49:44,128 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'value.converter' was supplied but isn't a known config.
2023-07-30 13:49:44,128 - WARN  [DistributedHerder-connect-6-1:AbstractConfig@369] - The configuration 'key.converter' was supplied but isn't a known config.
2023-07-30 13:49:44,128 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:44,128 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:44,128 - INFO  [DistributedHerder-connect-6-1:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739384128
2023-07-30 13:49:44,129 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:33591/ is {"version":"unknown","commit":"unknown","kafka_cluster_id":"XnnaS7BkQ22Tt1TiwnBd0Q"}
2023-07-30 13:49:44,129 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-24, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,134 - INFO  [DistributedHerder-connect-6-1:Metadata@279] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Cluster ID: l67hsn_BQke0DtmYXtLNog
2023-07-30 13:49:44,135 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-24 in /tmp/junit7660765717748375067/junit866559250785637651/__consumer_offsets-24 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,135 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-24 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-24
2023-07-30 13:49:44,135 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-24 broker=0] Log loaded for partition __consumer_offsets-24 with initial high watermark 0
2023-07-30 13:49:44,137 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:37073/ is {"version":"unknown","commit":"unknown","kafka_cluster_id":"XnnaS7BkQ22Tt1TiwnBd0Q"}
2023-07-30 13:49:44,137 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:35687]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:44,138 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:44,138 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:44,138 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739384138
2023-07-30 13:49:44,139 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-31, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,140 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-31 in /tmp/junit7660765717748375067/junit866559250785637651/__consumer_offsets-31 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,140 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-31 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-31
2023-07-30 13:49:44,140 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-31 broker=0] Log loaded for partition __consumer_offsets-31 with initial high watermark 0
2023-07-30 13:49:44,141 - INFO  [DistributedHerder-connect-6-1:KafkaConsumer@1116] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Subscribed to partition(s): mm2-configs.primary.internal-0
2023-07-30 13:49:44,141 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@618] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Seeking to EARLIEST offset of partition mm2-configs.primary.internal-0
2023-07-30 13:49:44,143 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Creating topic test-topic-1 with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0), 5 -> ArrayBuffer(0), 6 -> ArrayBuffer(0), 7 -> ArrayBuffer(0), 8 -> ArrayBuffer(0), 9 -> ArrayBuffer(0))
2023-07-30 13:49:44,144 - INFO  [DistributedHerder-connect-6-1:SubscriptionState@396] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Resetting offset for partition mm2-configs.primary.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:44,144 - INFO  [DistributedHerder-connect-6-1:KafkaBasedLog@161] - Finished reading KafkaBasedLog for topic mm2-configs.primary.internal
2023-07-30 13:49:44,145 - INFO  [DistributedHerder-connect-6-1:KafkaBasedLog@163] - Started KafkaBasedLog for topic mm2-configs.primary.internal
2023-07-30 13:49:44,145 - INFO  [DistributedHerder-connect-6-1:KafkaConfigBackingStore@280] - Started KafkaConfigBackingStore
2023-07-30 13:49:44,145 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@290] - [Worker clientId=connect-6, groupId=primary-mm2] Herder started
2023-07-30 13:49:44,150 - INFO  [DistributedHerder-connect-6-1:Metadata@279] - [Worker clientId=connect-6, groupId=primary-mm2] Cluster ID: l67hsn_BQke0DtmYXtLNog
2023-07-30 13:49:44,150 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-46, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,151 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-46 in /tmp/junit7660765717748375067/junit866559250785637651/__consumer_offsets-46 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,151 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-46 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-46
2023-07-30 13:49:44,151 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-46 broker=0] Log loaded for partition __consumer_offsets-46 with initial high watermark 0
2023-07-30 13:49:44,154 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-1, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,155 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-1 in /tmp/junit7660765717748375067/junit866559250785637651/__consumer_offsets-1 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,155 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1
2023-07-30 13:49:44,155 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-1 broker=0] Log loaded for partition __consumer_offsets-1 with initial high watermark 0
2023-07-30 13:49:44,158 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(test-topic-1-0, test-topic-1-3, test-topic-1-9, test-topic-1-5, test-topic-1-1, test-topic-1-6, test-topic-1-2, test-topic-1-8, test-topic-1-7, test-topic-1-4)
2023-07-30 13:49:44,160 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=test-topic-1-0, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,160 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition test-topic-1-0 in /tmp/junit6293896813403981889/junit8934247217114872089/test-topic-1-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,161 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-1-0 broker=0] No checkpointed highwatermark is found for partition test-topic-1-0
2023-07-30 13:49:44,161 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-1-0 broker=0] Log loaded for partition test-topic-1-0 with initial high watermark 0
2023-07-30 13:49:44,163 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-16, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,163 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-16 in /tmp/junit7660765717748375067/junit866559250785637651/__consumer_offsets-16 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,164 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-16 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-16
2023-07-30 13:49:44,164 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-16 broker=0] Log loaded for partition __consumer_offsets-16 with initial high watermark 0
2023-07-30 13:49:44,171 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=test-topic-1-9, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,171 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-2, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,171 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition test-topic-1-9 in /tmp/junit6293896813403981889/junit8934247217114872089/test-topic-1-9 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,171 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-1-9 broker=0] No checkpointed highwatermark is found for partition test-topic-1-9
2023-07-30 13:49:44,171 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-1-9 broker=0] Log loaded for partition test-topic-1-9 with initial high watermark 0
2023-07-30 13:49:44,171 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-2 in /tmp/junit7660765717748375067/junit866559250785637651/__consumer_offsets-2 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,172 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2
2023-07-30 13:49:44,172 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-2 broker=0] Log loaded for partition __consumer_offsets-2 with initial high watermark 0
2023-07-30 13:49:44,179 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-25, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,179 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=test-topic-1-7, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,179 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-25 in /tmp/junit7660765717748375067/junit866559250785637651/__consumer_offsets-25 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,179 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-25 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-25
2023-07-30 13:49:44,179 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-25 broker=0] Log loaded for partition __consumer_offsets-25 with initial high watermark 0
2023-07-30 13:49:44,179 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition test-topic-1-7 in /tmp/junit6293896813403981889/junit8934247217114872089/test-topic-1-7 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,180 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-1-7 broker=0] No checkpointed highwatermark is found for partition test-topic-1-7
2023-07-30 13:49:44,180 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-1-7 broker=0] Log loaded for partition test-topic-1-7 with initial high watermark 0
2023-07-30 13:49:44,187 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-40, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,187 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=test-topic-1-8, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,188 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-40 in /tmp/junit7660765717748375067/junit866559250785637651/__consumer_offsets-40 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,188 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-40 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-40
2023-07-30 13:49:44,188 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition test-topic-1-8 in /tmp/junit6293896813403981889/junit8934247217114872089/test-topic-1-8 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,188 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-40 broker=0] Log loaded for partition __consumer_offsets-40 with initial high watermark 0
2023-07-30 13:49:44,188 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-1-8 broker=0] No checkpointed highwatermark is found for partition test-topic-1-8
2023-07-30 13:49:44,188 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-1-8 broker=0] Log loaded for partition test-topic-1-8 with initial high watermark 0
2023-07-30 13:49:44,196 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=test-topic-1-5, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,196 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-47, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,196 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition test-topic-1-5 in /tmp/junit6293896813403981889/junit8934247217114872089/test-topic-1-5 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,196 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-47 in /tmp/junit7660765717748375067/junit866559250785637651/__consumer_offsets-47 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,196 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-1-5 broker=0] No checkpointed highwatermark is found for partition test-topic-1-5
2023-07-30 13:49:44,196 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-47 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-47
2023-07-30 13:49:44,196 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-1-5 broker=0] Log loaded for partition test-topic-1-5 with initial high watermark 0
2023-07-30 13:49:44,196 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-47 broker=0] Log loaded for partition __consumer_offsets-47 with initial high watermark 0
2023-07-30 13:49:44,204 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-17, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,204 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=test-topic-1-6, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,205 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-17 in /tmp/junit7660765717748375067/junit866559250785637651/__consumer_offsets-17 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,205 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-17 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-17
2023-07-30 13:49:44,205 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition test-topic-1-6 in /tmp/junit6293896813403981889/junit8934247217114872089/test-topic-1-6 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,205 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-17 broker=0] Log loaded for partition __consumer_offsets-17 with initial high watermark 0
2023-07-30 13:49:44,205 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-1-6 broker=0] No checkpointed highwatermark is found for partition test-topic-1-6
2023-07-30 13:49:44,205 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-1-6 broker=0] Log loaded for partition test-topic-1-6 with initial high watermark 0
2023-07-30 13:49:44,213 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=test-topic-1-3, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,213 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-32, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,213 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition test-topic-1-3 in /tmp/junit6293896813403981889/junit8934247217114872089/test-topic-1-3 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,213 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-1-3 broker=0] No checkpointed highwatermark is found for partition test-topic-1-3
2023-07-30 13:49:44,213 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-32 in /tmp/junit7660765717748375067/junit866559250785637651/__consumer_offsets-32 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,213 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-1-3 broker=0] Log loaded for partition test-topic-1-3 with initial high watermark 0
2023-07-30 13:49:44,213 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-32 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-32
2023-07-30 13:49:44,213 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-32 broker=0] Log loaded for partition __consumer_offsets-32 with initial high watermark 0
2023-07-30 13:49:44,221 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-37, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,221 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=test-topic-1-4, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,221 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-37 in /tmp/junit7660765717748375067/junit866559250785637651/__consumer_offsets-37 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,222 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-37 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-37
2023-07-30 13:49:44,222 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition test-topic-1-4 in /tmp/junit6293896813403981889/junit8934247217114872089/test-topic-1-4 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,222 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-37 broker=0] Log loaded for partition __consumer_offsets-37 with initial high watermark 0
2023-07-30 13:49:44,222 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-1-4 broker=0] No checkpointed highwatermark is found for partition test-topic-1-4
2023-07-30 13:49:44,222 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-1-4 broker=0] Log loaded for partition test-topic-1-4 with initial high watermark 0
2023-07-30 13:49:44,231 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-7, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,231 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=test-topic-1-1, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,231 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-7 in /tmp/junit7660765717748375067/junit866559250785637651/__consumer_offsets-7 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,231 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-7 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-7
2023-07-30 13:49:44,231 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-7 broker=0] Log loaded for partition __consumer_offsets-7 with initial high watermark 0
2023-07-30 13:49:44,232 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition test-topic-1-1 in /tmp/junit6293896813403981889/junit8934247217114872089/test-topic-1-1 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,232 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-1-1 broker=0] No checkpointed highwatermark is found for partition test-topic-1-1
2023-07-30 13:49:44,232 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-1-1 broker=0] Log loaded for partition test-topic-1-1 with initial high watermark 0
2023-07-30 13:49:44,238 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-22, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,238 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=test-topic-1-2, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,238 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-22 in /tmp/junit7660765717748375067/junit866559250785637651/__consumer_offsets-22 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,238 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-22 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-22
2023-07-30 13:49:44,238 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-22 broker=0] Log loaded for partition __consumer_offsets-22 with initial high watermark 0
2023-07-30 13:49:44,238 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition test-topic-1-2 in /tmp/junit6293896813403981889/junit8934247217114872089/test-topic-1-2 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,238 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-1-2 broker=0] No checkpointed highwatermark is found for partition test-topic-1-2
2023-07-30 13:49:44,238 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-1-2 broker=0] Log loaded for partition test-topic-1-2 with initial high watermark 0
2023-07-30 13:49:44,246 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-29, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,247 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-29 in /tmp/junit7660765717748375067/junit866559250785637651/__consumer_offsets-29 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,247 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-29 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-29
2023-07-30 13:49:44,247 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-29 broker=0] Log loaded for partition __consumer_offsets-29 with initial high watermark 0
2023-07-30 13:49:44,248 - INFO  [kafka-admin-client-thread | adminclient-61:AppInfoParser@83] - App info kafka.admin.client for adminclient-61 unregistered
2023-07-30 13:49:44,248 - INFO  [kafka-admin-client-thread | adminclient-61:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:44,248 - INFO  [kafka-admin-client-thread | adminclient-61:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:44,249 - INFO  [kafka-admin-client-thread | adminclient-61:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:44,249 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:35687]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:44,250 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:44,250 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:44,250 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739384250
2023-07-30 13:49:44,253 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Creating topic backup.test-topic-1 with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0))
2023-07-30 13:49:44,259 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(backup.test-topic-1-0)
2023-07-30 13:49:44,290 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-44, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,290 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Log partition=backup.test-topic-1-0, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,291 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-44 in /tmp/junit7660765717748375067/junit866559250785637651/__consumer_offsets-44 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,291 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-44 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-44
2023-07-30 13:49:44,291 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-44 broker=0] Log loaded for partition __consumer_offsets-44 with initial high watermark 0
2023-07-30 13:49:44,291 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Created log for partition backup.test-topic-1-0 in /tmp/junit6293896813403981889/junit8934247217114872089/backup.test-topic-1-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,291 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition backup.test-topic-1-0 broker=0] No checkpointed highwatermark is found for partition backup.test-topic-1-0
2023-07-30 13:49:44,292 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [Partition backup.test-topic-1-0 broker=0] Log loaded for partition backup.test-topic-1-0 with initial high watermark 0
2023-07-30 13:49:44,293 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-14, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,293 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-14 in /tmp/junit7660765717748375067/junit866559250785637651/__consumer_offsets-14 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,294 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-14 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-14
2023-07-30 13:49:44,294 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-14 broker=0] Log loaded for partition __consumer_offsets-14 with initial high watermark 0
2023-07-30 13:49:44,302 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-23, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,303 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-23 in /tmp/junit7660765717748375067/junit866559250785637651/__consumer_offsets-23 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,303 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-23 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-23
2023-07-30 13:49:44,303 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-23 broker=0] Log loaded for partition __consumer_offsets-23 with initial high watermark 0
2023-07-30 13:49:44,303 - INFO  [kafka-admin-client-thread | adminclient-62:AppInfoParser@83] - App info kafka.admin.client for adminclient-62 unregistered
2023-07-30 13:49:44,304 - INFO  [kafka-admin-client-thread | adminclient-62:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:44,304 - INFO  [kafka-admin-client-thread | adminclient-62:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:44,304 - INFO  [kafka-admin-client-thread | adminclient-62:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:44,305 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:35687]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:44,305 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:44,305 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:44,306 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739384305
2023-07-30 13:49:44,309 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Creating topic heartbeats with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0))
2023-07-30 13:49:44,310 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-38, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,311 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-38 in /tmp/junit7660765717748375067/junit866559250785637651/__consumer_offsets-38 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,311 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-38 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-38
2023-07-30 13:49:44,311 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-38 broker=0] Log loaded for partition __consumer_offsets-38 with initial high watermark 0
2023-07-30 13:49:44,313 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(heartbeats-0)
2023-07-30 13:49:44,315 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Log partition=heartbeats-0, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,315 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Created log for partition heartbeats-0 in /tmp/junit6293896813403981889/junit8934247217114872089/heartbeats-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,316 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition heartbeats-0 broker=0] No checkpointed highwatermark is found for partition heartbeats-0
2023-07-30 13:49:44,316 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition heartbeats-0 broker=0] Log loaded for partition heartbeats-0 with initial high watermark 0
2023-07-30 13:49:44,318 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-8, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,319 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-8 in /tmp/junit7660765717748375067/junit866559250785637651/__consumer_offsets-8 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,320 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-8 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-8
2023-07-30 13:49:44,320 - INFO  [kafka-admin-client-thread | adminclient-63:AppInfoParser@83] - App info kafka.admin.client for adminclient-63 unregistered
2023-07-30 13:49:44,320 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-8 broker=0] Log loaded for partition __consumer_offsets-8 with initial high watermark 0
2023-07-30 13:49:44,320 - INFO  [kafka-admin-client-thread | adminclient-63:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:44,320 - INFO  [kafka-admin-client-thread | adminclient-63:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:44,320 - INFO  [kafka-admin-client-thread | adminclient-63:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:44,321 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:45395]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:44,321 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:44,322 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:44,322 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739384321
2023-07-30 13:49:44,325 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Creating topic test-topic-1 with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0), 5 -> ArrayBuffer(0), 6 -> ArrayBuffer(0), 7 -> ArrayBuffer(0), 8 -> ArrayBuffer(0), 9 -> ArrayBuffer(0))
2023-07-30 13:49:44,327 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-45, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,327 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-45 in /tmp/junit7660765717748375067/junit866559250785637651/__consumer_offsets-45 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,327 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-45 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-45
2023-07-30 13:49:44,327 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-45 broker=0] Log loaded for partition __consumer_offsets-45 with initial high watermark 0
2023-07-30 13:49:44,335 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-15, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,335 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-15 in /tmp/junit7660765717748375067/junit866559250785637651/__consumer_offsets-15 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,335 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-15 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-15
2023-07-30 13:49:44,335 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-15 broker=0] Log loaded for partition __consumer_offsets-15 with initial high watermark 0
2023-07-30 13:49:44,343 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-30, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,343 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-30 in /tmp/junit7660765717748375067/junit866559250785637651/__consumer_offsets-30 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,344 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-30 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-30
2023-07-30 13:49:44,344 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-30 broker=0] Log loaded for partition __consumer_offsets-30 with initial high watermark 0
2023-07-30 13:49:44,352 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-0, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,354 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-0 in /tmp/junit7660765717748375067/junit866559250785637651/__consumer_offsets-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,354 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0
2023-07-30 13:49:44,354 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-0 broker=0] Log loaded for partition __consumer_offsets-0 with initial high watermark 0
2023-07-30 13:49:44,360 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-35, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,360 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-35 in /tmp/junit7660765717748375067/junit866559250785637651/__consumer_offsets-35 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,360 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-35 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-35
2023-07-30 13:49:44,360 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-35 broker=0] Log loaded for partition __consumer_offsets-35 with initial high watermark 0
2023-07-30 13:49:44,368 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-5, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,369 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-5 in /tmp/junit7660765717748375067/junit866559250785637651/__consumer_offsets-5 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,369 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-5 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-5
2023-07-30 13:49:44,369 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-5 broker=0] Log loaded for partition __consumer_offsets-5 with initial high watermark 0
2023-07-30 13:49:44,377 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-20, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,377 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-20 in /tmp/junit7660765717748375067/junit866559250785637651/__consumer_offsets-20 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,377 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-20 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-20
2023-07-30 13:49:44,377 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-20 broker=0] Log loaded for partition __consumer_offsets-20 with initial high watermark 0
2023-07-30 13:49:44,385 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-27, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,385 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-27 in /tmp/junit7660765717748375067/junit866559250785637651/__consumer_offsets-27 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,385 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-27 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-27
2023-07-30 13:49:44,385 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-27 broker=0] Log loaded for partition __consumer_offsets-27 with initial high watermark 0
2023-07-30 13:49:44,393 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-42, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,394 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-42 in /tmp/junit7660765717748375067/junit866559250785637651/__consumer_offsets-42 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,394 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-42 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-42
2023-07-30 13:49:44,394 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-42 broker=0] Log loaded for partition __consumer_offsets-42 with initial high watermark 0
2023-07-30 13:49:44,402 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-12, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,402 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-12 in /tmp/junit7660765717748375067/junit866559250785637651/__consumer_offsets-12 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,402 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-12 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-12
2023-07-30 13:49:44,402 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-12 broker=0] Log loaded for partition __consumer_offsets-12 with initial high watermark 0
2023-07-30 13:49:44,410 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-21, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,411 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-21 in /tmp/junit7660765717748375067/junit866559250785637651/__consumer_offsets-21 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,411 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-21 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-21
2023-07-30 13:49:44,411 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-21 broker=0] Log loaded for partition __consumer_offsets-21 with initial high watermark 0
2023-07-30 13:49:44,419 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-36, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,419 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-36 in /tmp/junit7660765717748375067/junit866559250785637651/__consumer_offsets-36 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,419 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-36 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-36
2023-07-30 13:49:44,419 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-36 broker=0] Log loaded for partition __consumer_offsets-36 with initial high watermark 0
2023-07-30 13:49:44,426 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-6, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,427 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-6 in /tmp/junit7660765717748375067/junit866559250785637651/__consumer_offsets-6 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,427 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-6 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-6
2023-07-30 13:49:44,427 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-6 broker=0] Log loaded for partition __consumer_offsets-6 with initial high watermark 0
2023-07-30 13:49:44,435 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-43, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,439 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-43 in /tmp/junit7660765717748375067/junit866559250785637651/__consumer_offsets-43 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,439 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-43 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-43
2023-07-30 13:49:44,439 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-43 broker=0] Log loaded for partition __consumer_offsets-43 with initial high watermark 0
2023-07-30 13:49:44,442 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-13, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,442 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-13 in /tmp/junit7660765717748375067/junit866559250785637651/__consumer_offsets-13 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,442 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-13 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-13
2023-07-30 13:49:44,442 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-13 broker=0] Log loaded for partition __consumer_offsets-13 with initial high watermark 0
2023-07-30 13:49:44,450 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=__consumer_offsets-28, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,450 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition __consumer_offsets-28 in /tmp/junit7660765717748375067/junit866559250785637651/__consumer_offsets-28 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,450 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-28 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-28
2023-07-30 13:49:44,450 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition __consumer_offsets-28 broker=0] Log loaded for partition __consumer_offsets-28 with initial high watermark 0
2023-07-30 13:49:44,457 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3
2023-07-30 13:49:44,458 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-18
2023-07-30 13:49:44,458 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-41
2023-07-30 13:49:44,458 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-10
2023-07-30 13:49:44,458 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:49:44,458 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-33
2023-07-30 13:49:44,458 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-48
2023-07-30 13:49:44,458 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-18 in 0 milliseconds, of which 0 milliseconds was spent in the scheduler.
2023-07-30 13:49:44,458 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-19
2023-07-30 13:49:44,458 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-34
2023-07-30 13:49:44,458 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-41 in 0 milliseconds, of which 0 milliseconds was spent in the scheduler.
2023-07-30 13:49:44,458 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4
2023-07-30 13:49:44,458 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-11
2023-07-30 13:49:44,459 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-10 in 1 milliseconds, of which 0 milliseconds was spent in the scheduler.
2023-07-30 13:49:44,459 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-26
2023-07-30 13:49:44,459 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-33 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:49:44,459 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-49
2023-07-30 13:49:44,459 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-48 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:49:44,459 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-39
2023-07-30 13:49:44,459 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-19 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:49:44,459 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-9
2023-07-30 13:49:44,459 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-34 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:49:44,459 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-24
2023-07-30 13:49:44,459 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:49:44,459 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-31
2023-07-30 13:49:44,459 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-11 in 0 milliseconds, of which 0 milliseconds was spent in the scheduler.
2023-07-30 13:49:44,459 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-46
2023-07-30 13:49:44,459 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-26 in 0 milliseconds, of which 0 milliseconds was spent in the scheduler.
2023-07-30 13:49:44,459 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1
2023-07-30 13:49:44,459 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-16
2023-07-30 13:49:44,459 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-49 in 0 milliseconds, of which 0 milliseconds was spent in the scheduler.
2023-07-30 13:49:44,459 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2
2023-07-30 13:49:44,459 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-25
2023-07-30 13:49:44,460 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-39 in 0 milliseconds, of which 0 milliseconds was spent in the scheduler.
2023-07-30 13:49:44,460 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-40
2023-07-30 13:49:44,460 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-9 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:49:44,460 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-47
2023-07-30 13:49:44,460 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-24 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:49:44,460 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-17
2023-07-30 13:49:44,460 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-31 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:49:44,460 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-32
2023-07-30 13:49:44,460 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-46 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:49:44,460 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-37
2023-07-30 13:49:44,460 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:49:44,460 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-7
2023-07-30 13:49:44,460 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-16 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:49:44,460 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-22
2023-07-30 13:49:44,460 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:49:44,460 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-29
2023-07-30 13:49:44,460 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-25 in 0 milliseconds, of which 0 milliseconds was spent in the scheduler.
2023-07-30 13:49:44,460 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-44
2023-07-30 13:49:44,460 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-40 in 0 milliseconds, of which 0 milliseconds was spent in the scheduler.
2023-07-30 13:49:44,460 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-14
2023-07-30 13:49:44,460 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-47 in 0 milliseconds, of which 0 milliseconds was spent in the scheduler.
2023-07-30 13:49:44,460 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-23
2023-07-30 13:49:44,461 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-17 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:49:44,461 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-38
2023-07-30 13:49:44,461 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-32 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:49:44,461 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-8
2023-07-30 13:49:44,461 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-37 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:49:44,461 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-45
2023-07-30 13:49:44,461 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-7 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:49:44,461 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-15
2023-07-30 13:49:44,461 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-22 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:49:44,461 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-30
2023-07-30 13:49:44,461 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0
2023-07-30 13:49:44,461 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-29 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:49:44,461 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-35
2023-07-30 13:49:44,461 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-5
2023-07-30 13:49:44,461 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-44 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:49:44,461 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-20
2023-07-30 13:49:44,461 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-14 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:49:44,461 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-27
2023-07-30 13:49:44,461 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-42
2023-07-30 13:49:44,461 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-12
2023-07-30 13:49:44,462 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-23 in 0 milliseconds, of which 0 milliseconds was spent in the scheduler.
2023-07-30 13:49:44,462 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-21
2023-07-30 13:49:44,462 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-36
2023-07-30 13:49:44,462 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-38 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:49:44,462 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-6
2023-07-30 13:49:44,462 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-8 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:49:44,462 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-43
2023-07-30 13:49:44,462 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-45 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:49:44,462 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-13
2023-07-30 13:49:44,462 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-15 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:49:44,462 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-28
2023-07-30 13:49:44,462 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-30 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:49:44,462 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:49:44,462 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-35 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:49:44,462 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-5 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:49:44,462 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-20 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:49:44,462 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-27 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:49:44,463 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-42 in 2 milliseconds, of which 2 milliseconds was spent in the scheduler.
2023-07-30 13:49:44,463 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-12 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:49:44,463 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-21 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:49:44,463 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-36 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:49:44,463 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-6 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:49:44,463 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-43 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:49:44,463 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-13 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:49:44,463 - INFO  [group-metadata-manager-0:Logging@66] - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-28 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
2023-07-30 13:49:44,467 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(test-topic-1-0, test-topic-1-3, test-topic-1-9, test-topic-1-5, test-topic-1-1, test-topic-1-6, test-topic-1-2, test-topic-1-8, test-topic-1-7, test-topic-1-4)
2023-07-30 13:49:44,468 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Log partition=test-topic-1-0, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,469 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Created log for partition test-topic-1-0 in /tmp/junit7660765717748375067/junit866559250785637651/test-topic-1-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,469 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$FindCoordinatorResponseHandler@841] - [Worker clientId=connect-5, groupId=primary-mm2] Discovered group coordinator localhost:45395 (id: 2147483647 rack: null)
2023-07-30 13:49:44,470 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition test-topic-1-0 broker=0] No checkpointed highwatermark is found for partition test-topic-1-0
2023-07-30 13:49:44,470 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition test-topic-1-0 broker=0] Log loaded for partition test-topic-1-0 with initial high watermark 0
2023-07-30 13:49:44,470 - INFO  [DistributedHerder-connect-5-1:WorkerCoordinator@225] - [Worker clientId=connect-5, groupId=primary-mm2] Rebalance started
2023-07-30 13:49:44,471 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator@534] - [Worker clientId=connect-5, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:49:44,472 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator@468] - [Worker clientId=connect-5, groupId=primary-mm2] Rebalance failed.
org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group.
2023-07-30 13:49:44,472 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator@534] - [Worker clientId=connect-5, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:49:44,472 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Log partition=test-topic-1-9, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,473 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group primary-mm2 in state PreparingRebalance with old generation 0 (__consumer_offsets-17) (reason: Adding new member connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175 with group instance id None)
2023-07-30 13:49:44,474 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Created log for partition test-topic-1-9 in /tmp/junit7660765717748375067/junit866559250785637651/test-topic-1-9 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,474 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition test-topic-1-9 broker=0] No checkpointed highwatermark is found for partition test-topic-1-9
2023-07-30 13:49:44,475 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition test-topic-1-9 broker=0] Log loaded for partition test-topic-1-9 with initial high watermark 0
2023-07-30 13:49:44,475 - INFO  [executor-Rebalance:Logging@66] - [GroupCoordinator 0]: Stabilized group primary-mm2 generation 1 (__consumer_offsets-17)
2023-07-30 13:49:44,476 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=1, memberId='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', protocol='sessioned'}
2023-07-30 13:49:44,480 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group primary-mm2 for generation 1
2023-07-30 13:49:44,483 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=1, memberId='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', protocol='sessioned'}
2023-07-30 13:49:44,483 - INFO  [DistributedHerder-connect-5-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-5, groupId=primary-mm2] Joined group at generation 1 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', leaderUrl='http://localhost:39987/', offset=-1, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:49:44,483 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1215] - [Worker clientId=connect-5, groupId=primary-mm2] Starting connectors and tasks using config offset -1
2023-07-30 13:49:44,483 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1243] - [Worker clientId=connect-5, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:49:44,483 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Log partition=test-topic-1-7, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,484 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Created log for partition test-topic-1-7 in /tmp/junit7660765717748375067/junit866559250785637651/test-topic-1-7 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,484 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition test-topic-1-7 broker=0] No checkpointed highwatermark is found for partition test-topic-1-7
2023-07-30 13:49:44,484 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition test-topic-1-7 broker=0] Log loaded for partition test-topic-1-7 with initial high watermark 0
2023-07-30 13:49:44,490 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Log partition=test-topic-1-8, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,491 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Created log for partition test-topic-1-8 in /tmp/junit7660765717748375067/junit866559250785637651/test-topic-1-8 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,492 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition test-topic-1-8 broker=0] No checkpointed highwatermark is found for partition test-topic-1-8
2023-07-30 13:49:44,492 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition test-topic-1-8 broker=0] Log loaded for partition test-topic-1-8 with initial high watermark 0
2023-07-30 13:49:44,495 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1577] - [Worker clientId=connect-4, groupId=primary-mm2] Session key updated
2023-07-30 13:49:44,495 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1577] - [Worker clientId=connect-6, groupId=primary-mm2] Session key updated
2023-07-30 13:49:44,496 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1577] - [Worker clientId=connect-5, groupId=primary-mm2] Session key updated
2023-07-30 13:49:44,498 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Log partition=test-topic-1-5, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,498 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Created log for partition test-topic-1-5 in /tmp/junit7660765717748375067/junit866559250785637651/test-topic-1-5 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,498 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition test-topic-1-5 broker=0] No checkpointed highwatermark is found for partition test-topic-1-5
2023-07-30 13:49:44,498 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition test-topic-1-5 broker=0] Log loaded for partition test-topic-1-5 with initial high watermark 0
2023-07-30 13:49:44,506 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Log partition=test-topic-1-6, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,506 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Created log for partition test-topic-1-6 in /tmp/junit7660765717748375067/junit866559250785637651/test-topic-1-6 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,506 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition test-topic-1-6 broker=0] No checkpointed highwatermark is found for partition test-topic-1-6
2023-07-30 13:49:44,506 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition test-topic-1-6 broker=0] Log loaded for partition test-topic-1-6 with initial high watermark 0
2023-07-30 13:49:44,514 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Log partition=test-topic-1-3, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,515 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Created log for partition test-topic-1-3 in /tmp/junit7660765717748375067/junit866559250785637651/test-topic-1-3 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,515 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition test-topic-1-3 broker=0] No checkpointed highwatermark is found for partition test-topic-1-3
2023-07-30 13:49:44,515 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition test-topic-1-3 broker=0] Log loaded for partition test-topic-1-3 with initial high watermark 0
2023-07-30 13:49:44,517 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$FindCoordinatorResponseHandler@841] - [Worker clientId=connect-4, groupId=primary-mm2] Discovered group coordinator localhost:45395 (id: 2147483647 rack: null)
2023-07-30 13:49:44,517 - INFO  [DistributedHerder-connect-4-1:WorkerCoordinator@225] - [Worker clientId=connect-4, groupId=primary-mm2] Rebalance started
2023-07-30 13:49:44,517 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator@534] - [Worker clientId=connect-4, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:49:44,519 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator@468] - [Worker clientId=connect-4, groupId=primary-mm2] Rebalance failed.
org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group.
2023-07-30 13:49:44,519 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator@534] - [Worker clientId=connect-4, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:49:44,520 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group primary-mm2 in state PreparingRebalance with old generation 1 (__consumer_offsets-17) (reason: Adding new member connect-4-4677c970-fed2-41c6-8269-6c02eca18a2c with group instance id None)
2023-07-30 13:49:44,522 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Log partition=test-topic-1-4, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,523 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Created log for partition test-topic-1-4 in /tmp/junit7660765717748375067/junit866559250785637651/test-topic-1-4 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,523 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition test-topic-1-4 broker=0] No checkpointed highwatermark is found for partition test-topic-1-4
2023-07-30 13:49:44,523 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition test-topic-1-4 broker=0] Log loaded for partition test-topic-1-4 with initial high watermark 0
2023-07-30 13:49:44,531 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Log partition=test-topic-1-1, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,531 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Created log for partition test-topic-1-1 in /tmp/junit7660765717748375067/junit866559250785637651/test-topic-1-1 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,531 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition test-topic-1-1 broker=0] No checkpointed highwatermark is found for partition test-topic-1-1
2023-07-30 13:49:44,531 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition test-topic-1-1 broker=0] Log loaded for partition test-topic-1-1 with initial high watermark 0
2023-07-30 13:49:44,539 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Log partition=test-topic-1-2, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,539 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Created log for partition test-topic-1-2 in /tmp/junit7660765717748375067/junit866559250785637651/test-topic-1-2 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,539 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition test-topic-1-2 broker=0] No checkpointed highwatermark is found for partition test-topic-1-2
2023-07-30 13:49:44,539 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [Partition test-topic-1-2 broker=0] Log loaded for partition test-topic-1-2 with initial high watermark 0
2023-07-30 13:49:44,549 - INFO  [kafka-admin-client-thread | adminclient-64:AppInfoParser@83] - App info kafka.admin.client for adminclient-64 unregistered
2023-07-30 13:49:44,550 - INFO  [kafka-admin-client-thread | adminclient-64:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:44,550 - INFO  [kafka-admin-client-thread | adminclient-64:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:44,550 - INFO  [kafka-admin-client-thread | adminclient-64:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:44,550 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:45395]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:44,551 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:44,551 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:44,551 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739384551
2023-07-30 13:49:44,555 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$FindCoordinatorResponseHandler@841] - [Worker clientId=connect-6, groupId=primary-mm2] Discovered group coordinator localhost:45395 (id: 2147483647 rack: null)
2023-07-30 13:49:44,555 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Creating topic primary.test-topic-1 with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0))
2023-07-30 13:49:44,555 - INFO  [DistributedHerder-connect-6-1:WorkerCoordinator@225] - [Worker clientId=connect-6, groupId=primary-mm2] Rebalance started
2023-07-30 13:49:44,555 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator@534] - [Worker clientId=connect-6, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:49:44,557 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator@468] - [Worker clientId=connect-6, groupId=primary-mm2] Rebalance failed.
org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group.
2023-07-30 13:49:44,557 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator@534] - [Worker clientId=connect-6, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:49:44,562 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(primary.test-topic-1-0)
2023-07-30 13:49:44,564 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Log partition=primary.test-topic-1-0, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,564 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Created log for partition primary.test-topic-1-0 in /tmp/junit7660765717748375067/junit866559250785637651/primary.test-topic-1-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,565 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition primary.test-topic-1-0 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-1-0
2023-07-30 13:49:44,565 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [Partition primary.test-topic-1-0 broker=0] Log loaded for partition primary.test-topic-1-0 with initial high watermark 0
2023-07-30 13:49:44,568 - INFO  [kafka-admin-client-thread | adminclient-65:AppInfoParser@83] - App info kafka.admin.client for adminclient-65 unregistered
2023-07-30 13:49:44,569 - INFO  [kafka-admin-client-thread | adminclient-65:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:44,569 - INFO  [kafka-admin-client-thread | adminclient-65:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:44,569 - INFO  [kafka-admin-client-thread | adminclient-65:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:44,569 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:45395]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:44,570 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:44,571 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:44,571 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739384570
2023-07-30 13:49:44,574 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Creating topic heartbeats with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0))
2023-07-30 13:49:44,579 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(heartbeats-0)
2023-07-30 13:49:44,580 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=heartbeats-0, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,581 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition heartbeats-0 in /tmp/junit7660765717748375067/junit866559250785637651/heartbeats-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,581 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition heartbeats-0 broker=0] No checkpointed highwatermark is found for partition heartbeats-0
2023-07-30 13:49:44,581 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition heartbeats-0 broker=0] Log loaded for partition heartbeats-0 with initial high watermark 0
2023-07-30 13:49:44,585 - INFO  [kafka-admin-client-thread | adminclient-66:AppInfoParser@83] - App info kafka.admin.client for adminclient-66 unregistered
2023-07-30 13:49:44,586 - INFO  [kafka-admin-client-thread | adminclient-66:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:44,586 - INFO  [kafka-admin-client-thread | adminclient-66:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:44,586 - INFO  [kafka-admin-client-thread | adminclient-66:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:44,626 - INFO  [main:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:35687]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-consumer-group-dummy-19
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = consumer-group-dummy
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:49:44,628 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:44,628 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:44,628 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739384628
2023-07-30 13:49:44,628 - INFO  [main:KafkaConsumer@961] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Subscribed to topic(s): test-topic-1
2023-07-30 13:49:44,631 - INFO  [main:Metadata@279] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Cluster ID: XnnaS7BkQ22Tt1TiwnBd0Q
2023-07-30 13:49:44,631 - INFO  [main:AbstractCoordinator$FindCoordinatorResponseHandler@841] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Discovered group coordinator localhost:35687 (id: 2147483647 rack: null)
2023-07-30 13:49:44,632 - INFO  [main:AbstractCoordinator@534] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] (Re-)joining group
2023-07-30 13:49:44,636 - INFO  [main:AbstractCoordinator@468] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Rebalance failed.
org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group.
2023-07-30 13:49:44,637 - INFO  [main:AbstractCoordinator@534] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] (Re-)joining group
2023-07-30 13:49:44,638 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group consumer-group-dummy in state PreparingRebalance with old generation 0 (__consumer_offsets-35) (reason: Adding new member consumer-consumer-group-dummy-19-07bb80c6-14c4-4b53-908f-f91b66c6e470 with group instance id None)
2023-07-30 13:49:44,640 - INFO  [executor-Rebalance:Logging@66] - [GroupCoordinator 0]: Stabilized group consumer-group-dummy generation 1 (__consumer_offsets-35)
2023-07-30 13:49:44,641 - INFO  [main:AbstractCoordinator$JoinGroupResponseHandler@590] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Successfully joined group with generation Generation{generationId=1, memberId='consumer-consumer-group-dummy-19-07bb80c6-14c4-4b53-908f-f91b66c6e470', protocol='range'}
2023-07-30 13:49:44,642 - INFO  [main:ConsumerCoordinator@626] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Finished assignment for group at generation 1: {consumer-consumer-group-dummy-19-07bb80c6-14c4-4b53-908f-f91b66c6e470=Assignment(partitions=[test-topic-1-0, test-topic-1-1, test-topic-1-2, test-topic-1-3, test-topic-1-4, test-topic-1-5, test-topic-1-6, test-topic-1-7, test-topic-1-8, test-topic-1-9])}
2023-07-30 13:49:44,647 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group consumer-group-dummy for generation 1
2023-07-30 13:49:44,648 - INFO  [main:AbstractCoordinator$SyncGroupResponseHandler@750] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Successfully synced group in generation Generation{generationId=1, memberId='consumer-consumer-group-dummy-19-07bb80c6-14c4-4b53-908f-f91b66c6e470', protocol='range'}
2023-07-30 13:49:44,648 - INFO  [main:ConsumerCoordinator@276] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Notifying assignor about the new Assignment(partitions=[test-topic-1-0, test-topic-1-1, test-topic-1-2, test-topic-1-3, test-topic-1-4, test-topic-1-5, test-topic-1-6, test-topic-1-7, test-topic-1-8, test-topic-1-9])
2023-07-30 13:49:44,648 - INFO  [main:ConsumerCoordinator@288] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Adding newly assigned partitions: test-topic-1-3, test-topic-1-2, test-topic-1-5, test-topic-1-4, test-topic-1-7, test-topic-1-6, test-topic-1-9, test-topic-1-8, test-topic-1-1, test-topic-1-0
2023-07-30 13:49:44,653 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-3
2023-07-30 13:49:44,653 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-2
2023-07-30 13:49:44,654 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-5
2023-07-30 13:49:44,654 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-4
2023-07-30 13:49:44,654 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-7
2023-07-30 13:49:44,654 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-6
2023-07-30 13:49:44,654 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-9
2023-07-30 13:49:44,654 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-8
2023-07-30 13:49:44,654 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-1
2023-07-30 13:49:44,654 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-0
2023-07-30 13:49:44,656 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:44,657 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:44,657 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:44,657 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:44,657 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-7 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:44,657 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-6 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:44,657 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-9 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:44,657 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-8 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:44,658 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:44,658 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:44,681 - INFO  [main:ConsumerCoordinator@307] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Revoke previously assigned partitions test-topic-1-3, test-topic-1-2, test-topic-1-5, test-topic-1-4, test-topic-1-7, test-topic-1-6, test-topic-1-9, test-topic-1-8, test-topic-1-1, test-topic-1-0
2023-07-30 13:49:44,681 - INFO  [main:AbstractCoordinator@1016] - [Consumer clientId=consumer-consumer-group-dummy-19, groupId=consumer-group-dummy] Member consumer-consumer-group-dummy-19-07bb80c6-14c4-4b53-908f-f91b66c6e470 sending LeaveGroup request to coordinator localhost:35687 (id: 2147483647 rack: null) due to the consumer is being closed
2023-07-30 13:49:44,684 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [GroupCoordinator 0]: Member[group.instance.id None, member.id consumer-consumer-group-dummy-19-07bb80c6-14c4-4b53-908f-f91b66c6e470] in group consumer-group-dummy has left, removing it from the group
2023-07-30 13:49:44,685 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group consumer-group-dummy in state PreparingRebalance with old generation 1 (__consumer_offsets-35) (reason: removing member consumer-consumer-group-dummy-19-07bb80c6-14c4-4b53-908f-f91b66c6e470 on LeaveGroup)
2023-07-30 13:49:44,685 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [GroupCoordinator 0]: Group consumer-group-dummy with generation 2 is now empty (__consumer_offsets-35)
2023-07-30 13:49:44,689 - INFO  [main:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:44,689 - INFO  [main:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:44,689 - INFO  [main:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:44,690 - INFO  [main:AppInfoParser@83] - App info kafka.consumer for consumer-consumer-group-dummy-19 unregistered
2023-07-30 13:49:44,691 - INFO  [main:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:45395]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-consumer-group-dummy-20
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = consumer-group-dummy
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:49:44,692 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:44,692 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:44,692 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739384692
2023-07-30 13:49:44,693 - INFO  [main:KafkaConsumer@961] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Subscribed to topic(s): test-topic-1
2023-07-30 13:49:44,694 - INFO  [main:Metadata@279] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Cluster ID: l67hsn_BQke0DtmYXtLNog
2023-07-30 13:49:44,695 - INFO  [main:AbstractCoordinator$FindCoordinatorResponseHandler@841] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Discovered group coordinator localhost:45395 (id: 2147483647 rack: null)
2023-07-30 13:49:44,695 - INFO  [main:AbstractCoordinator@534] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] (Re-)joining group
2023-07-30 13:49:44,697 - INFO  [main:AbstractCoordinator@468] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Rebalance failed.
org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group.
2023-07-30 13:49:44,697 - INFO  [main:AbstractCoordinator@534] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] (Re-)joining group
2023-07-30 13:49:44,698 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group consumer-group-dummy in state PreparingRebalance with old generation 0 (__consumer_offsets-35) (reason: Adding new member consumer-consumer-group-dummy-20-461f17f5-b433-4f67-869e-21bb714cb24a with group instance id None)
2023-07-30 13:49:44,699 - INFO  [executor-Rebalance:Logging@66] - [GroupCoordinator 0]: Stabilized group consumer-group-dummy generation 1 (__consumer_offsets-35)
2023-07-30 13:49:44,700 - INFO  [main:AbstractCoordinator$JoinGroupResponseHandler@590] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Successfully joined group with generation Generation{generationId=1, memberId='consumer-consumer-group-dummy-20-461f17f5-b433-4f67-869e-21bb714cb24a', protocol='range'}
2023-07-30 13:49:44,701 - INFO  [main:ConsumerCoordinator@626] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Finished assignment for group at generation 1: {consumer-consumer-group-dummy-20-461f17f5-b433-4f67-869e-21bb714cb24a=Assignment(partitions=[test-topic-1-0, test-topic-1-1, test-topic-1-2, test-topic-1-3, test-topic-1-4, test-topic-1-5, test-topic-1-6, test-topic-1-7, test-topic-1-8, test-topic-1-9])}
2023-07-30 13:49:44,704 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group consumer-group-dummy for generation 1
2023-07-30 13:49:44,706 - INFO  [main:AbstractCoordinator$SyncGroupResponseHandler@750] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Successfully synced group in generation Generation{generationId=1, memberId='consumer-consumer-group-dummy-20-461f17f5-b433-4f67-869e-21bb714cb24a', protocol='range'}
2023-07-30 13:49:44,707 - INFO  [main:ConsumerCoordinator@276] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Notifying assignor about the new Assignment(partitions=[test-topic-1-0, test-topic-1-1, test-topic-1-2, test-topic-1-3, test-topic-1-4, test-topic-1-5, test-topic-1-6, test-topic-1-7, test-topic-1-8, test-topic-1-9])
2023-07-30 13:49:44,707 - INFO  [main:ConsumerCoordinator@288] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Adding newly assigned partitions: test-topic-1-3, test-topic-1-2, test-topic-1-5, test-topic-1-4, test-topic-1-7, test-topic-1-6, test-topic-1-9, test-topic-1-8, test-topic-1-1, test-topic-1-0
2023-07-30 13:49:44,708 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-3
2023-07-30 13:49:44,708 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-2
2023-07-30 13:49:44,708 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-5
2023-07-30 13:49:44,708 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-4
2023-07-30 13:49:44,708 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-7
2023-07-30 13:49:44,708 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-6
2023-07-30 13:49:44,708 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-9
2023-07-30 13:49:44,708 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-8
2023-07-30 13:49:44,708 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-1
2023-07-30 13:49:44,709 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Found no committed offset for partition test-topic-1-0
2023-07-30 13:49:44,711 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:44,711 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:44,711 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:44,711 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:44,711 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-7 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:44,711 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-6 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:44,711 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-9 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:44,712 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-8 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:44,712 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:44,712 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Resetting offset for partition test-topic-1-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:44,721 - INFO  [main:ConsumerCoordinator@307] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Revoke previously assigned partitions test-topic-1-3, test-topic-1-2, test-topic-1-5, test-topic-1-4, test-topic-1-7, test-topic-1-6, test-topic-1-9, test-topic-1-8, test-topic-1-1, test-topic-1-0
2023-07-30 13:49:44,721 - INFO  [main:AbstractCoordinator@1016] - [Consumer clientId=consumer-consumer-group-dummy-20, groupId=consumer-group-dummy] Member consumer-consumer-group-dummy-20-461f17f5-b433-4f67-869e-21bb714cb24a sending LeaveGroup request to coordinator localhost:45395 (id: 2147483647 rack: null) due to the consumer is being closed
2023-07-30 13:49:44,723 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [GroupCoordinator 0]: Member[group.instance.id None, member.id consumer-consumer-group-dummy-20-461f17f5-b433-4f67-869e-21bb714cb24a] in group consumer-group-dummy has left, removing it from the group
2023-07-30 13:49:44,724 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group consumer-group-dummy in state PreparingRebalance with old generation 1 (__consumer_offsets-35) (reason: removing member consumer-consumer-group-dummy-20-461f17f5-b433-4f67-869e-21bb714cb24a on LeaveGroup)
2023-07-30 13:49:44,724 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [GroupCoordinator 0]: Group consumer-group-dummy with generation 2 is now empty (__consumer_offsets-35)
2023-07-30 13:49:44,725 - INFO  [main:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:44,725 - INFO  [main:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:44,725 - INFO  [main:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:44,726 - INFO  [main:AppInfoParser@83] - App info kafka.consumer for consumer-consumer-group-dummy-20 unregistered
2023-07-30 13:49:44,727 - INFO  [main:MirrorConnectorsIntegrationTest@158] - primary REST service: http://localhost:43083/connectors
2023-07-30 13:49:44,727 - INFO  [main:MirrorConnectorsIntegrationTest@159] - backup REST service: http://localhost:43163/connectors
2023-07-30 13:49:44,728 - INFO  [main:MirrorConnectorsIntegrationTest@161] - primary brokers: localhost:35687
2023-07-30 13:49:44,728 - INFO  [main:MirrorConnectorsIntegrationTest@162] - backup brokers: localhost:45395
2023-07-30 13:49:44,728 - INFO  [main:AbstractConfig@361] - MirrorMakerConfig values: 
	clusters = [primary, backup]
	config.providers = []
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:44,916 - INFO  [main:Reflections@239] - Reflections took 188 ms to scan 1 urls, producing 349 keys and 1055 values [using 20 cores]
2023-07-30 13:49:44,918 - WARN  [main:ReflectionUtils@318] - could not get type for name kotlin.jvm.internal.Lambda from any class loader
org.reflections.ReflectionsException: could not get type for name kotlin.jvm.internal.Lambda
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.mirror.MirrorMakerConfig.<init>(MirrorMakerConfig.java:80)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:167)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: kotlin.jvm.internal.Lambda
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 64 more
2023-07-30 13:49:44,920 - WARN  [main:ReflectionUtils@318] - could not get type for name kotlin.jvm.functions.Function0 from any class loader
org.reflections.ReflectionsException: could not get type for name kotlin.jvm.functions.Function0
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.mirror.MirrorMakerConfig.<init>(MirrorMakerConfig.java:80)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:167)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: kotlin.jvm.functions.Function0
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 64 more
2023-07-30 13:49:44,921 - WARN  [main:ReflectionUtils@318] - could not get type for name java.util.spi.ToolProvider from any class loader
org.reflections.ReflectionsException: could not get type for name java.util.spi.ToolProvider
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)
	at org.reflections.Reflections.expandSuperTypes(Reflections.java:382)
	at org.reflections.Reflections.<init>(Reflections.java:140)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader$InternalReflections.<init>(DelegatingClassLoader.java:444)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanPluginPath(DelegatingClassLoader.java:334)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.scanUrlsAndAddPlugins(DelegatingClassLoader.java:268)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initPluginLoader(DelegatingClassLoader.java:216)
	at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.initLoaders(DelegatingClassLoader.java:209)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:61)
	at org.apache.kafka.connect.mirror.MirrorMakerConfig.<init>(MirrorMakerConfig.java:80)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.setup(MirrorConnectorsIntegrationTest.java:167)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
Caused by: java.lang.ClassNotFoundException: java.util.spi.ToolProvider
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:817)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)
	... 64 more
2023-07-30 13:49:44,923 - INFO  [main:DelegatingClassLoader@269] - Registered loader: java.net.FactoryURLClassLoader@483bf400
2023-07-30 13:49:44,924 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension'
2023-07-30 13:49:44,924 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.runtime.isolation.PluginsTest$TestConnectRestExtension'
2023-07-30 13:49:44,924 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy'
2023-07-30 13:49:44,924 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy'
2023-07-30 13:49:44,924 - INFO  [main:DelegatingClassLoader@198] - Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy'
2023-07-30 13:49:44,924 - INFO  [main:DelegatingClassLoader@427] - Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension'
2023-07-30 13:49:44,925 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'TestConnectRestExtension' and 'Test' to plugin 'org.apache.kafka.connect.runtime.isolation.PluginsTest$TestConnectRestExtension'
2023-07-30 13:49:44,925 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy'
2023-07-30 13:49:44,925 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy'
2023-07-30 13:49:44,925 - INFO  [main:DelegatingClassLoader@430] - Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy'
2023-07-30 13:49:44,926 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:35687]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:44,926 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:44,926 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:44,927 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739384926
2023-07-30 13:49:44,930 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - Creating topic test-topic-with-empty-partition with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0), 5 -> ArrayBuffer(0), 6 -> ArrayBuffer(0), 7 -> ArrayBuffer(0), 8 -> ArrayBuffer(0), 9 -> ArrayBuffer(0))
2023-07-30 13:49:44,940 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(test-topic-with-empty-partition-7, test-topic-with-empty-partition-1, test-topic-with-empty-partition-6, test-topic-with-empty-partition-3, test-topic-with-empty-partition-5, test-topic-with-empty-partition-2, test-topic-with-empty-partition-0, test-topic-with-empty-partition-9, test-topic-with-empty-partition-4, test-topic-with-empty-partition-8)
2023-07-30 13:49:44,941 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=test-topic-with-empty-partition-0, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,942 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition test-topic-with-empty-partition-0 in /tmp/junit6293896813403981889/junit8934247217114872089/test-topic-with-empty-partition-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,942 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-with-empty-partition-0 broker=0] No checkpointed highwatermark is found for partition test-topic-with-empty-partition-0
2023-07-30 13:49:44,942 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-with-empty-partition-0 broker=0] Log loaded for partition test-topic-with-empty-partition-0 with initial high watermark 0
2023-07-30 13:49:44,944 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=test-topic-with-empty-partition-3, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,945 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition test-topic-with-empty-partition-3 in /tmp/junit6293896813403981889/junit8934247217114872089/test-topic-with-empty-partition-3 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,945 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-with-empty-partition-3 broker=0] No checkpointed highwatermark is found for partition test-topic-with-empty-partition-3
2023-07-30 13:49:44,945 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-with-empty-partition-3 broker=0] Log loaded for partition test-topic-with-empty-partition-3 with initial high watermark 0
2023-07-30 13:49:44,953 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=test-topic-with-empty-partition-4, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,953 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition test-topic-with-empty-partition-4 in /tmp/junit6293896813403981889/junit8934247217114872089/test-topic-with-empty-partition-4 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,953 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-with-empty-partition-4 broker=0] No checkpointed highwatermark is found for partition test-topic-with-empty-partition-4
2023-07-30 13:49:44,953 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-with-empty-partition-4 broker=0] Log loaded for partition test-topic-with-empty-partition-4 with initial high watermark 0
2023-07-30 13:49:44,961 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=test-topic-with-empty-partition-1, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,962 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition test-topic-with-empty-partition-1 in /tmp/junit6293896813403981889/junit8934247217114872089/test-topic-with-empty-partition-1 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,962 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-with-empty-partition-1 broker=0] No checkpointed highwatermark is found for partition test-topic-with-empty-partition-1
2023-07-30 13:49:44,962 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-with-empty-partition-1 broker=0] Log loaded for partition test-topic-with-empty-partition-1 with initial high watermark 0
2023-07-30 13:49:44,969 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=test-topic-with-empty-partition-2, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,970 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition test-topic-with-empty-partition-2 in /tmp/junit6293896813403981889/junit8934247217114872089/test-topic-with-empty-partition-2 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,970 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-with-empty-partition-2 broker=0] No checkpointed highwatermark is found for partition test-topic-with-empty-partition-2
2023-07-30 13:49:44,970 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-with-empty-partition-2 broker=0] Log loaded for partition test-topic-with-empty-partition-2 with initial high watermark 0
2023-07-30 13:49:44,978 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=test-topic-with-empty-partition-7, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,978 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition test-topic-with-empty-partition-7 in /tmp/junit6293896813403981889/junit8934247217114872089/test-topic-with-empty-partition-7 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,979 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-with-empty-partition-7 broker=0] No checkpointed highwatermark is found for partition test-topic-with-empty-partition-7
2023-07-30 13:49:44,979 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-with-empty-partition-7 broker=0] Log loaded for partition test-topic-with-empty-partition-7 with initial high watermark 0
2023-07-30 13:49:44,986 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=test-topic-with-empty-partition-8, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,987 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition test-topic-with-empty-partition-8 in /tmp/junit6293896813403981889/junit8934247217114872089/test-topic-with-empty-partition-8 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,987 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-with-empty-partition-8 broker=0] No checkpointed highwatermark is found for partition test-topic-with-empty-partition-8
2023-07-30 13:49:44,987 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-with-empty-partition-8 broker=0] Log loaded for partition test-topic-with-empty-partition-8 with initial high watermark 0
2023-07-30 13:49:44,994 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=test-topic-with-empty-partition-5, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:44,995 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition test-topic-with-empty-partition-5 in /tmp/junit6293896813403981889/junit8934247217114872089/test-topic-with-empty-partition-5 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:44,995 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-with-empty-partition-5 broker=0] No checkpointed highwatermark is found for partition test-topic-with-empty-partition-5
2023-07-30 13:49:44,995 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-with-empty-partition-5 broker=0] Log loaded for partition test-topic-with-empty-partition-5 with initial high watermark 0
2023-07-30 13:49:45,003 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=test-topic-with-empty-partition-6, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:45,003 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition test-topic-with-empty-partition-6 in /tmp/junit6293896813403981889/junit8934247217114872089/test-topic-with-empty-partition-6 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:45,003 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-with-empty-partition-6 broker=0] No checkpointed highwatermark is found for partition test-topic-with-empty-partition-6
2023-07-30 13:49:45,003 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-with-empty-partition-6 broker=0] Log loaded for partition test-topic-with-empty-partition-6 with initial high watermark 0
2023-07-30 13:49:45,011 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=test-topic-with-empty-partition-9, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:45,011 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition test-topic-with-empty-partition-9 in /tmp/junit6293896813403981889/junit8934247217114872089/test-topic-with-empty-partition-9 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:45,012 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-with-empty-partition-9 broker=0] No checkpointed highwatermark is found for partition test-topic-with-empty-partition-9
2023-07-30 13:49:45,012 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition test-topic-with-empty-partition-9 broker=0] Log loaded for partition test-topic-with-empty-partition-9 with initial high watermark 0
2023-07-30 13:49:45,021 - INFO  [kafka-admin-client-thread | adminclient-67:AppInfoParser@83] - App info kafka.admin.client for adminclient-67 unregistered
2023-07-30 13:49:45,022 - INFO  [kafka-admin-client-thread | adminclient-67:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:45,022 - INFO  [kafka-admin-client-thread | adminclient-67:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:45,022 - INFO  [kafka-admin-client-thread | adminclient-67:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:45,039 - INFO  [main:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:35687]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-consumer-group-testReplicationWithEmptyPartition-21
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = consumer-group-testReplicationWithEmptyPartition
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:49:45,040 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:45,040 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:45,041 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739385040
2023-07-30 13:49:45,041 - INFO  [main:KafkaConsumer@961] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Subscribed to topic(s): test-topic-with-empty-partition
2023-07-30 13:49:45,042 - INFO  [main:Metadata@279] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Cluster ID: XnnaS7BkQ22Tt1TiwnBd0Q
2023-07-30 13:49:45,042 - INFO  [main:AbstractCoordinator$FindCoordinatorResponseHandler@841] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Discovered group coordinator localhost:35687 (id: 2147483647 rack: null)
2023-07-30 13:49:45,043 - INFO  [main:AbstractCoordinator@534] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] (Re-)joining group
2023-07-30 13:49:45,045 - INFO  [main:AbstractCoordinator@468] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Rebalance failed.
org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group.
2023-07-30 13:49:45,045 - INFO  [main:AbstractCoordinator@534] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] (Re-)joining group
2023-07-30 13:49:45,046 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group consumer-group-testReplicationWithEmptyPartition in state PreparingRebalance with old generation 0 (__consumer_offsets-34) (reason: Adding new member consumer-consumer-group-testReplicationWithEmptyPartition-21-a8105d94-43a9-458f-b7d4-59adb87f0001 with group instance id None)
2023-07-30 13:49:45,047 - INFO  [executor-Rebalance:Logging@66] - [GroupCoordinator 0]: Stabilized group consumer-group-testReplicationWithEmptyPartition generation 1 (__consumer_offsets-34)
2023-07-30 13:49:45,047 - INFO  [main:AbstractCoordinator$JoinGroupResponseHandler@590] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Successfully joined group with generation Generation{generationId=1, memberId='consumer-consumer-group-testReplicationWithEmptyPartition-21-a8105d94-43a9-458f-b7d4-59adb87f0001', protocol='range'}
2023-07-30 13:49:45,048 - INFO  [main:ConsumerCoordinator@626] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Finished assignment for group at generation 1: {consumer-consumer-group-testReplicationWithEmptyPartition-21-a8105d94-43a9-458f-b7d4-59adb87f0001=Assignment(partitions=[test-topic-with-empty-partition-0, test-topic-with-empty-partition-1, test-topic-with-empty-partition-2, test-topic-with-empty-partition-3, test-topic-with-empty-partition-4, test-topic-with-empty-partition-5, test-topic-with-empty-partition-6, test-topic-with-empty-partition-7, test-topic-with-empty-partition-8, test-topic-with-empty-partition-9])}
2023-07-30 13:49:45,050 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group consumer-group-testReplicationWithEmptyPartition for generation 1
2023-07-30 13:49:45,051 - INFO  [main:AbstractCoordinator$SyncGroupResponseHandler@750] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Successfully synced group in generation Generation{generationId=1, memberId='consumer-consumer-group-testReplicationWithEmptyPartition-21-a8105d94-43a9-458f-b7d4-59adb87f0001', protocol='range'}
2023-07-30 13:49:45,051 - INFO  [main:ConsumerCoordinator@276] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Notifying assignor about the new Assignment(partitions=[test-topic-with-empty-partition-0, test-topic-with-empty-partition-1, test-topic-with-empty-partition-2, test-topic-with-empty-partition-3, test-topic-with-empty-partition-4, test-topic-with-empty-partition-5, test-topic-with-empty-partition-6, test-topic-with-empty-partition-7, test-topic-with-empty-partition-8, test-topic-with-empty-partition-9])
2023-07-30 13:49:45,051 - INFO  [main:ConsumerCoordinator@288] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Adding newly assigned partitions: test-topic-with-empty-partition-3, test-topic-with-empty-partition-2, test-topic-with-empty-partition-5, test-topic-with-empty-partition-4, test-topic-with-empty-partition-1, test-topic-with-empty-partition-0, test-topic-with-empty-partition-7, test-topic-with-empty-partition-6, test-topic-with-empty-partition-9, test-topic-with-empty-partition-8
2023-07-30 13:49:45,052 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Found no committed offset for partition test-topic-with-empty-partition-3
2023-07-30 13:49:45,052 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Found no committed offset for partition test-topic-with-empty-partition-2
2023-07-30 13:49:45,052 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Found no committed offset for partition test-topic-with-empty-partition-5
2023-07-30 13:49:45,053 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Found no committed offset for partition test-topic-with-empty-partition-4
2023-07-30 13:49:45,053 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Found no committed offset for partition test-topic-with-empty-partition-1
2023-07-30 13:49:45,053 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Found no committed offset for partition test-topic-with-empty-partition-0
2023-07-30 13:49:45,053 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Found no committed offset for partition test-topic-with-empty-partition-7
2023-07-30 13:49:45,053 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Found no committed offset for partition test-topic-with-empty-partition-6
2023-07-30 13:49:45,053 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Found no committed offset for partition test-topic-with-empty-partition-9
2023-07-30 13:49:45,053 - INFO  [main:ConsumerCoordinator$OffsetFetchResponseHandler@1354] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Found no committed offset for partition test-topic-with-empty-partition-8
2023-07-30 13:49:45,055 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Resetting offset for partition test-topic-with-empty-partition-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:45,055 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Resetting offset for partition test-topic-with-empty-partition-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:45,055 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Resetting offset for partition test-topic-with-empty-partition-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:45,055 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Resetting offset for partition test-topic-with-empty-partition-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:45,055 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Resetting offset for partition test-topic-with-empty-partition-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:45,056 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Resetting offset for partition test-topic-with-empty-partition-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:45,056 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Resetting offset for partition test-topic-with-empty-partition-7 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:45,056 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Resetting offset for partition test-topic-with-empty-partition-6 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:45,056 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Resetting offset for partition test-topic-with-empty-partition-9 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:45,056 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Resetting offset for partition test-topic-with-empty-partition-8 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:45,063 - INFO  [main:ConsumerCoordinator@307] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Revoke previously assigned partitions test-topic-with-empty-partition-3, test-topic-with-empty-partition-2, test-topic-with-empty-partition-5, test-topic-with-empty-partition-4, test-topic-with-empty-partition-1, test-topic-with-empty-partition-0, test-topic-with-empty-partition-7, test-topic-with-empty-partition-6, test-topic-with-empty-partition-9, test-topic-with-empty-partition-8
2023-07-30 13:49:45,063 - INFO  [main:AbstractCoordinator@1016] - [Consumer clientId=consumer-consumer-group-testReplicationWithEmptyPartition-21, groupId=consumer-group-testReplicationWithEmptyPartition] Member consumer-consumer-group-testReplicationWithEmptyPartition-21-a8105d94-43a9-458f-b7d4-59adb87f0001 sending LeaveGroup request to coordinator localhost:35687 (id: 2147483647 rack: null) due to the consumer is being closed
2023-07-30 13:49:45,064 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupCoordinator 0]: Member[group.instance.id None, member.id consumer-consumer-group-testReplicationWithEmptyPartition-21-a8105d94-43a9-458f-b7d4-59adb87f0001] in group consumer-group-testReplicationWithEmptyPartition has left, removing it from the group
2023-07-30 13:49:45,064 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group consumer-group-testReplicationWithEmptyPartition in state PreparingRebalance with old generation 1 (__consumer_offsets-34) (reason: removing member consumer-consumer-group-testReplicationWithEmptyPartition-21-a8105d94-43a9-458f-b7d4-59adb87f0001 on LeaveGroup)
2023-07-30 13:49:45,065 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupCoordinator 0]: Group consumer-group-testReplicationWithEmptyPartition with generation 2 is now empty (__consumer_offsets-34)
2023-07-30 13:49:45,065 - INFO  [main:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:45,065 - INFO  [main:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:45,066 - INFO  [main:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:45,066 - INFO  [main:AppInfoParser@83] - App info kafka.consumer for consumer-consumer-group-testReplicationWithEmptyPartition-21 unregistered
2023-07-30 13:49:45,743 - INFO  [DistributedHerder-connect-2-1:AbstractCoordinator$HeartbeatResponseHandler@1097] - [Worker clientId=connect-2, groupId=backup-mm2] Attempt to heartbeat failed since group is rebalancing
2023-07-30 13:49:45,743 - INFO  [DistributedHerder-connect-2-1:WorkerCoordinator@225] - [Worker clientId=connect-2, groupId=backup-mm2] Rebalance started
2023-07-30 13:49:45,743 - INFO  [DistributedHerder-connect-2-1:AbstractCoordinator@534] - [Worker clientId=connect-2, groupId=backup-mm2] (Re-)joining group
2023-07-30 13:49:45,745 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [GroupCoordinator 0]: Stabilized group backup-mm2 generation 2 (__consumer_offsets-5)
2023-07-30 13:49:45,746 - INFO  [DistributedHerder-connect-3-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-3, groupId=backup-mm2] Successfully joined group with generation Generation{generationId=2, memberId='connect-3-e9f825fd-ae6e-40a7-b695-08f8d5226443', protocol='sessioned'}
2023-07-30 13:49:45,746 - INFO  [DistributedHerder-connect-2-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-2, groupId=backup-mm2] Successfully joined group with generation Generation{generationId=2, memberId='connect-2-12409f17-1c6b-4299-8d44-e2b23de6867f', protocol='sessioned'}
2023-07-30 13:49:45,746 - INFO  [DistributedHerder-connect-1-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-1, groupId=backup-mm2] Successfully joined group with generation Generation{generationId=2, memberId='connect-1-e0f7ca44-8c98-43fc-a034-dae0d9034b88', protocol='sessioned'}
2023-07-30 13:49:45,749 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group backup-mm2 for generation 2
2023-07-30 13:49:45,750 - INFO  [DistributedHerder-connect-3-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-3, groupId=backup-mm2] Successfully synced group in generation Generation{generationId=2, memberId='connect-3-e9f825fd-ae6e-40a7-b695-08f8d5226443', protocol='sessioned'}
2023-07-30 13:49:45,751 - INFO  [DistributedHerder-connect-2-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-2, groupId=backup-mm2] Successfully synced group in generation Generation{generationId=2, memberId='connect-2-12409f17-1c6b-4299-8d44-e2b23de6867f', protocol='sessioned'}
2023-07-30 13:49:45,751 - INFO  [DistributedHerder-connect-1-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-1, groupId=backup-mm2] Successfully synced group in generation Generation{generationId=2, memberId='connect-1-e0f7ca44-8c98-43fc-a034-dae0d9034b88', protocol='sessioned'}
2023-07-30 13:49:45,751 - INFO  [DistributedHerder-connect-3-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-3, groupId=backup-mm2] Joined group at generation 2 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-2-12409f17-1c6b-4299-8d44-e2b23de6867f', leaderUrl='http://localhost:33591/', offset=1, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:49:45,751 - INFO  [DistributedHerder-connect-1-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-1, groupId=backup-mm2] Joined group at generation 2 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-2-12409f17-1c6b-4299-8d44-e2b23de6867f', leaderUrl='http://localhost:33591/', offset=1, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:49:45,751 - INFO  [DistributedHerder-connect-2-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-2, groupId=backup-mm2] Joined group at generation 2 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-2-12409f17-1c6b-4299-8d44-e2b23de6867f', leaderUrl='http://localhost:33591/', offset=1, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:49:45,751 - WARN  [DistributedHerder-connect-1-1:DistributedHerder@1094] - [Worker clientId=connect-1, groupId=backup-mm2] Catching up to assignment's config offset.
2023-07-30 13:49:45,751 - WARN  [DistributedHerder-connect-3-1:DistributedHerder@1094] - [Worker clientId=connect-3, groupId=backup-mm2] Catching up to assignment's config offset.
2023-07-30 13:49:45,751 - WARN  [DistributedHerder-connect-2-1:DistributedHerder@1094] - [Worker clientId=connect-2, groupId=backup-mm2] Catching up to assignment's config offset.
2023-07-30 13:49:45,751 - INFO  [DistributedHerder-connect-1-1:DistributedHerder@1157] - [Worker clientId=connect-1, groupId=backup-mm2] Current config state offset -1 is behind group assignment 1, reading to end of config log
2023-07-30 13:49:45,752 - INFO  [DistributedHerder-connect-2-1:DistributedHerder@1157] - [Worker clientId=connect-2, groupId=backup-mm2] Current config state offset -1 is behind group assignment 1, reading to end of config log
2023-07-30 13:49:45,752 - INFO  [DistributedHerder-connect-3-1:DistributedHerder@1157] - [Worker clientId=connect-3, groupId=backup-mm2] Current config state offset -1 is behind group assignment 1, reading to end of config log
2023-07-30 13:49:45,877 - INFO  [DistributedHerder-connect-1-1:DistributedHerder@1161] - [Worker clientId=connect-1, groupId=backup-mm2] Finished reading to end of log and updated config snapshot, new config log offset: 1
2023-07-30 13:49:45,877 - INFO  [DistributedHerder-connect-2-1:DistributedHerder@1161] - [Worker clientId=connect-2, groupId=backup-mm2] Finished reading to end of log and updated config snapshot, new config log offset: 1
2023-07-30 13:49:45,877 - INFO  [DistributedHerder-connect-1-1:DistributedHerder@1215] - [Worker clientId=connect-1, groupId=backup-mm2] Starting connectors and tasks using config offset 1
2023-07-30 13:49:45,877 - INFO  [DistributedHerder-connect-2-1:DistributedHerder@1215] - [Worker clientId=connect-2, groupId=backup-mm2] Starting connectors and tasks using config offset 1
2023-07-30 13:49:45,877 - INFO  [DistributedHerder-connect-1-1:DistributedHerder@1243] - [Worker clientId=connect-1, groupId=backup-mm2] Finished starting connectors and tasks
2023-07-30 13:49:45,877 - INFO  [DistributedHerder-connect-2-1:DistributedHerder@1243] - [Worker clientId=connect-2, groupId=backup-mm2] Finished starting connectors and tasks
2023-07-30 13:49:45,972 - INFO  [DistributedHerder-connect-3-1:DistributedHerder@1161] - [Worker clientId=connect-3, groupId=backup-mm2] Finished reading to end of log and updated config snapshot, new config log offset: 1
2023-07-30 13:49:45,973 - INFO  [DistributedHerder-connect-3-1:DistributedHerder@1215] - [Worker clientId=connect-3, groupId=backup-mm2] Starting connectors and tasks using config offset 1
2023-07-30 13:49:45,973 - INFO  [DistributedHerder-connect-3-1:DistributedHerder@1243] - [Worker clientId=connect-3, groupId=backup-mm2] Finished starting connectors and tasks
2023-07-30 13:49:47,477 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$HeartbeatResponseHandler@1097] - [Worker clientId=connect-5, groupId=primary-mm2] Attempt to heartbeat failed since group is rebalancing
2023-07-30 13:49:47,477 - INFO  [DistributedHerder-connect-5-1:WorkerCoordinator@225] - [Worker clientId=connect-5, groupId=primary-mm2] Rebalance started
2023-07-30 13:49:47,477 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator@534] - [Worker clientId=connect-5, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:49:47,480 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Stabilized group primary-mm2 generation 2 (__consumer_offsets-17)
2023-07-30 13:49:47,481 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=2, memberId='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', protocol='sessioned'}
2023-07-30 13:49:47,481 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=2, memberId='connect-6-5d9250b5-104e-4755-a5c9-fa2e5283a564', protocol='sessioned'}
2023-07-30 13:49:47,481 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=2, memberId='connect-4-4677c970-fed2-41c6-8269-6c02eca18a2c', protocol='sessioned'}
2023-07-30 13:49:47,484 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group primary-mm2 for generation 2
2023-07-30 13:49:47,486 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=2, memberId='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', protocol='sessioned'}
2023-07-30 13:49:47,487 - INFO  [DistributedHerder-connect-5-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-5, groupId=primary-mm2] Joined group at generation 2 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', leaderUrl='http://localhost:39987/', offset=1, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:49:47,487 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=2, memberId='connect-6-5d9250b5-104e-4755-a5c9-fa2e5283a564', protocol='sessioned'}
2023-07-30 13:49:47,487 - WARN  [DistributedHerder-connect-5-1:DistributedHerder@1094] - [Worker clientId=connect-5, groupId=primary-mm2] Catching up to assignment's config offset.
2023-07-30 13:49:47,487 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=2, memberId='connect-4-4677c970-fed2-41c6-8269-6c02eca18a2c', protocol='sessioned'}
2023-07-30 13:49:47,487 - INFO  [DistributedHerder-connect-6-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-6, groupId=primary-mm2] Joined group at generation 2 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', leaderUrl='http://localhost:39987/', offset=1, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:49:47,487 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1157] - [Worker clientId=connect-5, groupId=primary-mm2] Current config state offset -1 is behind group assignment 1, reading to end of config log
2023-07-30 13:49:47,487 - INFO  [DistributedHerder-connect-4-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-4, groupId=primary-mm2] Joined group at generation 2 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', leaderUrl='http://localhost:39987/', offset=1, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:49:47,487 - WARN  [DistributedHerder-connect-6-1:DistributedHerder@1094] - [Worker clientId=connect-6, groupId=primary-mm2] Catching up to assignment's config offset.
2023-07-30 13:49:47,488 - WARN  [DistributedHerder-connect-4-1:DistributedHerder@1094] - [Worker clientId=connect-4, groupId=primary-mm2] Catching up to assignment's config offset.
2023-07-30 13:49:47,488 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1157] - [Worker clientId=connect-6, groupId=primary-mm2] Current config state offset -1 is behind group assignment 1, reading to end of config log
2023-07-30 13:49:47,488 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1157] - [Worker clientId=connect-4, groupId=primary-mm2] Current config state offset -1 is behind group assignment 1, reading to end of config log
2023-07-30 13:49:47,499 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1161] - [Worker clientId=connect-4, groupId=primary-mm2] Finished reading to end of log and updated config snapshot, new config log offset: 1
2023-07-30 13:49:47,499 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1161] - [Worker clientId=connect-6, groupId=primary-mm2] Finished reading to end of log and updated config snapshot, new config log offset: 1
2023-07-30 13:49:47,499 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1215] - [Worker clientId=connect-4, groupId=primary-mm2] Starting connectors and tasks using config offset 1
2023-07-30 13:49:47,499 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1215] - [Worker clientId=connect-6, groupId=primary-mm2] Starting connectors and tasks using config offset 1
2023-07-30 13:49:47,499 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1243] - [Worker clientId=connect-4, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:49:47,499 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1243] - [Worker clientId=connect-6, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:49:47,499 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1161] - [Worker clientId=connect-5, groupId=primary-mm2] Finished reading to end of log and updated config snapshot, new config log offset: 1
2023-07-30 13:49:47,500 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1215] - [Worker clientId=connect-5, groupId=primary-mm2] Starting connectors and tasks using config offset 1
2023-07-30 13:49:47,500 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1243] - [Worker clientId=connect-5, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:49:47,508 - INFO  [pool-25-thread-1:AbstractConfig@361] - AbstractConfig values: 

2023-07-30 13:49:47,599 - INFO  [pool-28-thread-1:AbstractConfig@361] - AbstractConfig values: 

2023-07-30 13:49:47,603 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1533] - [Worker clientId=connect-4, groupId=primary-mm2] Connector MirrorSourceConnector config updated
2023-07-30 13:49:47,603 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1533] - [Worker clientId=connect-6, groupId=primary-mm2] Connector MirrorSourceConnector config updated
2023-07-30 13:49:47,603 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1533] - [Worker clientId=connect-5, groupId=primary-mm2] Connector MirrorSourceConnector config updated
2023-07-30 13:49:47,603 - INFO  [DistributedHerder-connect-4-1:WorkerCoordinator@225] - [Worker clientId=connect-4, groupId=primary-mm2] Rebalance started
2023-07-30 13:49:47,603 - INFO  [DistributedHerder-connect-6-1:WorkerCoordinator@225] - [Worker clientId=connect-6, groupId=primary-mm2] Rebalance started
2023-07-30 13:49:47,603 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator@534] - [Worker clientId=connect-4, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:49:47,603 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator@534] - [Worker clientId=connect-6, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:49:47,604 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group primary-mm2 in state PreparingRebalance with old generation 2 (__consumer_offsets-17) (reason: Updating metadata for member connect-4-4677c970-fed2-41c6-8269-6c02eca18a2c during Stable)
2023-07-30 13:49:47,604 - INFO  [DistributedHerder-connect-5-1:WorkerCoordinator@225] - [Worker clientId=connect-5, groupId=primary-mm2] Rebalance started
2023-07-30 13:49:47,604 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator@534] - [Worker clientId=connect-5, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:49:47,606 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupCoordinator 0]: Stabilized group primary-mm2 generation 3 (__consumer_offsets-17)
2023-07-30 13:49:47,606 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=3, memberId='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', protocol='sessioned'}
2023-07-30 13:49:47,607 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=3, memberId='connect-6-5d9250b5-104e-4755-a5c9-fa2e5283a564', protocol='sessioned'}
2023-07-30 13:49:47,607 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=3, memberId='connect-4-4677c970-fed2-41c6-8269-6c02eca18a2c', protocol='sessioned'}
2023-07-30 13:49:47,609 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group primary-mm2 for generation 3
2023-07-30 13:49:47,611 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=3, memberId='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', protocol='sessioned'}
2023-07-30 13:49:47,611 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=3, memberId='connect-6-5d9250b5-104e-4755-a5c9-fa2e5283a564', protocol='sessioned'}
2023-07-30 13:49:47,611 - INFO  [DistributedHerder-connect-5-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-5, groupId=primary-mm2] Joined group at generation 3 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', leaderUrl='http://localhost:39987/', offset=2, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:49:47,611 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=3, memberId='connect-4-4677c970-fed2-41c6-8269-6c02eca18a2c', protocol='sessioned'}
2023-07-30 13:49:47,611 - INFO  [DistributedHerder-connect-6-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-6, groupId=primary-mm2] Joined group at generation 3 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', leaderUrl='http://localhost:39987/', offset=2, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:49:47,612 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1215] - [Worker clientId=connect-5, groupId=primary-mm2] Starting connectors and tasks using config offset 2
2023-07-30 13:49:47,612 - INFO  [DistributedHerder-connect-4-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-4, groupId=primary-mm2] Joined group at generation 3 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', leaderUrl='http://localhost:39987/', offset=2, connectorIds=[MirrorSourceConnector], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:49:47,612 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1215] - [Worker clientId=connect-6, groupId=primary-mm2] Starting connectors and tasks using config offset 2
2023-07-30 13:49:47,612 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1243] - [Worker clientId=connect-5, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:49:47,612 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1215] - [Worker clientId=connect-4, groupId=primary-mm2] Starting connectors and tasks using config offset 2
2023-07-30 13:49:47,612 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1243] - [Worker clientId=connect-6, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:49:47,613 - INFO  [StartAndStopExecutor-connect-4-1:DistributedHerder@1298] - [Worker clientId=connect-4, groupId=primary-mm2] Starting connector MirrorSourceConnector
2023-07-30 13:49:47,614 - INFO  [StartAndStopExecutor-connect-4-1:Worker@274] - Creating connector MirrorSourceConnector of type org.apache.kafka.connect.mirror.MirrorSourceConnector
2023-07-30 13:49:47,615 - INFO  [StartAndStopExecutor-connect-4-1:AbstractConfig@361] - SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:49:47,616 - INFO  [StartAndStopExecutor-connect-4-1:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:49:47,620 - INFO  [StartAndStopExecutor-connect-4-1:Worker@284] - Instantiated connector MirrorSourceConnector with version 1 of type class org.apache.kafka.connect.mirror.MirrorSourceConnector
2023-07-30 13:49:47,622 - INFO  [StartAndStopExecutor-connect-4-1:Worker@310] - Finished creating connector MirrorSourceConnector
2023-07-30 13:49:47,622 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1243] - [Worker clientId=connect-4, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:49:47,623 - INFO  [connector-thread-MirrorSourceConnector:AbstractConfig@361] - MirrorConnectorConfig values: 
	admin.timeout.ms = 60000
	checkpoints.topic.replication.factor = 1
	config.action.reload = restart
	config.properties.blacklist = [follower\.replication\.throttled\.replicas, leader\.replication\.throttled\.replicas, message\.timestamp\.difference\.max\.ms, message\.timestamp\.type, unclean\.leader\.election\.enable, min\.insync\.replicas]
	config.property.filter.class = class org.apache.kafka.connect.mirror.DefaultConfigPropertyFilter
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	consumer.poll.timeout.ms = 1000
	emit.checkpoints.enabled = true
	emit.checkpoints.interval.seconds = 1
	emit.heartbeats.enabled = true
	emit.heartbeats.interval.seconds = 1
	enabled = true
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	group.filter.class = class org.apache.kafka.connect.mirror.DefaultGroupFilter
	groups = [consumer-group-.*]
	groups.blacklist = [console-consumer-.*, connect-.*, __.*]
	header.converter = null
	heartbeats.topic.replication.factor = 1
	key.converter = null
	metric.reporters = null
	name = MirrorSourceConnector
	offset-syncs.topic.replication.factor = 1
	offset.lag.max = 100
	predicates = []
	refresh.groups.enabled = true
	refresh.groups.interval.seconds = 1
	refresh.topics.enabled = true
	refresh.topics.interval.seconds = 1
	replication.factor = 1
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	source.cluster.alias = primary
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	sync.group.offsets.enabled = false
	sync.group.offsets.interval.seconds = 60
	sync.topic.acls.enabled = false
	sync.topic.acls.interval.seconds = 600
	sync.topic.configs.enabled = true
	sync.topic.configs.interval.seconds = 600
	target.cluster.alias = backup
	tasks.max = 1
	topic.filter.class = class org.apache.kafka.connect.mirror.DefaultTopicFilter
	topics = [test-topic-.*, primary.test-topic-.*, backup.test-topic-.*]
	topics.blacklist = [.*[\-\.]internal, .*\.replica, __.*]
	transforms = []
	value.converter = null

2023-07-30 13:49:47,625 - INFO  [connector-thread-MirrorSourceConnector:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:35687]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:47,626 - INFO  [connector-thread-MirrorSourceConnector:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:47,626 - INFO  [connector-thread-MirrorSourceConnector:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:47,626 - INFO  [connector-thread-MirrorSourceConnector:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739387626
2023-07-30 13:49:47,627 - INFO  [connector-thread-MirrorSourceConnector:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:45395]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:47,627 - INFO  [connector-thread-MirrorSourceConnector:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:47,628 - INFO  [connector-thread-MirrorSourceConnector:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:47,628 - INFO  [connector-thread-MirrorSourceConnector:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739387627
2023-07-30 13:49:47,630 - INFO  [Scheduler for MirrorSourceConnector-creating upstream offset-syncs topic:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:35687]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:47,631 - INFO  [Scheduler for MirrorSourceConnector-creating upstream offset-syncs topic:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:47,631 - INFO  [Scheduler for MirrorSourceConnector-creating upstream offset-syncs topic:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:47,631 - INFO  [Scheduler for MirrorSourceConnector-creating upstream offset-syncs topic:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739387631
2023-07-30 13:49:47,635 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Creating topic mm2-offset-syncs.backup.internal with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0))
2023-07-30 13:49:47,643 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(mm2-offset-syncs.backup.internal-0)
2023-07-30 13:49:47,645 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=mm2-offset-syncs.backup.internal-0, dir=/tmp/junit6293896813403981889/junit8934247217114872089] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:47,646 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition mm2-offset-syncs.backup.internal-0 in /tmp/junit6293896813403981889/junit8934247217114872089/mm2-offset-syncs.backup.internal-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:47,646 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition mm2-offset-syncs.backup.internal-0 broker=0] No checkpointed highwatermark is found for partition mm2-offset-syncs.backup.internal-0
2023-07-30 13:49:47,646 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition mm2-offset-syncs.backup.internal-0 broker=0] Log loaded for partition mm2-offset-syncs.backup.internal-0 with initial high watermark 0
2023-07-30 13:49:47,650 - INFO  [Scheduler for MirrorSourceConnector-creating upstream offset-syncs topic:TopicAdmin@284] - Created topic (name=mm2-offset-syncs.backup.internal, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at localhost:35687
2023-07-30 13:49:47,650 - INFO  [kafka-admin-client-thread | adminclient-70:AppInfoParser@83] - App info kafka.admin.client for adminclient-70 unregistered
2023-07-30 13:49:47,651 - INFO  [kafka-admin-client-thread | adminclient-70:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:47,651 - INFO  [kafka-admin-client-thread | adminclient-70:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:47,651 - INFO  [kafka-admin-client-thread | adminclient-70:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:47,651 - INFO  [Scheduler for MirrorSourceConnector-creating upstream offset-syncs topic:Scheduler@95] - creating upstream offset-syncs topic took 22 ms
2023-07-30 13:49:47,652 - INFO  [main:EmbeddedConnectCluster@689] - PUT response for URL=http://localhost:43163/connectors/MirrorSourceConnector/config is {"name":"MirrorSourceConnector","config":{"connector.class":"org.apache.kafka.connect.mirror.MirrorSourceConnector","offset-syncs.topic.replication.factor":"1","replication.factor":"1","source.cluster.producer.bootstrap.servers":"localhost:35687","sync.topic.acls.enabled":"false","topics":"test-topic-.*, primary.test-topic-.*, backup.test-topic-.*","emit.checkpoints.interval.seconds":"1","heartbeats.topic.replication.factor":"1","source.cluster.alias":"primary","groups":"consumer-group-.*","source.cluster.bootstrap.servers":"localhost:35687","target.cluster.producer.bootstrap.servers":"localhost:45395","enabled":"true","target.cluster.admin.bootstrap.servers":"localhost:45395","target.cluster.alias":"backup","target.cluster.consumer.bootstrap.servers":"localhost:45395","name":"MirrorSourceConnector","target.cluster.bootstrap.servers":"localhost:45395","emit.heartbeats.interval.seconds":"1","checkpoints.topic.replication.factor":"1","source.cluster.admin.bootstrap.servers":"localhost:35687","refresh.groups.interval.seconds":"1","refresh.topics.interval.seconds":"1","source.cluster.consumer.bootstrap.servers":"localhost:35687"},"tasks":[],"type":"source"}
2023-07-30 13:49:47,660 - INFO  [pool-25-thread-1:AbstractConfig@361] - AbstractConfig values: 

2023-07-30 13:49:47,668 - INFO  [Scheduler for MirrorSourceConnector-loading initial set of topic-partitions:Scheduler@95] - loading initial set of topic-partitions took 16 ms
2023-07-30 13:49:47,671 - INFO  [pool-28-thread-1:AbstractConfig@361] - AbstractConfig values: 

2023-07-30 13:49:47,673 - INFO  [Scheduler for MirrorSourceConnector-creating downstream topic-partitions:Scheduler@95] - creating downstream topic-partitions took 5 ms
2023-07-30 13:49:47,673 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Creating topic primary.test-topic-with-empty-partition with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(0), 5 -> ArrayBuffer(0), 6 -> ArrayBuffer(0), 7 -> ArrayBuffer(0), 8 -> ArrayBuffer(0), 9 -> ArrayBuffer(0))
2023-07-30 13:49:47,674 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1533] - [Worker clientId=connect-6, groupId=primary-mm2] Connector MirrorCheckpointConnector config updated
2023-07-30 13:49:47,675 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1533] - [Worker clientId=connect-4, groupId=primary-mm2] Connector MirrorCheckpointConnector config updated
2023-07-30 13:49:47,675 - INFO  [DistributedHerder-connect-6-1:WorkerCoordinator@225] - [Worker clientId=connect-6, groupId=primary-mm2] Rebalance started
2023-07-30 13:49:47,675 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator@534] - [Worker clientId=connect-6, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:49:47,675 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1533] - [Worker clientId=connect-5, groupId=primary-mm2] Connector MirrorCheckpointConnector config updated
2023-07-30 13:49:47,675 - INFO  [DistributedHerder-connect-4-1:WorkerCoordinator@225] - [Worker clientId=connect-4, groupId=primary-mm2] Rebalance started
2023-07-30 13:49:47,675 - INFO  [DistributedHerder-connect-5-1:WorkerCoordinator@225] - [Worker clientId=connect-5, groupId=primary-mm2] Rebalance started
2023-07-30 13:49:47,675 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator@534] - [Worker clientId=connect-4, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:49:47,675 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator@534] - [Worker clientId=connect-5, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:49:47,676 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group primary-mm2 in state PreparingRebalance with old generation 3 (__consumer_offsets-17) (reason: Updating metadata for member connect-6-5d9250b5-104e-4755-a5c9-fa2e5283a564 during Stable)
2023-07-30 13:49:47,678 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [GroupCoordinator 0]: Stabilized group primary-mm2 generation 4 (__consumer_offsets-17)
2023-07-30 13:49:47,679 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Creating topic primary.heartbeats with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(0))
2023-07-30 13:49:47,679 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=4, memberId='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', protocol='sessioned'}
2023-07-30 13:49:47,679 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=4, memberId='connect-4-4677c970-fed2-41c6-8269-6c02eca18a2c', protocol='sessioned'}
2023-07-30 13:49:47,679 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=4, memberId='connect-6-5d9250b5-104e-4755-a5c9-fa2e5283a564', protocol='sessioned'}
2023-07-30 13:49:47,683 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group primary-mm2 for generation 4
2023-07-30 13:49:47,685 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=4, memberId='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', protocol='sessioned'}
2023-07-30 13:49:47,685 - INFO  [DistributedHerder-connect-5-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-5, groupId=primary-mm2] Joined group at generation 4 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', leaderUrl='http://localhost:39987/', offset=3, connectorIds=[MirrorCheckpointConnector], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:49:47,685 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1215] - [Worker clientId=connect-5, groupId=primary-mm2] Starting connectors and tasks using config offset 3
2023-07-30 13:49:47,687 - INFO  [StartAndStopExecutor-connect-5-1:DistributedHerder@1298] - [Worker clientId=connect-5, groupId=primary-mm2] Starting connector MirrorCheckpointConnector
2023-07-30 13:49:47,688 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=4, memberId='connect-6-5d9250b5-104e-4755-a5c9-fa2e5283a564', protocol='sessioned'}
2023-07-30 13:49:47,688 - INFO  [StartAndStopExecutor-connect-5-1:Worker@274] - Creating connector MirrorCheckpointConnector of type org.apache.kafka.connect.mirror.MirrorCheckpointConnector
2023-07-30 13:49:47,688 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=4, memberId='connect-4-4677c970-fed2-41c6-8269-6c02eca18a2c', protocol='sessioned'}
2023-07-30 13:49:47,688 - INFO  [DistributedHerder-connect-6-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-6, groupId=primary-mm2] Joined group at generation 4 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', leaderUrl='http://localhost:39987/', offset=3, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:49:47,690 - INFO  [DistributedHerder-connect-4-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-4, groupId=primary-mm2] Joined group at generation 4 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', leaderUrl='http://localhost:39987/', offset=3, connectorIds=[MirrorSourceConnector], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:49:47,689 - INFO  [StartAndStopExecutor-connect-5-1:AbstractConfig@361] - SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorCheckpointConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorCheckpointConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:49:47,690 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1215] - [Worker clientId=connect-4, groupId=primary-mm2] Starting connectors and tasks using config offset 3
2023-07-30 13:49:47,690 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1215] - [Worker clientId=connect-6, groupId=primary-mm2] Starting connectors and tasks using config offset 3
2023-07-30 13:49:47,690 - INFO  [StartAndStopExecutor-connect-5-1:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorCheckpointConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorCheckpointConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:49:47,690 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1243] - [Worker clientId=connect-4, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:49:47,690 - INFO  [main:EmbeddedConnectCluster@689] - PUT response for URL=http://localhost:43163/connectors/MirrorCheckpointConnector/config is {"name":"MirrorCheckpointConnector","config":{"connector.class":"org.apache.kafka.connect.mirror.MirrorCheckpointConnector","offset-syncs.topic.replication.factor":"1","replication.factor":"1","source.cluster.producer.bootstrap.servers":"localhost:35687","sync.topic.acls.enabled":"false","topics":"test-topic-.*, primary.test-topic-.*, backup.test-topic-.*","emit.checkpoints.interval.seconds":"1","heartbeats.topic.replication.factor":"1","source.cluster.alias":"primary","groups":"consumer-group-.*","source.cluster.bootstrap.servers":"localhost:35687","target.cluster.producer.bootstrap.servers":"localhost:45395","enabled":"true","target.cluster.admin.bootstrap.servers":"localhost:45395","target.cluster.alias":"backup","target.cluster.consumer.bootstrap.servers":"localhost:45395","name":"MirrorCheckpointConnector","target.cluster.bootstrap.servers":"localhost:45395","emit.heartbeats.interval.seconds":"1","checkpoints.topic.replication.factor":"1","source.cluster.admin.bootstrap.servers":"localhost:35687","refresh.groups.interval.seconds":"1","refresh.topics.interval.seconds":"1","source.cluster.consumer.bootstrap.servers":"localhost:35687"},"tasks":[],"type":"source"}
2023-07-30 13:49:47,691 - INFO  [StartAndStopExecutor-connect-5-1:Worker@284] - Instantiated connector MirrorCheckpointConnector with version 1 of type class org.apache.kafka.connect.mirror.MirrorCheckpointConnector
2023-07-30 13:49:47,690 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1243] - [Worker clientId=connect-6, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:49:47,691 - INFO  [StartAndStopExecutor-connect-5-1:Worker@310] - Finished creating connector MirrorCheckpointConnector
2023-07-30 13:49:47,691 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1243] - [Worker clientId=connect-5, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:49:47,691 - INFO  [connector-thread-MirrorCheckpointConnector:AbstractConfig@361] - MirrorConnectorConfig values: 
	admin.timeout.ms = 60000
	checkpoints.topic.replication.factor = 1
	config.action.reload = restart
	config.properties.blacklist = [follower\.replication\.throttled\.replicas, leader\.replication\.throttled\.replicas, message\.timestamp\.difference\.max\.ms, message\.timestamp\.type, unclean\.leader\.election\.enable, min\.insync\.replicas]
	config.property.filter.class = class org.apache.kafka.connect.mirror.DefaultConfigPropertyFilter
	connector.class = org.apache.kafka.connect.mirror.MirrorCheckpointConnector
	consumer.poll.timeout.ms = 1000
	emit.checkpoints.enabled = true
	emit.checkpoints.interval.seconds = 1
	emit.heartbeats.enabled = true
	emit.heartbeats.interval.seconds = 1
	enabled = true
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	group.filter.class = class org.apache.kafka.connect.mirror.DefaultGroupFilter
	groups = [consumer-group-.*]
	groups.blacklist = [console-consumer-.*, connect-.*, __.*]
	header.converter = null
	heartbeats.topic.replication.factor = 1
	key.converter = null
	metric.reporters = null
	name = MirrorCheckpointConnector
	offset-syncs.topic.replication.factor = 1
	offset.lag.max = 100
	predicates = []
	refresh.groups.enabled = true
	refresh.groups.interval.seconds = 1
	refresh.topics.enabled = true
	refresh.topics.interval.seconds = 1
	replication.factor = 1
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	source.cluster.alias = primary
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	sync.group.offsets.enabled = false
	sync.group.offsets.interval.seconds = 60
	sync.topic.acls.enabled = false
	sync.topic.acls.interval.seconds = 600
	sync.topic.configs.enabled = true
	sync.topic.configs.interval.seconds = 600
	target.cluster.alias = backup
	tasks.max = 1
	topic.filter.class = class org.apache.kafka.connect.mirror.DefaultTopicFilter
	topics = [test-topic-.*, primary.test-topic-.*, backup.test-topic-.*]
	topics.blacklist = [.*[\-\.]internal, .*\.replica, __.*]
	transforms = []
	value.converter = null

2023-07-30 13:49:47,693 - INFO  [connector-thread-MirrorCheckpointConnector:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:35687]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:47,694 - INFO  [connector-thread-MirrorCheckpointConnector:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:47,694 - INFO  [connector-thread-MirrorCheckpointConnector:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:47,694 - INFO  [connector-thread-MirrorCheckpointConnector:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739387694
2023-07-30 13:49:47,696 - INFO  [pool-25-thread-1:AbstractConfig@361] - AbstractConfig values: 

2023-07-30 13:49:47,697 - INFO  [Scheduler for MirrorCheckpointConnector-creating internal topics:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:45395]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:47,698 - INFO  [Scheduler for MirrorCheckpointConnector-creating internal topics:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:47,698 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(primary.test-topic-with-empty-partition-3, primary.test-topic-with-empty-partition-9, primary.test-topic-with-empty-partition-7, primary.test-topic-with-empty-partition-6, primary.test-topic-with-empty-partition-8, primary.test-topic-with-empty-partition-2, primary.test-topic-with-empty-partition-0, primary.test-topic-with-empty-partition-4, primary.test-topic-with-empty-partition-5, primary.test-topic-with-empty-partition-1)
2023-07-30 13:49:47,698 - INFO  [Scheduler for MirrorCheckpointConnector-creating internal topics:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:47,698 - INFO  [Scheduler for MirrorCheckpointConnector-creating internal topics:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739387698
2023-07-30 13:49:47,700 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Log partition=primary.test-topic-with-empty-partition-9, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:47,700 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Created log for partition primary.test-topic-with-empty-partition-9 in /tmp/junit7660765717748375067/junit866559250785637651/primary.test-topic-with-empty-partition-9 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:47,701 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition primary.test-topic-with-empty-partition-9 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-with-empty-partition-9
2023-07-30 13:49:47,701 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition primary.test-topic-with-empty-partition-9 broker=0] Log loaded for partition primary.test-topic-with-empty-partition-9 with initial high watermark 0
2023-07-30 13:49:47,705 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Creating topic primary.checkpoints.internal with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(0))
2023-07-30 13:49:47,708 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Log partition=primary.test-topic-with-empty-partition-0, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:47,709 - INFO  [pool-28-thread-1:AbstractConfig@361] - AbstractConfig values: 

2023-07-30 13:49:47,709 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Created log for partition primary.test-topic-with-empty-partition-0 in /tmp/junit7660765717748375067/junit866559250785637651/primary.test-topic-with-empty-partition-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:47,709 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition primary.test-topic-with-empty-partition-0 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-with-empty-partition-0
2023-07-30 13:49:47,709 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition primary.test-topic-with-empty-partition-0 broker=0] Log loaded for partition primary.test-topic-with-empty-partition-0 with initial high watermark 0
2023-07-30 13:49:47,711 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1533] - [Worker clientId=connect-4, groupId=primary-mm2] Connector MirrorHeartbeatConnector config updated
2023-07-30 13:49:47,711 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1533] - [Worker clientId=connect-6, groupId=primary-mm2] Connector MirrorHeartbeatConnector config updated
2023-07-30 13:49:47,712 - INFO  [DistributedHerder-connect-4-1:WorkerCoordinator@225] - [Worker clientId=connect-4, groupId=primary-mm2] Rebalance started
2023-07-30 13:49:47,712 - INFO  [DistributedHerder-connect-6-1:WorkerCoordinator@225] - [Worker clientId=connect-6, groupId=primary-mm2] Rebalance started
2023-07-30 13:49:47,712 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator@534] - [Worker clientId=connect-4, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:49:47,712 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1533] - [Worker clientId=connect-5, groupId=primary-mm2] Connector MirrorHeartbeatConnector config updated
2023-07-30 13:49:47,712 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator@534] - [Worker clientId=connect-6, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:49:47,712 - INFO  [DistributedHerder-connect-5-1:WorkerCoordinator@225] - [Worker clientId=connect-5, groupId=primary-mm2] Rebalance started
2023-07-30 13:49:47,712 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group primary-mm2 in state PreparingRebalance with old generation 4 (__consumer_offsets-17) (reason: Updating metadata for member connect-4-4677c970-fed2-41c6-8269-6c02eca18a2c during Stable)
2023-07-30 13:49:47,712 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator@534] - [Worker clientId=connect-5, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:49:47,714 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [GroupCoordinator 0]: Stabilized group primary-mm2 generation 5 (__consumer_offsets-17)
2023-07-30 13:49:47,717 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=5, memberId='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', protocol='sessioned'}
2023-07-30 13:49:47,717 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=5, memberId='connect-6-5d9250b5-104e-4755-a5c9-fa2e5283a564', protocol='sessioned'}
2023-07-30 13:49:47,717 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=5, memberId='connect-4-4677c970-fed2-41c6-8269-6c02eca18a2c', protocol='sessioned'}
2023-07-30 13:49:47,718 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Log partition=primary.test-topic-with-empty-partition-3, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:47,719 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group primary-mm2 for generation 5
2023-07-30 13:49:47,719 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Created log for partition primary.test-topic-with-empty-partition-3 in /tmp/junit7660765717748375067/junit866559250785637651/primary.test-topic-with-empty-partition-3 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:47,719 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition primary.test-topic-with-empty-partition-3 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-with-empty-partition-3
2023-07-30 13:49:47,719 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition primary.test-topic-with-empty-partition-3 broker=0] Log loaded for partition primary.test-topic-with-empty-partition-3 with initial high watermark 0
2023-07-30 13:49:47,720 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=5, memberId='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', protocol='sessioned'}
2023-07-30 13:49:47,720 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=5, memberId='connect-6-5d9250b5-104e-4755-a5c9-fa2e5283a564', protocol='sessioned'}
2023-07-30 13:49:47,720 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=5, memberId='connect-4-4677c970-fed2-41c6-8269-6c02eca18a2c', protocol='sessioned'}
2023-07-30 13:49:47,720 - INFO  [DistributedHerder-connect-6-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-6, groupId=primary-mm2] Joined group at generation 5 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', leaderUrl='http://localhost:39987/', offset=4, connectorIds=[MirrorHeartbeatConnector], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:49:47,720 - INFO  [DistributedHerder-connect-5-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-5, groupId=primary-mm2] Joined group at generation 5 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', leaderUrl='http://localhost:39987/', offset=4, connectorIds=[MirrorCheckpointConnector], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:49:47,721 - INFO  [DistributedHerder-connect-4-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-4, groupId=primary-mm2] Joined group at generation 5 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', leaderUrl='http://localhost:39987/', offset=4, connectorIds=[MirrorSourceConnector], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:49:47,721 - INFO  [main:EmbeddedConnectCluster@689] - PUT response for URL=http://localhost:43163/connectors/MirrorHeartbeatConnector/config is {"name":"MirrorHeartbeatConnector","config":{"connector.class":"org.apache.kafka.connect.mirror.MirrorHeartbeatConnector","offset-syncs.topic.replication.factor":"1","replication.factor":"1","source.cluster.producer.bootstrap.servers":"localhost:35687","sync.topic.acls.enabled":"false","topics":"test-topic-.*, primary.test-topic-.*, backup.test-topic-.*","emit.checkpoints.interval.seconds":"1","heartbeats.topic.replication.factor":"1","source.cluster.alias":"primary","groups":"consumer-group-.*","source.cluster.bootstrap.servers":"localhost:35687","target.cluster.producer.bootstrap.servers":"localhost:45395","enabled":"true","target.cluster.admin.bootstrap.servers":"localhost:45395","target.cluster.alias":"backup","target.cluster.consumer.bootstrap.servers":"localhost:45395","name":"MirrorHeartbeatConnector","target.cluster.bootstrap.servers":"localhost:45395","emit.heartbeats.interval.seconds":"1","checkpoints.topic.replication.factor":"1","source.cluster.admin.bootstrap.servers":"localhost:35687","refresh.groups.interval.seconds":"1","refresh.topics.interval.seconds":"1","source.cluster.consumer.bootstrap.servers":"localhost:35687"},"tasks":[],"type":"source"}
2023-07-30 13:49:47,721 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1215] - [Worker clientId=connect-5, groupId=primary-mm2] Starting connectors and tasks using config offset 4
2023-07-30 13:49:47,721 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1215] - [Worker clientId=connect-6, groupId=primary-mm2] Starting connectors and tasks using config offset 4
2023-07-30 13:49:47,721 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1243] - [Worker clientId=connect-5, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:49:47,721 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1215] - [Worker clientId=connect-4, groupId=primary-mm2] Starting connectors and tasks using config offset 4
2023-07-30 13:49:47,721 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1243] - [Worker clientId=connect-4, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:49:47,722 - INFO  [StartAndStopExecutor-connect-6-1:DistributedHerder@1298] - [Worker clientId=connect-6, groupId=primary-mm2] Starting connector MirrorHeartbeatConnector
2023-07-30 13:49:47,722 - INFO  [StartAndStopExecutor-connect-6-1:Worker@274] - Creating connector MirrorHeartbeatConnector of type org.apache.kafka.connect.mirror.MirrorHeartbeatConnector
2023-07-30 13:49:47,723 - INFO  [StartAndStopExecutor-connect-6-1:AbstractConfig@361] - SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorHeartbeatConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorHeartbeatConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:49:47,723 - INFO  [StartAndStopExecutor-connect-6-1:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorHeartbeatConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorHeartbeatConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:49:47,723 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Log partition=primary.test-topic-with-empty-partition-4, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:47,723 - INFO  [StartAndStopExecutor-connect-6-1:Worker@284] - Instantiated connector MirrorHeartbeatConnector with version 1 of type class org.apache.kafka.connect.mirror.MirrorHeartbeatConnector
2023-07-30 13:49:47,723 - INFO  [StartAndStopExecutor-connect-6-1:Worker@310] - Finished creating connector MirrorHeartbeatConnector
2023-07-30 13:49:47,724 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1243] - [Worker clientId=connect-6, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:49:47,724 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Created log for partition primary.test-topic-with-empty-partition-4 in /tmp/junit7660765717748375067/junit866559250785637651/primary.test-topic-with-empty-partition-4 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:47,724 - INFO  [connector-thread-MirrorHeartbeatConnector:AbstractConfig@361] - MirrorConnectorConfig values: 
	admin.timeout.ms = 60000
	checkpoints.topic.replication.factor = 1
	config.action.reload = restart
	config.properties.blacklist = [follower\.replication\.throttled\.replicas, leader\.replication\.throttled\.replicas, message\.timestamp\.difference\.max\.ms, message\.timestamp\.type, unclean\.leader\.election\.enable, min\.insync\.replicas]
	config.property.filter.class = class org.apache.kafka.connect.mirror.DefaultConfigPropertyFilter
	connector.class = org.apache.kafka.connect.mirror.MirrorHeartbeatConnector
	consumer.poll.timeout.ms = 1000
	emit.checkpoints.enabled = true
	emit.checkpoints.interval.seconds = 1
	emit.heartbeats.enabled = true
	emit.heartbeats.interval.seconds = 1
	enabled = true
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	group.filter.class = class org.apache.kafka.connect.mirror.DefaultGroupFilter
	groups = [consumer-group-.*]
	groups.blacklist = [console-consumer-.*, connect-.*, __.*]
	header.converter = null
	heartbeats.topic.replication.factor = 1
	key.converter = null
	metric.reporters = null
	name = MirrorHeartbeatConnector
	offset-syncs.topic.replication.factor = 1
	offset.lag.max = 100
	predicates = []
	refresh.groups.enabled = true
	refresh.groups.interval.seconds = 1
	refresh.topics.enabled = true
	refresh.topics.interval.seconds = 1
	replication.factor = 1
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	source.cluster.alias = primary
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	sync.group.offsets.enabled = false
	sync.group.offsets.interval.seconds = 60
	sync.topic.acls.enabled = false
	sync.topic.acls.interval.seconds = 600
	sync.topic.configs.enabled = true
	sync.topic.configs.interval.seconds = 600
	target.cluster.alias = backup
	tasks.max = 1
	topic.filter.class = class org.apache.kafka.connect.mirror.DefaultTopicFilter
	topics = [test-topic-.*, primary.test-topic-.*, backup.test-topic-.*]
	topics.blacklist = [.*[\-\.]internal, .*\.replica, __.*]
	transforms = []
	value.converter = null

2023-07-30 13:49:47,724 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition primary.test-topic-with-empty-partition-4 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-with-empty-partition-4
2023-07-30 13:49:47,724 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition primary.test-topic-with-empty-partition-4 broker=0] Log loaded for partition primary.test-topic-with-empty-partition-4 with initial high watermark 0
2023-07-30 13:49:47,725 - INFO  [Scheduler for MirrorHeartbeatConnector-creating internal topics:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:45395]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:47,726 - INFO  [Scheduler for MirrorHeartbeatConnector-creating internal topics:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:47,726 - INFO  [Scheduler for MirrorHeartbeatConnector-creating internal topics:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:47,726 - INFO  [Scheduler for MirrorHeartbeatConnector-creating internal topics:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739387726
2023-07-30 13:49:47,729 - INFO  [kafka-admin-client-thread | adminclient-73:AppInfoParser@83] - App info kafka.admin.client for adminclient-73 unregistered
2023-07-30 13:49:47,730 - INFO  [kafka-admin-client-thread | adminclient-73:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:47,730 - INFO  [kafka-admin-client-thread | adminclient-73:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:47,730 - INFO  [kafka-admin-client-thread | adminclient-73:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:47,730 - INFO  [Scheduler for MirrorHeartbeatConnector-creating internal topics:Scheduler@95] - creating internal topics took 6 ms
2023-07-30 13:49:47,731 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Log partition=primary.test-topic-with-empty-partition-1, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:47,732 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Created log for partition primary.test-topic-with-empty-partition-1 in /tmp/junit7660765717748375067/junit866559250785637651/primary.test-topic-with-empty-partition-1 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:47,732 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition primary.test-topic-with-empty-partition-1 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-with-empty-partition-1
2023-07-30 13:49:47,732 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition primary.test-topic-with-empty-partition-1 broker=0] Log loaded for partition primary.test-topic-with-empty-partition-1 with initial high watermark 0
2023-07-30 13:49:47,735 - INFO  [DistributedHerder-connect-6-1:AbstractConfig@361] - SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorHeartbeatConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorHeartbeatConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:49:47,735 - INFO  [DistributedHerder-connect-6-1:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorHeartbeatConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorHeartbeatConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:49:47,742 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Log partition=primary.test-topic-with-empty-partition-2, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:47,742 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Created log for partition primary.test-topic-with-empty-partition-2 in /tmp/junit7660765717748375067/junit866559250785637651/primary.test-topic-with-empty-partition-2 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:47,743 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition primary.test-topic-with-empty-partition-2 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-with-empty-partition-2
2023-07-30 13:49:47,743 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition primary.test-topic-with-empty-partition-2 broker=0] Log loaded for partition primary.test-topic-with-empty-partition-2 with initial high watermark 0
2023-07-30 13:49:47,743 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:43163/connectors/MirrorSourceConnector/status is {"error_code":404,"message":"No status found for connector MirrorSourceConnector"}
2023-07-30 13:49:47,744 - ERROR [main:EmbeddedConnectClusterAssertions@420] - Could not check connector state info.
org.apache.kafka.connect.runtime.rest.errors.ConnectRestException: Could not read connector state. Error response: {"error_code":404,"message":"No status found for connector MirrorSourceConnector"}
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.connectorStatus(EmbeddedConnectCluster.java:459)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectClusterAssertions.checkConnectorState(EmbeddedConnectClusterAssertions.java:413)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectClusterAssertions.lambda$assertConnectorAndAtLeastNumTasksAreRunning$16(EmbeddedConnectClusterAssertions.java:286)
	at org.apache.kafka.test.TestUtils.lambda$waitForCondition$6(TestUtils.java:400)
	at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:448)
	at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:416)
	at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:397)
	at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:387)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectClusterAssertions.assertConnectorAndAtLeastNumTasksAreRunning(EmbeddedConnectClusterAssertions.java:285)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.waitUntilMirrorMakerIsRunning(MirrorConnectorsIntegrationTest.java:191)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.testReplicationWithEmptyPartition(MirrorConnectorsIntegrationTest.java:367)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
2023-07-30 13:49:47,748 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Log partition=primary.test-topic-with-empty-partition-7, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:47,748 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Created log for partition primary.test-topic-with-empty-partition-7 in /tmp/junit7660765717748375067/junit866559250785637651/primary.test-topic-with-empty-partition-7 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:47,749 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition primary.test-topic-with-empty-partition-7 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-with-empty-partition-7
2023-07-30 13:49:47,749 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition primary.test-topic-with-empty-partition-7 broker=0] Log loaded for partition primary.test-topic-with-empty-partition-7 with initial high watermark 0
2023-07-30 13:49:47,757 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Log partition=primary.test-topic-with-empty-partition-8, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:47,757 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Created log for partition primary.test-topic-with-empty-partition-8 in /tmp/junit7660765717748375067/junit866559250785637651/primary.test-topic-with-empty-partition-8 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:47,757 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition primary.test-topic-with-empty-partition-8 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-with-empty-partition-8
2023-07-30 13:49:47,758 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition primary.test-topic-with-empty-partition-8 broker=0] Log loaded for partition primary.test-topic-with-empty-partition-8 with initial high watermark 0
2023-07-30 13:49:47,764 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Log partition=primary.test-topic-with-empty-partition-5, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:47,764 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Created log for partition primary.test-topic-with-empty-partition-5 in /tmp/junit7660765717748375067/junit866559250785637651/primary.test-topic-with-empty-partition-5 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:47,765 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition primary.test-topic-with-empty-partition-5 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-with-empty-partition-5
2023-07-30 13:49:47,765 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition primary.test-topic-with-empty-partition-5 broker=0] Log loaded for partition primary.test-topic-with-empty-partition-5 with initial high watermark 0
2023-07-30 13:49:47,772 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Log partition=primary.test-topic-with-empty-partition-6, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:47,773 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - Created log for partition primary.test-topic-with-empty-partition-6 in /tmp/junit7660765717748375067/junit866559250785637651/primary.test-topic-with-empty-partition-6 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:47,773 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition primary.test-topic-with-empty-partition-6 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-with-empty-partition-6
2023-07-30 13:49:47,773 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [Partition primary.test-topic-with-empty-partition-6 broker=0] Log loaded for partition primary.test-topic-with-empty-partition-6 with initial high watermark 0
2023-07-30 13:49:47,782 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(primary.heartbeats-0)
2023-07-30 13:49:47,783 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Log partition=primary.heartbeats-0, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:47,784 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Created log for partition primary.heartbeats-0 in /tmp/junit7660765717748375067/junit866559250785637651/primary.heartbeats-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:47,784 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition primary.heartbeats-0 broker=0] No checkpointed highwatermark is found for partition primary.heartbeats-0
2023-07-30 13:49:47,784 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Partition primary.heartbeats-0 broker=0] Log loaded for partition primary.heartbeats-0 with initial high watermark 0
2023-07-30 13:49:47,790 - INFO  [kafka-admin-client-thread | adminclient-69:MirrorSourceConnector@308] - Created remote topic primary.test-topic-with-empty-partition with 10 partitions.
2023-07-30 13:49:47,790 - INFO  [kafka-admin-client-thread | adminclient-69:MirrorSourceConnector@308] - Created remote topic primary.heartbeats with 1 partitions.
2023-07-30 13:49:47,790 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(primary.checkpoints.internal-0)
2023-07-30 13:49:47,792 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Log partition=primary.checkpoints.internal-0, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:47,793 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Created log for partition primary.checkpoints.internal-0 in /tmp/junit7660765717748375067/junit866559250785637651/primary.checkpoints.internal-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:47,793 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition primary.checkpoints.internal-0 broker=0] No checkpointed highwatermark is found for partition primary.checkpoints.internal-0
2023-07-30 13:49:47,793 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [Partition primary.checkpoints.internal-0 broker=0] Log loaded for partition primary.checkpoints.internal-0 with initial high watermark 0
2023-07-30 13:49:47,798 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - Creating 9 partitions for 'primary.test-topic-1' with the following replica assignment: HashMap(1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), 2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), 3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), 4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), 5 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), 6 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), 7 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), 8 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), 9 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=)).
2023-07-30 13:49:47,798 - INFO  [Scheduler for MirrorCheckpointConnector-creating internal topics:TopicAdmin@284] - Created topic (name=primary.checkpoints.internal, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at localhost:45395
2023-07-30 13:49:47,798 - INFO  [kafka-admin-client-thread | adminclient-72:AppInfoParser@83] - App info kafka.admin.client for adminclient-72 unregistered
2023-07-30 13:49:47,799 - INFO  [kafka-admin-client-thread | adminclient-72:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:47,799 - INFO  [kafka-admin-client-thread | adminclient-72:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:47,799 - INFO  [kafka-admin-client-thread | adminclient-72:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:47,799 - INFO  [Scheduler for MirrorCheckpointConnector-creating internal topics:Scheduler@95] - creating internal topics took 103 ms
2023-07-30 13:49:47,806 - INFO  [Scheduler for MirrorCheckpointConnector-loading initial consumer groups:Scheduler@95] - loading initial consumer groups took 7 ms
2023-07-30 13:49:47,807 - INFO  [connector-thread-MirrorCheckpointConnector:MirrorCheckpointConnector@79] - Started MirrorCheckpointConnector with 2 consumer groups.
2023-07-30 13:49:47,808 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(primary.test-topic-1-1, primary.test-topic-1-8, primary.test-topic-1-4, primary.test-topic-1-7, primary.test-topic-1-3, primary.test-topic-1-2, primary.test-topic-1-6, primary.test-topic-1-5, primary.test-topic-1-9)
2023-07-30 13:49:47,811 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=primary.test-topic-1-4, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:47,814 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition primary.test-topic-1-4 in /tmp/junit7660765717748375067/junit866559250785637651/primary.test-topic-1-4 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:47,814 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.test-topic-1-4 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-1-4
2023-07-30 13:49:47,814 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.test-topic-1-4 broker=0] Log loaded for partition primary.test-topic-1-4 with initial high watermark 0
2023-07-30 13:49:47,816 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=primary.test-topic-1-3, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:47,817 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition primary.test-topic-1-3 in /tmp/junit7660765717748375067/junit866559250785637651/primary.test-topic-1-3 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:47,817 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.test-topic-1-3 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-1-3
2023-07-30 13:49:47,817 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.test-topic-1-3 broker=0] Log loaded for partition primary.test-topic-1-3 with initial high watermark 0
2023-07-30 13:49:47,825 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=primary.test-topic-1-2, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:47,825 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition primary.test-topic-1-2 in /tmp/junit7660765717748375067/junit866559250785637651/primary.test-topic-1-2 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:47,825 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.test-topic-1-2 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-1-2
2023-07-30 13:49:47,825 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.test-topic-1-2 broker=0] Log loaded for partition primary.test-topic-1-2 with initial high watermark 0
2023-07-30 13:49:47,833 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=primary.test-topic-1-1, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:47,833 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition primary.test-topic-1-1 in /tmp/junit7660765717748375067/junit866559250785637651/primary.test-topic-1-1 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:47,833 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.test-topic-1-1 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-1-1
2023-07-30 13:49:47,834 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.test-topic-1-1 broker=0] Log loaded for partition primary.test-topic-1-1 with initial high watermark 0
2023-07-30 13:49:47,841 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=primary.test-topic-1-9, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:47,842 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition primary.test-topic-1-9 in /tmp/junit7660765717748375067/junit866559250785637651/primary.test-topic-1-9 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:47,842 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.test-topic-1-9 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-1-9
2023-07-30 13:49:47,842 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.test-topic-1-9 broker=0] Log loaded for partition primary.test-topic-1-9 with initial high watermark 0
2023-07-30 13:49:47,848 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:43163/connectors/MirrorSourceConnector/status is {"error_code":404,"message":"No status found for connector MirrorSourceConnector"}
2023-07-30 13:49:47,848 - ERROR [main:EmbeddedConnectClusterAssertions@420] - Could not check connector state info.
org.apache.kafka.connect.runtime.rest.errors.ConnectRestException: Could not read connector state. Error response: {"error_code":404,"message":"No status found for connector MirrorSourceConnector"}
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.connectorStatus(EmbeddedConnectCluster.java:459)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectClusterAssertions.checkConnectorState(EmbeddedConnectClusterAssertions.java:413)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectClusterAssertions.lambda$assertConnectorAndAtLeastNumTasksAreRunning$16(EmbeddedConnectClusterAssertions.java:286)
	at org.apache.kafka.test.TestUtils.lambda$waitForCondition$6(TestUtils.java:400)
	at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:448)
	at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:416)
	at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:397)
	at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:387)
	at org.apache.kafka.connect.util.clusters.EmbeddedConnectClusterAssertions.assertConnectorAndAtLeastNumTasksAreRunning(EmbeddedConnectClusterAssertions.java:285)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.waitUntilMirrorMakerIsRunning(MirrorConnectorsIntegrationTest.java:191)
	at org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.testReplicationWithEmptyPartition(MirrorConnectorsIntegrationTest.java:367)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.executeTests(ConsoleTestExecutor.java:66)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.lambda$execute$0(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.replaceThreadContextClassLoaderAndInvoke(CustomContextClassLoaderExecutor.java:41)
	at org.junit.platform.console.tasks.CustomContextClassLoaderExecutor.invoke(CustomContextClassLoaderExecutor.java:31)
	at org.junit.platform.console.tasks.ConsoleTestExecutor.execute(ConsoleTestExecutor.java:58)
	at org.junit.platform.console.ConsoleLauncher.executeTests(ConsoleLauncher.java:95)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:73)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:50)
	at org.junit.platform.console.ConsoleLauncher.execute(ConsoleLauncher.java:43)
	at org.junit.platform.console.ConsoleLauncher.main(ConsoleLauncher.java:37)
2023-07-30 13:49:47,849 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=primary.test-topic-1-8, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:47,850 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition primary.test-topic-1-8 in /tmp/junit7660765717748375067/junit866559250785637651/primary.test-topic-1-8 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:47,850 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.test-topic-1-8 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-1-8
2023-07-30 13:49:47,850 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.test-topic-1-8 broker=0] Log loaded for partition primary.test-topic-1-8 with initial high watermark 0
2023-07-30 13:49:47,858 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=primary.test-topic-1-7, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:47,858 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition primary.test-topic-1-7 in /tmp/junit7660765717748375067/junit866559250785637651/primary.test-topic-1-7 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:47,858 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.test-topic-1-7 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-1-7
2023-07-30 13:49:47,858 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.test-topic-1-7 broker=0] Log loaded for partition primary.test-topic-1-7 with initial high watermark 0
2023-07-30 13:49:47,866 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=primary.test-topic-1-6, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:47,866 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition primary.test-topic-1-6 in /tmp/junit7660765717748375067/junit866559250785637651/primary.test-topic-1-6 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:47,867 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.test-topic-1-6 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-1-6
2023-07-30 13:49:47,867 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.test-topic-1-6 broker=0] Log loaded for partition primary.test-topic-1-6 with initial high watermark 0
2023-07-30 13:49:47,875 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Log partition=primary.test-topic-1-5, dir=/tmp/junit7660765717748375067/junit866559250785637651] Loading producer state till offset 0 with message format version 2
2023-07-30 13:49:47,876 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - Created log for partition primary.test-topic-1-5 in /tmp/junit7660765717748375067/junit866559250785637651/primary.test-topic-1-5 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
2023-07-30 13:49:47,876 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.test-topic-1-5 broker=0] No checkpointed highwatermark is found for partition primary.test-topic-1-5
2023-07-30 13:49:47,876 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [Partition primary.test-topic-1-5 broker=0] Log loaded for partition primary.test-topic-1-5 with initial high watermark 0
2023-07-30 13:49:47,886 - INFO  [kafka-admin-client-thread | adminclient-69:MirrorSourceConnector@317] - Increased size of primary.test-topic-1 to 10 partitions.
2023-07-30 13:49:47,888 - INFO  [Scheduler for MirrorSourceConnector-refreshing known target topics:Scheduler@95] - refreshing known target topics took 215 ms
2023-07-30 13:49:47,889 - INFO  [connector-thread-MirrorSourceConnector:MirrorSourceConnector@126] - Started MirrorSourceConnector with 21 topic-partitions.
2023-07-30 13:49:47,890 - INFO  [connector-thread-MirrorSourceConnector:MirrorSourceConnector@127] - Starting MirrorSourceConnector took 267 ms.
2023-07-30 13:49:47,892 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:49:47,892 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:49:47,897 - INFO  [Scheduler for MirrorSourceConnector-syncing topic configs:Scheduler@95] - syncing topic configs took 8 ms
2023-07-30 13:49:47,904 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Admin Manager on Broker 0]: Updating topic primary.test-topic-with-empty-partition with new configuration kafka.server.KafkaConfig@a5f0dd93
2023-07-30 13:49:47,906 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Admin Manager on Broker 0]: Updating topic primary.test-topic-1 with new configuration kafka.server.KafkaConfig@a5f0dd93
2023-07-30 13:49:47,907 - INFO  [/config/changes-event-process-thread:Logging@66] - Processing notification(s) to /config/changes
2023-07-30 13:49:47,908 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [Admin Manager on Broker 0]: Updating topic primary.heartbeats with new configuration kafka.server.KafkaConfig@a5f0dd93
2023-07-30 13:49:47,910 - INFO  [/config/changes-event-process-thread:Logging@66] - Processing override for entityPath: topics/primary.test-topic-with-empty-partition with config: HashMap()
2023-07-30 13:49:47,916 - INFO  [/config/changes-event-process-thread:Logging@66] - Processing notification(s) to /config/changes
2023-07-30 13:49:47,916 - INFO  [/config/changes-event-process-thread:Logging@66] - Processing override for entityPath: topics/primary.test-topic-1 with config: HashMap()
2023-07-30 13:49:47,917 - INFO  [/config/changes-event-process-thread:Logging@66] - Processing override for entityPath: topics/primary.heartbeats with config: HashMap()
2023-07-30 13:49:47,955 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:43163/connectors/MirrorSourceConnector/status is {"name":"MirrorSourceConnector","connector":{"state":"RUNNING","worker_id":"localhost:43163"},"tasks":[],"type":"source"}
2023-07-30 13:49:48,061 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:43163/connectors/MirrorSourceConnector/status is {"name":"MirrorSourceConnector","connector":{"state":"RUNNING","worker_id":"localhost:43163"},"tasks":[],"type":"source"}
2023-07-30 13:49:48,166 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:43163/connectors/MirrorSourceConnector/status is {"name":"MirrorSourceConnector","connector":{"state":"RUNNING","worker_id":"localhost:43163"},"tasks":[],"type":"source"}
2023-07-30 13:49:48,218 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1548] - [Worker clientId=connect-4, groupId=primary-mm2] Tasks [MirrorHeartbeatConnector-0] configs updated
2023-07-30 13:49:48,218 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1548] - [Worker clientId=connect-6, groupId=primary-mm2] Tasks [MirrorHeartbeatConnector-0] configs updated
2023-07-30 13:49:48,218 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1548] - [Worker clientId=connect-5, groupId=primary-mm2] Tasks [MirrorHeartbeatConnector-0] configs updated
2023-07-30 13:49:48,218 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@646] - [Worker clientId=connect-6, groupId=primary-mm2] Handling task config update by restarting tasks []
2023-07-30 13:49:48,218 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@646] - [Worker clientId=connect-5, groupId=primary-mm2] Handling task config update by restarting tasks []
2023-07-30 13:49:48,218 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@646] - [Worker clientId=connect-4, groupId=primary-mm2] Handling task config update by restarting tasks []
2023-07-30 13:49:48,219 - INFO  [DistributedHerder-connect-5-1:WorkerCoordinator@225] - [Worker clientId=connect-5, groupId=primary-mm2] Rebalance started
2023-07-30 13:49:48,219 - INFO  [DistributedHerder-connect-6-1:WorkerCoordinator@225] - [Worker clientId=connect-6, groupId=primary-mm2] Rebalance started
2023-07-30 13:49:48,219 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator@534] - [Worker clientId=connect-5, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:49:48,219 - INFO  [DistributedHerder-connect-4-1:WorkerCoordinator@225] - [Worker clientId=connect-4, groupId=primary-mm2] Rebalance started
2023-07-30 13:49:48,219 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator@534] - [Worker clientId=connect-6, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:49:48,219 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator@534] - [Worker clientId=connect-4, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:49:48,219 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group primary-mm2 in state PreparingRebalance with old generation 5 (__consumer_offsets-17) (reason: leader connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175 re-joining group during Stable)
2023-07-30 13:49:48,221 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupCoordinator 0]: Stabilized group primary-mm2 generation 6 (__consumer_offsets-17)
2023-07-30 13:49:48,222 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=6, memberId='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', protocol='sessioned'}
2023-07-30 13:49:48,222 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=6, memberId='connect-4-4677c970-fed2-41c6-8269-6c02eca18a2c', protocol='sessioned'}
2023-07-30 13:49:48,222 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=6, memberId='connect-6-5d9250b5-104e-4755-a5c9-fa2e5283a564', protocol='sessioned'}
2023-07-30 13:49:48,224 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group primary-mm2 for generation 6
2023-07-30 13:49:48,225 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=6, memberId='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', protocol='sessioned'}
2023-07-30 13:49:48,226 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=6, memberId='connect-4-4677c970-fed2-41c6-8269-6c02eca18a2c', protocol='sessioned'}
2023-07-30 13:49:48,226 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=6, memberId='connect-6-5d9250b5-104e-4755-a5c9-fa2e5283a564', protocol='sessioned'}
2023-07-30 13:49:48,226 - INFO  [DistributedHerder-connect-4-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-4, groupId=primary-mm2] Joined group at generation 6 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', leaderUrl='http://localhost:39987/', offset=6, connectorIds=[MirrorSourceConnector], taskIds=[MirrorHeartbeatConnector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:49:48,226 - INFO  [DistributedHerder-connect-6-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-6, groupId=primary-mm2] Joined group at generation 6 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', leaderUrl='http://localhost:39987/', offset=6, connectorIds=[MirrorHeartbeatConnector], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:49:48,226 - INFO  [DistributedHerder-connect-5-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-5, groupId=primary-mm2] Joined group at generation 6 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', leaderUrl='http://localhost:39987/', offset=6, connectorIds=[MirrorCheckpointConnector], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:49:48,226 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1215] - [Worker clientId=connect-6, groupId=primary-mm2] Starting connectors and tasks using config offset 6
2023-07-30 13:49:48,226 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1215] - [Worker clientId=connect-4, groupId=primary-mm2] Starting connectors and tasks using config offset 6
2023-07-30 13:49:48,226 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1243] - [Worker clientId=connect-6, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:49:48,227 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1215] - [Worker clientId=connect-5, groupId=primary-mm2] Starting connectors and tasks using config offset 6
2023-07-30 13:49:48,227 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1243] - [Worker clientId=connect-5, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:49:48,227 - INFO  [StartAndStopExecutor-connect-4-2:DistributedHerder@1257] - [Worker clientId=connect-4, groupId=primary-mm2] Starting task MirrorHeartbeatConnector-0
2023-07-30 13:49:48,227 - INFO  [DistributedHerder-connect-5-1:AbstractConfig@361] - SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorCheckpointConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorCheckpointConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:49:48,227 - INFO  [StartAndStopExecutor-connect-4-2:Worker@509] - Creating task MirrorHeartbeatConnector-0
2023-07-30 13:49:48,227 - INFO  [DistributedHerder-connect-5-1:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorCheckpointConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorCheckpointConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:49:48,228 - INFO  [StartAndStopExecutor-connect-4-2:AbstractConfig@361] - ConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorHeartbeatConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorHeartbeatConnector
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null

2023-07-30 13:49:48,228 - INFO  [StartAndStopExecutor-connect-4-2:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorHeartbeatConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorHeartbeatConnector
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null

2023-07-30 13:49:48,228 - INFO  [StartAndStopExecutor-connect-4-2:AbstractConfig@361] - TaskConfig values: 
	task.class = class org.apache.kafka.connect.mirror.MirrorHeartbeatTask

2023-07-30 13:49:48,229 - INFO  [StartAndStopExecutor-connect-4-2:Worker@524] - Instantiated task MirrorHeartbeatConnector-0 with version 1 of type org.apache.kafka.connect.mirror.MirrorHeartbeatTask
2023-07-30 13:49:48,229 - INFO  [StartAndStopExecutor-connect-4-2:Worker@537] - Set up the key converter class org.apache.kafka.connect.converters.ByteArrayConverter for task MirrorHeartbeatConnector-0 using the worker config
2023-07-30 13:49:48,229 - INFO  [StartAndStopExecutor-connect-4-2:Worker@543] - Set up the value converter class org.apache.kafka.connect.converters.ByteArrayConverter for task MirrorHeartbeatConnector-0 using the worker config
2023-07-30 13:49:48,229 - INFO  [StartAndStopExecutor-connect-4-2:Worker@550] - Set up the header converter class org.apache.kafka.connect.converters.ByteArrayConverter for task MirrorHeartbeatConnector-0 using the worker config
2023-07-30 13:49:48,234 - INFO  [StartAndStopExecutor-connect-4-2:AbstractConfig@361] - SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorHeartbeatConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorHeartbeatConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:49:48,242 - INFO  [StartAndStopExecutor-connect-4-2:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorHeartbeatConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorHeartbeatConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:49:48,243 - INFO  [StartAndStopExecutor-connect-4-2:Worker@606] - Initializing: org.apache.kafka.connect.runtime.TransformationChain{}
2023-07-30 13:49:48,244 - INFO  [StartAndStopExecutor-connect-4-2:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:45395]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-MirrorHeartbeatConnector-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 2147483647
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:49:48,246 - WARN  [StartAndStopExecutor-connect-4-2:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:48,246 - WARN  [StartAndStopExecutor-connect-4-2:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:49:48,246 - INFO  [StartAndStopExecutor-connect-4-2:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:48,246 - INFO  [StartAndStopExecutor-connect-4-2:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:48,246 - INFO  [StartAndStopExecutor-connect-4-2:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739388246
2023-07-30 13:49:48,250 - INFO  [kafka-producer-network-thread | connector-producer-MirrorHeartbeatConnector-0:Metadata@279] - [Producer clientId=connector-producer-MirrorHeartbeatConnector-0] Cluster ID: l67hsn_BQke0DtmYXtLNog
2023-07-30 13:49:48,255 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1243] - [Worker clientId=connect-4, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:49:48,256 - INFO  [task-thread-MirrorHeartbeatConnector-0:AbstractConfig@361] - MirrorTaskConfig values: 
	admin.timeout.ms = 60000
	checkpoints.topic.replication.factor = 1
	config.action.reload = restart
	config.properties.blacklist = [follower\.replication\.throttled\.replicas, leader\.replication\.throttled\.replicas, message\.timestamp\.difference\.max\.ms, message\.timestamp\.type, unclean\.leader\.election\.enable, min\.insync\.replicas]
	config.property.filter.class = class org.apache.kafka.connect.mirror.DefaultConfigPropertyFilter
	connector.class = org.apache.kafka.connect.mirror.MirrorHeartbeatConnector
	consumer.poll.timeout.ms = 1000
	emit.checkpoints.enabled = true
	emit.checkpoints.interval.seconds = 1
	emit.heartbeats.enabled = true
	emit.heartbeats.interval.seconds = 1
	enabled = true
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	group.filter.class = class org.apache.kafka.connect.mirror.DefaultGroupFilter
	groups = [consumer-group-.*]
	groups.blacklist = [console-consumer-.*, connect-.*, __.*]
	header.converter = null
	heartbeats.topic.replication.factor = 1
	key.converter = null
	metric.reporters = null
	name = MirrorHeartbeatConnector
	offset-syncs.topic.replication.factor = 1
	offset.lag.max = 100
	predicates = []
	refresh.groups.enabled = true
	refresh.groups.interval.seconds = 1
	refresh.topics.enabled = true
	refresh.topics.interval.seconds = 1
	replication.factor = 1
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	source.cluster.alias = primary
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	sync.group.offsets.enabled = false
	sync.group.offsets.interval.seconds = 60
	sync.topic.acls.enabled = false
	sync.topic.acls.interval.seconds = 600
	sync.topic.configs.enabled = true
	sync.topic.configs.interval.seconds = 600
	target.cluster.alias = backup
	task.assigned.groups = null
	task.assigned.partitions = null
	tasks.max = 1
	topic.filter.class = class org.apache.kafka.connect.mirror.DefaultTopicFilter
	topics = [test-topic-.*, primary.test-topic-.*, backup.test-topic-.*]
	topics.blacklist = [.*[\-\.]internal, .*\.replica, __.*]
	transforms = []
	value.converter = null

2023-07-30 13:49:48,257 - INFO  [task-thread-MirrorHeartbeatConnector-0:WorkerSourceTask@233] - WorkerSourceTask{id=MirrorHeartbeatConnector-0} Source task finished initialization and start
2023-07-30 13:49:48,271 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:43163/connectors/MirrorSourceConnector/status is {"name":"MirrorSourceConnector","connector":{"state":"RUNNING","worker_id":"localhost:43163"},"tasks":[],"type":"source"}
2023-07-30 13:49:48,376 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:43163/connectors/MirrorSourceConnector/status is {"name":"MirrorSourceConnector","connector":{"state":"RUNNING","worker_id":"localhost:43163"},"tasks":[],"type":"source"}
2023-07-30 13:49:48,483 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:43163/connectors/MirrorSourceConnector/status is {"name":"MirrorSourceConnector","connector":{"state":"RUNNING","worker_id":"localhost:43163"},"tasks":[],"type":"source"}
2023-07-30 13:49:48,589 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:43163/connectors/MirrorSourceConnector/status is {"name":"MirrorSourceConnector","connector":{"state":"RUNNING","worker_id":"localhost:43163"},"tasks":[],"type":"source"}
2023-07-30 13:49:48,694 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:43163/connectors/MirrorSourceConnector/status is {"name":"MirrorSourceConnector","connector":{"state":"RUNNING","worker_id":"localhost:43163"},"tasks":[],"type":"source"}
2023-07-30 13:49:48,724 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1548] - [Worker clientId=connect-4, groupId=primary-mm2] Tasks [MirrorCheckpointConnector-0] configs updated
2023-07-30 13:49:48,724 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1548] - [Worker clientId=connect-6, groupId=primary-mm2] Tasks [MirrorCheckpointConnector-0] configs updated
2023-07-30 13:49:48,725 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@646] - [Worker clientId=connect-4, groupId=primary-mm2] Handling task config update by restarting tasks []
2023-07-30 13:49:48,725 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@646] - [Worker clientId=connect-6, groupId=primary-mm2] Handling task config update by restarting tasks []
2023-07-30 13:49:48,725 - INFO  [DistributedHerder-connect-4-1:WorkerCoordinator@225] - [Worker clientId=connect-4, groupId=primary-mm2] Rebalance started
2023-07-30 13:49:48,725 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1548] - [Worker clientId=connect-5, groupId=primary-mm2] Tasks [MirrorCheckpointConnector-0] configs updated
2023-07-30 13:49:48,725 - INFO  [DistributedHerder-connect-6-1:WorkerCoordinator@225] - [Worker clientId=connect-6, groupId=primary-mm2] Rebalance started
2023-07-30 13:49:48,725 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator@534] - [Worker clientId=connect-4, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:49:48,725 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator@534] - [Worker clientId=connect-6, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:49:48,726 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group primary-mm2 in state PreparingRebalance with old generation 6 (__consumer_offsets-17) (reason: Updating metadata for member connect-4-4677c970-fed2-41c6-8269-6c02eca18a2c during Stable)
2023-07-30 13:49:48,800 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:43163/connectors/MirrorSourceConnector/status is {"name":"MirrorSourceConnector","connector":{"state":"RUNNING","worker_id":"localhost:43163"},"tasks":[],"type":"source"}
2023-07-30 13:49:48,808 - INFO  [Scheduler for MirrorCheckpointConnector-refreshing consumer groups:Scheduler@95] - refreshing consumer groups took 1 ms
2023-07-30 13:49:48,893 - INFO  [Scheduler for MirrorSourceConnector-refreshing topics:MirrorSourceConnector@225] - Found 21 topic-partitions on primary. 0 are new. 1 were removed. Previously had 21.
2023-07-30 13:49:48,894 - INFO  [Scheduler for MirrorSourceConnector-refreshing topics:Scheduler@95] - refreshing topics took 5 ms
2023-07-30 13:49:48,905 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:43163/connectors/MirrorSourceConnector/status is {"name":"MirrorSourceConnector","connector":{"state":"RUNNING","worker_id":"localhost:43163"},"tasks":[],"type":"source"}
2023-07-30 13:49:49,011 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:43163/connectors/MirrorSourceConnector/status is {"name":"MirrorSourceConnector","connector":{"state":"RUNNING","worker_id":"localhost:43163"},"tasks":[],"type":"source"}
2023-07-30 13:49:49,116 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:43163/connectors/MirrorSourceConnector/status is {"name":"MirrorSourceConnector","connector":{"state":"RUNNING","worker_id":"localhost:43163"},"tasks":[],"type":"source"}
2023-07-30 13:49:49,222 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:43163/connectors/MirrorSourceConnector/status is {"name":"MirrorSourceConnector","connector":{"state":"RUNNING","worker_id":"localhost:43163"},"tasks":[],"type":"source"}
2023-07-30 13:49:49,231 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1548] - [Worker clientId=connect-4, groupId=primary-mm2] Tasks [MirrorSourceConnector-0] configs updated
2023-07-30 13:49:49,232 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1548] - [Worker clientId=connect-6, groupId=primary-mm2] Tasks [MirrorSourceConnector-0] configs updated
2023-07-30 13:49:49,232 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1548] - [Worker clientId=connect-5, groupId=primary-mm2] Tasks [MirrorSourceConnector-0] configs updated
2023-07-30 13:49:49,232 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@646] - [Worker clientId=connect-5, groupId=primary-mm2] Handling task config update by restarting tasks []
2023-07-30 13:49:49,232 - INFO  [DistributedHerder-connect-5-1:WorkerCoordinator@225] - [Worker clientId=connect-5, groupId=primary-mm2] Rebalance started
2023-07-30 13:49:49,232 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator@534] - [Worker clientId=connect-5, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:49:49,234 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [GroupCoordinator 0]: Stabilized group primary-mm2 generation 7 (__consumer_offsets-17)
2023-07-30 13:49:49,235 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=7, memberId='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', protocol='sessioned'}
2023-07-30 13:49:49,236 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=7, memberId='connect-6-5d9250b5-104e-4755-a5c9-fa2e5283a564', protocol='sessioned'}
2023-07-30 13:49:49,236 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=7, memberId='connect-4-4677c970-fed2-41c6-8269-6c02eca18a2c', protocol='sessioned'}
2023-07-30 13:49:49,237 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group primary-mm2 for generation 7
2023-07-30 13:49:49,238 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=7, memberId='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', protocol='sessioned'}
2023-07-30 13:49:49,238 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=7, memberId='connect-4-4677c970-fed2-41c6-8269-6c02eca18a2c', protocol='sessioned'}
2023-07-30 13:49:49,238 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=7, memberId='connect-6-5d9250b5-104e-4755-a5c9-fa2e5283a564', protocol='sessioned'}
2023-07-30 13:49:49,238 - INFO  [DistributedHerder-connect-4-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-4, groupId=primary-mm2] Joined group at generation 7 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', leaderUrl='http://localhost:39987/', offset=10, connectorIds=[MirrorSourceConnector], taskIds=[MirrorHeartbeatConnector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:49:49,238 - INFO  [DistributedHerder-connect-6-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-6, groupId=primary-mm2] Joined group at generation 7 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', leaderUrl='http://localhost:39987/', offset=10, connectorIds=[MirrorHeartbeatConnector], taskIds=[MirrorSourceConnector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:49:49,238 - INFO  [DistributedHerder-connect-5-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-5, groupId=primary-mm2] Joined group at generation 7 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', leaderUrl='http://localhost:39987/', offset=10, connectorIds=[MirrorCheckpointConnector], taskIds=[MirrorCheckpointConnector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:49:49,238 - WARN  [DistributedHerder-connect-6-1:DistributedHerder@1094] - [Worker clientId=connect-6, groupId=primary-mm2] Catching up to assignment's config offset.
2023-07-30 13:49:49,238 - WARN  [DistributedHerder-connect-4-1:DistributedHerder@1094] - [Worker clientId=connect-4, groupId=primary-mm2] Catching up to assignment's config offset.
2023-07-30 13:49:49,238 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1157] - [Worker clientId=connect-6, groupId=primary-mm2] Current config state offset 8 is behind group assignment 10, reading to end of config log
2023-07-30 13:49:49,238 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1157] - [Worker clientId=connect-4, groupId=primary-mm2] Current config state offset 8 is behind group assignment 10, reading to end of config log
2023-07-30 13:49:49,238 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1215] - [Worker clientId=connect-5, groupId=primary-mm2] Starting connectors and tasks using config offset 10
2023-07-30 13:49:49,239 - INFO  [StartAndStopExecutor-connect-5-2:DistributedHerder@1257] - [Worker clientId=connect-5, groupId=primary-mm2] Starting task MirrorCheckpointConnector-0
2023-07-30 13:49:49,239 - INFO  [StartAndStopExecutor-connect-5-2:Worker@509] - Creating task MirrorCheckpointConnector-0
2023-07-30 13:49:49,239 - INFO  [StartAndStopExecutor-connect-5-2:AbstractConfig@361] - ConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorCheckpointConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorCheckpointConnector
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null

2023-07-30 13:49:49,240 - INFO  [StartAndStopExecutor-connect-5-2:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorCheckpointConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorCheckpointConnector
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null

2023-07-30 13:49:49,240 - INFO  [StartAndStopExecutor-connect-5-2:AbstractConfig@361] - TaskConfig values: 
	task.class = class org.apache.kafka.connect.mirror.MirrorCheckpointTask

2023-07-30 13:49:49,241 - INFO  [StartAndStopExecutor-connect-5-2:Worker@524] - Instantiated task MirrorCheckpointConnector-0 with version 1 of type org.apache.kafka.connect.mirror.MirrorCheckpointTask
2023-07-30 13:49:49,242 - INFO  [StartAndStopExecutor-connect-5-2:Worker@537] - Set up the key converter class org.apache.kafka.connect.converters.ByteArrayConverter for task MirrorCheckpointConnector-0 using the worker config
2023-07-30 13:49:49,242 - INFO  [StartAndStopExecutor-connect-5-2:Worker@543] - Set up the value converter class org.apache.kafka.connect.converters.ByteArrayConverter for task MirrorCheckpointConnector-0 using the worker config
2023-07-30 13:49:49,242 - INFO  [StartAndStopExecutor-connect-5-2:Worker@550] - Set up the header converter class org.apache.kafka.connect.converters.ByteArrayConverter for task MirrorCheckpointConnector-0 using the worker config
2023-07-30 13:49:49,242 - INFO  [StartAndStopExecutor-connect-5-2:AbstractConfig@361] - SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorCheckpointConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorCheckpointConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:49:49,242 - INFO  [StartAndStopExecutor-connect-5-2:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorCheckpointConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorCheckpointConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:49:49,242 - INFO  [StartAndStopExecutor-connect-5-2:Worker@606] - Initializing: org.apache.kafka.connect.runtime.TransformationChain{}
2023-07-30 13:49:49,242 - INFO  [StartAndStopExecutor-connect-5-2:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:45395]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-MirrorCheckpointConnector-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 2147483647
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:49:49,244 - WARN  [StartAndStopExecutor-connect-5-2:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:49,245 - WARN  [StartAndStopExecutor-connect-5-2:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:49:49,245 - INFO  [StartAndStopExecutor-connect-5-2:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:49,245 - INFO  [StartAndStopExecutor-connect-5-2:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:49,245 - INFO  [StartAndStopExecutor-connect-5-2:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739389245
2023-07-30 13:49:49,246 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1243] - [Worker clientId=connect-5, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:49:49,248 - INFO  [task-thread-MirrorCheckpointConnector-0:AbstractConfig@361] - MirrorTaskConfig values: 
	admin.timeout.ms = 60000
	checkpoints.topic.replication.factor = 1
	config.action.reload = restart
	config.properties.blacklist = [follower\.replication\.throttled\.replicas, leader\.replication\.throttled\.replicas, message\.timestamp\.difference\.max\.ms, message\.timestamp\.type, unclean\.leader\.election\.enable, min\.insync\.replicas]
	config.property.filter.class = class org.apache.kafka.connect.mirror.DefaultConfigPropertyFilter
	connector.class = org.apache.kafka.connect.mirror.MirrorCheckpointConnector
	consumer.poll.timeout.ms = 1000
	emit.checkpoints.enabled = true
	emit.checkpoints.interval.seconds = 1
	emit.heartbeats.enabled = true
	emit.heartbeats.interval.seconds = 1
	enabled = true
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	group.filter.class = class org.apache.kafka.connect.mirror.DefaultGroupFilter
	groups = [consumer-group-.*]
	groups.blacklist = [console-consumer-.*, connect-.*, __.*]
	header.converter = null
	heartbeats.topic.replication.factor = 1
	key.converter = null
	metric.reporters = null
	name = MirrorCheckpointConnector
	offset-syncs.topic.replication.factor = 1
	offset.lag.max = 100
	predicates = []
	refresh.groups.enabled = true
	refresh.groups.interval.seconds = 1
	refresh.topics.enabled = true
	refresh.topics.interval.seconds = 1
	replication.factor = 1
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	source.cluster.alias = primary
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	sync.group.offsets.enabled = false
	sync.group.offsets.interval.seconds = 60
	sync.topic.acls.enabled = false
	sync.topic.acls.interval.seconds = 600
	sync.topic.configs.enabled = true
	sync.topic.configs.interval.seconds = 600
	target.cluster.alias = backup
	task.assigned.groups = [consumer-group-testReplicationWithEmptyPartition, consumer-group-dummy]
	task.assigned.partitions = null
	tasks.max = 1
	topic.filter.class = class org.apache.kafka.connect.mirror.DefaultTopicFilter
	topics = [test-topic-.*, primary.test-topic-.*, backup.test-topic-.*]
	topics.blacklist = [.*[\-\.]internal, .*\.replica, __.*]
	transforms = []
	value.converter = null

2023-07-30 13:49:49,248 - INFO  [kafka-producer-network-thread | connector-producer-MirrorCheckpointConnector-0:Metadata@279] - [Producer clientId=connector-producer-MirrorCheckpointConnector-0] Cluster ID: l67hsn_BQke0DtmYXtLNog
2023-07-30 13:49:49,249 - INFO  [task-thread-MirrorCheckpointConnector-0:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:35687]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-null-22
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:49:49,250 - INFO  [task-thread-MirrorCheckpointConnector-0:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:49,250 - INFO  [task-thread-MirrorCheckpointConnector-0:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:49,250 - INFO  [task-thread-MirrorCheckpointConnector-0:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739389250
2023-07-30 13:49:49,250 - INFO  [task-thread-MirrorCheckpointConnector-0:KafkaConsumer@1116] - [Consumer clientId=consumer-null-22, groupId=null] Subscribed to partition(s): mm2-offset-syncs.backup.internal-0
2023-07-30 13:49:49,250 - INFO  [task-thread-MirrorCheckpointConnector-0:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:35687]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:49,251 - INFO  [task-thread-MirrorCheckpointConnector-0:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:49,251 - INFO  [task-thread-MirrorCheckpointConnector-0:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:49,251 - INFO  [task-thread-MirrorCheckpointConnector-0:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739389251
2023-07-30 13:49:49,251 - INFO  [task-thread-MirrorCheckpointConnector-0:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:45395]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:49,252 - INFO  [task-thread-MirrorCheckpointConnector-0:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:49,252 - INFO  [task-thread-MirrorCheckpointConnector-0:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:49,252 - INFO  [task-thread-MirrorCheckpointConnector-0:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739389252
2023-07-30 13:49:49,255 - INFO  [task-thread-MirrorCheckpointConnector-0:WorkerSourceTask@233] - WorkerSourceTask{id=MirrorCheckpointConnector-0} Source task finished initialization and start
2023-07-30 13:49:49,258 - INFO  [task-thread-MirrorCheckpointConnector-0:Metadata@279] - [Consumer clientId=consumer-null-22, groupId=null] Cluster ID: XnnaS7BkQ22Tt1TiwnBd0Q
2023-07-30 13:49:49,259 - INFO  [task-thread-MirrorCheckpointConnector-0:SubscriptionState@396] - [Consumer clientId=consumer-null-22, groupId=null] Resetting offset for partition mm2-offset-syncs.backup.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35687 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:49,328 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:43163/connectors/MirrorSourceConnector/status is {"name":"MirrorSourceConnector","connector":{"state":"RUNNING","worker_id":"localhost:43163"},"tasks":[],"type":"source"}
2023-07-30 13:49:49,433 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:43163/connectors/MirrorSourceConnector/status is {"name":"MirrorSourceConnector","connector":{"state":"RUNNING","worker_id":"localhost:43163"},"tasks":[],"type":"source"}
2023-07-30 13:49:49,539 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:43163/connectors/MirrorSourceConnector/status is {"name":"MirrorSourceConnector","connector":{"state":"RUNNING","worker_id":"localhost:43163"},"tasks":[],"type":"source"}
2023-07-30 13:49:49,645 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:43163/connectors/MirrorSourceConnector/status is {"name":"MirrorSourceConnector","connector":{"state":"RUNNING","worker_id":"localhost:43163"},"tasks":[],"type":"source"}
2023-07-30 13:49:49,733 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1161] - [Worker clientId=connect-4, groupId=primary-mm2] Finished reading to end of log and updated config snapshot, new config log offset: 10
2023-07-30 13:49:49,733 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1161] - [Worker clientId=connect-6, groupId=primary-mm2] Finished reading to end of log and updated config snapshot, new config log offset: 10
2023-07-30 13:49:49,733 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1215] - [Worker clientId=connect-4, groupId=primary-mm2] Starting connectors and tasks using config offset 10
2023-07-30 13:49:49,733 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1215] - [Worker clientId=connect-6, groupId=primary-mm2] Starting connectors and tasks using config offset 10
2023-07-30 13:49:49,733 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1243] - [Worker clientId=connect-4, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:49:49,733 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:49:49,733 - INFO  [StartAndStopExecutor-connect-6-2:DistributedHerder@1257] - [Worker clientId=connect-6, groupId=primary-mm2] Starting task MirrorSourceConnector-0
2023-07-30 13:49:49,733 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:49:49,734 - INFO  [StartAndStopExecutor-connect-6-2:Worker@509] - Creating task MirrorSourceConnector-0
2023-07-30 13:49:49,734 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@646] - [Worker clientId=connect-4, groupId=primary-mm2] Handling task config update by restarting tasks []
2023-07-30 13:49:49,734 - INFO  [StartAndStopExecutor-connect-6-2:AbstractConfig@361] - ConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null

2023-07-30 13:49:49,734 - INFO  [DistributedHerder-connect-4-1:WorkerCoordinator@225] - [Worker clientId=connect-4, groupId=primary-mm2] Rebalance started
2023-07-30 13:49:49,734 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator@534] - [Worker clientId=connect-4, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:49:49,734 - INFO  [StartAndStopExecutor-connect-6-2:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null

2023-07-30 13:49:49,735 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group primary-mm2 in state PreparingRebalance with old generation 7 (__consumer_offsets-17) (reason: Updating metadata for member connect-4-4677c970-fed2-41c6-8269-6c02eca18a2c during Stable)
2023-07-30 13:49:49,735 - INFO  [StartAndStopExecutor-connect-6-2:AbstractConfig@361] - TaskConfig values: 
	task.class = class org.apache.kafka.connect.mirror.MirrorSourceTask

2023-07-30 13:49:49,735 - INFO  [StartAndStopExecutor-connect-6-2:Worker@524] - Instantiated task MirrorSourceConnector-0 with version 1 of type org.apache.kafka.connect.mirror.MirrorSourceTask
2023-07-30 13:49:49,735 - INFO  [StartAndStopExecutor-connect-6-2:Worker@537] - Set up the key converter class org.apache.kafka.connect.converters.ByteArrayConverter for task MirrorSourceConnector-0 using the worker config
2023-07-30 13:49:49,735 - INFO  [StartAndStopExecutor-connect-6-2:Worker@543] - Set up the value converter class org.apache.kafka.connect.converters.ByteArrayConverter for task MirrorSourceConnector-0 using the worker config
2023-07-30 13:49:49,735 - INFO  [StartAndStopExecutor-connect-6-2:Worker@550] - Set up the header converter class org.apache.kafka.connect.converters.ByteArrayConverter for task MirrorSourceConnector-0 using the worker config
2023-07-30 13:49:49,735 - INFO  [StartAndStopExecutor-connect-6-2:AbstractConfig@361] - SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:49:49,736 - INFO  [StartAndStopExecutor-connect-6-2:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:49:49,736 - INFO  [StartAndStopExecutor-connect-6-2:Worker@606] - Initializing: org.apache.kafka.connect.runtime.TransformationChain{}
2023-07-30 13:49:49,736 - INFO  [StartAndStopExecutor-connect-6-2:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:45395]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-MirrorSourceConnector-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 2147483647
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:49:49,737 - WARN  [StartAndStopExecutor-connect-6-2:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:49,737 - WARN  [StartAndStopExecutor-connect-6-2:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:49:49,738 - INFO  [StartAndStopExecutor-connect-6-2:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:49,738 - INFO  [StartAndStopExecutor-connect-6-2:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:49,739 - INFO  [StartAndStopExecutor-connect-6-2:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739389738
2023-07-30 13:49:49,740 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1243] - [Worker clientId=connect-6, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:49:49,741 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@646] - [Worker clientId=connect-6, groupId=primary-mm2] Handling task config update by restarting tasks [MirrorSourceConnector-0]
2023-07-30 13:49:49,742 - INFO  [DistributedHerder-connect-6-1:Worker@836] - Stopping task MirrorSourceConnector-0
2023-07-30 13:49:49,742 - INFO  [kafka-producer-network-thread | connector-producer-MirrorSourceConnector-0:Metadata@279] - [Producer clientId=connector-producer-MirrorSourceConnector-0] Cluster ID: l67hsn_BQke0DtmYXtLNog
2023-07-30 13:49:49,743 - INFO  [task-thread-MirrorSourceConnector-0:AbstractConfig@361] - MirrorTaskConfig values: 
	admin.timeout.ms = 60000
	checkpoints.topic.replication.factor = 1
	config.action.reload = restart
	config.properties.blacklist = [follower\.replication\.throttled\.replicas, leader\.replication\.throttled\.replicas, message\.timestamp\.difference\.max\.ms, message\.timestamp\.type, unclean\.leader\.election\.enable, min\.insync\.replicas]
	config.property.filter.class = class org.apache.kafka.connect.mirror.DefaultConfigPropertyFilter
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	consumer.poll.timeout.ms = 1000
	emit.checkpoints.enabled = true
	emit.checkpoints.interval.seconds = 1
	emit.heartbeats.enabled = true
	emit.heartbeats.interval.seconds = 1
	enabled = true
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	group.filter.class = class org.apache.kafka.connect.mirror.DefaultGroupFilter
	groups = [consumer-group-.*]
	groups.blacklist = [console-consumer-.*, connect-.*, __.*]
	header.converter = null
	heartbeats.topic.replication.factor = 1
	key.converter = null
	metric.reporters = null
	name = MirrorSourceConnector
	offset-syncs.topic.replication.factor = 1
	offset.lag.max = 100
	predicates = []
	refresh.groups.enabled = true
	refresh.groups.interval.seconds = 1
	refresh.topics.enabled = true
	refresh.topics.interval.seconds = 1
	replication.factor = 1
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	source.cluster.alias = primary
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	sync.group.offsets.enabled = false
	sync.group.offsets.interval.seconds = 60
	sync.topic.acls.enabled = false
	sync.topic.acls.interval.seconds = 600
	sync.topic.configs.enabled = true
	sync.topic.configs.interval.seconds = 600
	target.cluster.alias = backup
	task.assigned.groups = null
	task.assigned.partitions = [test-topic-with-empty-partition-0, test-topic-with-empty-partition-1, test-topic-with-empty-partition-2, test-topic-with-empty-partition-3, test-topic-with-empty-partition-4, test-topic-with-empty-partition-5, test-topic-with-empty-partition-6, test-topic-with-empty-partition-7, test-topic-with-empty-partition-8, test-topic-with-empty-partition-9, test-topic-1-0, test-topic-1-1, test-topic-1-2, test-topic-1-3, test-topic-1-4, test-topic-1-5, test-topic-1-6, test-topic-1-7, test-topic-1-8, test-topic-1-9, heartbeats-0]
	tasks.max = 1
	topic.filter.class = class org.apache.kafka.connect.mirror.DefaultTopicFilter
	topics = [test-topic-.*, primary.test-topic-.*, backup.test-topic-.*]
	topics.blacklist = [.*[\-\.]internal, .*\.replica, __.*]
	transforms = []
	value.converter = null

2023-07-30 13:49:49,747 - INFO  [task-thread-MirrorSourceConnector-0:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:35687]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-null-23
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:49:49,748 - INFO  [task-thread-MirrorSourceConnector-0:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:49,748 - INFO  [task-thread-MirrorSourceConnector-0:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:49,748 - INFO  [task-thread-MirrorSourceConnector-0:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739389748
2023-07-30 13:49:49,748 - INFO  [task-thread-MirrorSourceConnector-0:AbstractConfig@361] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:35687]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-21
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:49:49,750 - INFO  [task-thread-MirrorSourceConnector-0:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:49,751 - INFO  [task-thread-MirrorSourceConnector-0:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:49,751 - INFO  [task-thread-MirrorSourceConnector-0:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739389750
2023-07-30 13:49:49,751 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:43163/connectors/MirrorSourceConnector/status is {"name":"MirrorSourceConnector","connector":{"state":"RUNNING","worker_id":"localhost:43163"},"tasks":[{"id":0,"state":"RUNNING","worker_id":"localhost:41733"}],"type":"source"}
2023-07-30 13:49:49,752 - INFO  [kafka-producer-network-thread | producer-21:Metadata@279] - [Producer clientId=producer-21] Cluster ID: XnnaS7BkQ22Tt1TiwnBd0Q
2023-07-30 13:49:49,757 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:43163/connectors/MirrorHeartbeatConnector/status is {"name":"MirrorHeartbeatConnector","connector":{"state":"RUNNING","worker_id":"localhost:41733"},"tasks":[{"id":0,"state":"RUNNING","worker_id":"localhost:43163"}],"type":"source"}
2023-07-30 13:49:49,762 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:43163/connectors/MirrorCheckpointConnector/status is {"name":"MirrorCheckpointConnector","connector":{"state":"RUNNING","worker_id":"localhost:39987"},"tasks":[{"id":0,"state":"RUNNING","worker_id":"localhost:39987"}],"type":"source"}
2023-07-30 13:49:49,765 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:45395]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:49,765 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:49,766 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:49,766 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739389765
2023-07-30 13:49:49,767 - INFO  [main:AbstractConfig@361] - MirrorClientConfig values: 
	bootstrap.servers = localhost:45395
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:49,769 - INFO  [main:AbstractConfig@361] - MirrorClientConfig values: 
	bootstrap.servers = localhost:45395
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:49,769 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:45395]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:49,770 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.protocol' was supplied but isn't a known config.
2023-07-30 13:49:49,770 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.window.factor' was supplied but isn't a known config.
2023-07-30 13:49:49,770 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.min.period.seconds' was supplied but isn't a known config.
2023-07-30 13:49:49,770 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.buffer.seconds' was supplied but isn't a known config.
2023-07-30 13:49:49,770 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.enabled.protocols' was supplied but isn't a known config.
2023-07-30 13:49:49,770 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.ticket.renew.window.factor' was supplied but isn't a known config.
2023-07-30 13:49:49,770 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.kinit.cmd' was supplied but isn't a known config.
2023-07-30 13:49:49,770 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.truststore.type' was supplied but isn't a known config.
2023-07-30 13:49:49,770 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.ticket.renew.jitter' was supplied but isn't a known config.
2023-07-30 13:49:49,770 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.keystore.type' was supplied but isn't a known config.
2023-07-30 13:49:49,771 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.trustmanager.algorithm' was supplied but isn't a known config.
2023-07-30 13:49:49,771 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.keymanager.algorithm' was supplied but isn't a known config.
2023-07-30 13:49:49,771 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.min.time.before.relogin' was supplied but isn't a known config.
2023-07-30 13:49:49,771 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.window.jitter' was supplied but isn't a known config.
2023-07-30 13:49:49,771 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
2023-07-30 13:49:49,771 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:49,771 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:49,771 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739389771
2023-07-30 13:49:49,772 - INFO  [main:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:45395]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-null-24
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:49:49,773 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:49,773 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:49,773 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739389772
2023-07-30 13:49:49,773 - INFO  [main:KafkaConsumer@1116] - [Consumer clientId=consumer-null-24, groupId=null] Subscribed to partition(s): primary.checkpoints.internal-0
2023-07-30 13:49:49,773 - INFO  [main:SubscriptionState@618] - [Consumer clientId=consumer-null-24, groupId=null] Seeking to EARLIEST offset of partition primary.checkpoints.internal-0
2023-07-30 13:49:49,774 - INFO  [main:Metadata@279] - [Consumer clientId=consumer-null-24, groupId=null] Cluster ID: l67hsn_BQke0DtmYXtLNog
2023-07-30 13:49:49,776 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-null-24, groupId=null] Resetting offset for partition primary.checkpoints.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:49,776 - INFO  [main:MirrorClient@177] - Consumed 0 checkpoint records for consumer-group-testReplicationWithEmptyPartition from primary.checkpoints.internal.
2023-07-30 13:49:49,777 - INFO  [main:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:49,777 - INFO  [main:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:49,777 - INFO  [main:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:49,777 - INFO  [main:AppInfoParser@83] - App info kafka.consumer for consumer-null-24 unregistered
2023-07-30 13:49:49,778 - INFO  [kafka-admin-client-thread | adminclient-77:AppInfoParser@83] - App info kafka.admin.client for adminclient-77 unregistered
2023-07-30 13:49:49,778 - INFO  [kafka-admin-client-thread | adminclient-77:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:49,778 - INFO  [kafka-admin-client-thread | adminclient-77:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:49,778 - INFO  [kafka-admin-client-thread | adminclient-77:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:49,808 - INFO  [Scheduler for MirrorCheckpointConnector-refreshing consumer groups:Scheduler@95] - refreshing consumer groups took 1 ms
2023-07-30 13:49:49,882 - INFO  [main:AbstractConfig@361] - MirrorClientConfig values: 
	bootstrap.servers = localhost:45395
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:49,882 - INFO  [main:AbstractConfig@361] - MirrorClientConfig values: 
	bootstrap.servers = localhost:45395
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:49,883 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:45395]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:49,884 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.protocol' was supplied but isn't a known config.
2023-07-30 13:49:49,884 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.window.factor' was supplied but isn't a known config.
2023-07-30 13:49:49,884 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.min.period.seconds' was supplied but isn't a known config.
2023-07-30 13:49:49,884 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.buffer.seconds' was supplied but isn't a known config.
2023-07-30 13:49:49,884 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.enabled.protocols' was supplied but isn't a known config.
2023-07-30 13:49:49,884 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.ticket.renew.window.factor' was supplied but isn't a known config.
2023-07-30 13:49:49,884 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.kinit.cmd' was supplied but isn't a known config.
2023-07-30 13:49:49,884 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.truststore.type' was supplied but isn't a known config.
2023-07-30 13:49:49,884 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.ticket.renew.jitter' was supplied but isn't a known config.
2023-07-30 13:49:49,885 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.keystore.type' was supplied but isn't a known config.
2023-07-30 13:49:49,885 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.trustmanager.algorithm' was supplied but isn't a known config.
2023-07-30 13:49:49,885 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.keymanager.algorithm' was supplied but isn't a known config.
2023-07-30 13:49:49,885 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.min.time.before.relogin' was supplied but isn't a known config.
2023-07-30 13:49:49,885 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.window.jitter' was supplied but isn't a known config.
2023-07-30 13:49:49,885 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
2023-07-30 13:49:49,885 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:49,885 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:49,885 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739389885
2023-07-30 13:49:49,886 - INFO  [main:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:45395]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-null-25
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:49:49,887 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:49,887 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:49,887 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739389887
2023-07-30 13:49:49,887 - INFO  [main:KafkaConsumer@1116] - [Consumer clientId=consumer-null-25, groupId=null] Subscribed to partition(s): primary.checkpoints.internal-0
2023-07-30 13:49:49,887 - INFO  [main:SubscriptionState@618] - [Consumer clientId=consumer-null-25, groupId=null] Seeking to EARLIEST offset of partition primary.checkpoints.internal-0
2023-07-30 13:49:49,889 - INFO  [main:Metadata@279] - [Consumer clientId=consumer-null-25, groupId=null] Cluster ID: l67hsn_BQke0DtmYXtLNog
2023-07-30 13:49:49,891 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-null-25, groupId=null] Resetting offset for partition primary.checkpoints.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:49,891 - INFO  [main:MirrorClient@177] - Consumed 0 checkpoint records for consumer-group-testReplicationWithEmptyPartition from primary.checkpoints.internal.
2023-07-30 13:49:49,891 - INFO  [main:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:49,891 - INFO  [main:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:49,891 - INFO  [main:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:49,892 - INFO  [main:AppInfoParser@83] - App info kafka.consumer for consumer-null-25 unregistered
2023-07-30 13:49:49,892 - INFO  [kafka-admin-client-thread | adminclient-78:AppInfoParser@83] - App info kafka.admin.client for adminclient-78 unregistered
2023-07-30 13:49:49,893 - INFO  [kafka-admin-client-thread | adminclient-78:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:49,893 - INFO  [kafka-admin-client-thread | adminclient-78:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:49,893 - INFO  [kafka-admin-client-thread | adminclient-78:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:49,893 - INFO  [Scheduler for MirrorSourceConnector-refreshing topics:MirrorSourceConnector@225] - Found 21 topic-partitions on primary. 0 are new. 1 were removed. Previously had 21.
2023-07-30 13:49:49,894 - INFO  [Scheduler for MirrorSourceConnector-refreshing topics:Scheduler@95] - refreshing topics took 5 ms
2023-07-30 13:49:49,995 - INFO  [main:AbstractConfig@361] - MirrorClientConfig values: 
	bootstrap.servers = localhost:45395
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:49,995 - INFO  [main:AbstractConfig@361] - MirrorClientConfig values: 
	bootstrap.servers = localhost:45395
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:49,995 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:45395]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:49,996 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.protocol' was supplied but isn't a known config.
2023-07-30 13:49:49,996 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.window.factor' was supplied but isn't a known config.
2023-07-30 13:49:49,997 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.min.period.seconds' was supplied but isn't a known config.
2023-07-30 13:49:49,997 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.buffer.seconds' was supplied but isn't a known config.
2023-07-30 13:49:49,997 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.enabled.protocols' was supplied but isn't a known config.
2023-07-30 13:49:49,997 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.ticket.renew.window.factor' was supplied but isn't a known config.
2023-07-30 13:49:49,997 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.kinit.cmd' was supplied but isn't a known config.
2023-07-30 13:49:49,997 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.truststore.type' was supplied but isn't a known config.
2023-07-30 13:49:49,997 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.ticket.renew.jitter' was supplied but isn't a known config.
2023-07-30 13:49:49,997 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.keystore.type' was supplied but isn't a known config.
2023-07-30 13:49:49,997 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.trustmanager.algorithm' was supplied but isn't a known config.
2023-07-30 13:49:49,997 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.keymanager.algorithm' was supplied but isn't a known config.
2023-07-30 13:49:49,998 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.min.time.before.relogin' was supplied but isn't a known config.
2023-07-30 13:49:49,998 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.window.jitter' was supplied but isn't a known config.
2023-07-30 13:49:49,998 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
2023-07-30 13:49:49,998 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:49,998 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:49,998 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739389998
2023-07-30 13:49:49,999 - INFO  [main:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:45395]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-null-26
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:49:50,000 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:50,000 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:50,000 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739390000
2023-07-30 13:49:50,000 - INFO  [main:KafkaConsumer@1116] - [Consumer clientId=consumer-null-26, groupId=null] Subscribed to partition(s): primary.checkpoints.internal-0
2023-07-30 13:49:50,000 - INFO  [main:SubscriptionState@618] - [Consumer clientId=consumer-null-26, groupId=null] Seeking to EARLIEST offset of partition primary.checkpoints.internal-0
2023-07-30 13:49:50,002 - INFO  [main:Metadata@279] - [Consumer clientId=consumer-null-26, groupId=null] Cluster ID: l67hsn_BQke0DtmYXtLNog
2023-07-30 13:49:50,003 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-null-26, groupId=null] Resetting offset for partition primary.checkpoints.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:50,004 - INFO  [main:MirrorClient@177] - Consumed 0 checkpoint records for consumer-group-testReplicationWithEmptyPartition from primary.checkpoints.internal.
2023-07-30 13:49:50,004 - INFO  [main:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:50,004 - INFO  [main:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:50,004 - INFO  [main:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:50,005 - INFO  [main:AppInfoParser@83] - App info kafka.consumer for consumer-null-26 unregistered
2023-07-30 13:49:50,005 - INFO  [kafka-admin-client-thread | adminclient-79:AppInfoParser@83] - App info kafka.admin.client for adminclient-79 unregistered
2023-07-30 13:49:50,005 - INFO  [kafka-admin-client-thread | adminclient-79:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:50,005 - INFO  [kafka-admin-client-thread | adminclient-79:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:50,005 - INFO  [kafka-admin-client-thread | adminclient-79:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:50,107 - INFO  [main:AbstractConfig@361] - MirrorClientConfig values: 
	bootstrap.servers = localhost:45395
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:50,107 - INFO  [main:AbstractConfig@361] - MirrorClientConfig values: 
	bootstrap.servers = localhost:45395
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:50,107 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:45395]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:50,108 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.protocol' was supplied but isn't a known config.
2023-07-30 13:49:50,108 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.window.factor' was supplied but isn't a known config.
2023-07-30 13:49:50,108 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.min.period.seconds' was supplied but isn't a known config.
2023-07-30 13:49:50,108 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.buffer.seconds' was supplied but isn't a known config.
2023-07-30 13:49:50,108 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.enabled.protocols' was supplied but isn't a known config.
2023-07-30 13:49:50,109 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.ticket.renew.window.factor' was supplied but isn't a known config.
2023-07-30 13:49:50,109 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.kinit.cmd' was supplied but isn't a known config.
2023-07-30 13:49:50,109 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.truststore.type' was supplied but isn't a known config.
2023-07-30 13:49:50,109 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.ticket.renew.jitter' was supplied but isn't a known config.
2023-07-30 13:49:50,109 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.keystore.type' was supplied but isn't a known config.
2023-07-30 13:49:50,109 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.trustmanager.algorithm' was supplied but isn't a known config.
2023-07-30 13:49:50,109 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.keymanager.algorithm' was supplied but isn't a known config.
2023-07-30 13:49:50,109 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.min.time.before.relogin' was supplied but isn't a known config.
2023-07-30 13:49:50,109 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.window.jitter' was supplied but isn't a known config.
2023-07-30 13:49:50,109 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
2023-07-30 13:49:50,110 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:50,110 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:50,110 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739390110
2023-07-30 13:49:50,110 - INFO  [main:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:45395]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-null-27
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:49:50,111 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:50,111 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:50,111 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739390111
2023-07-30 13:49:50,112 - INFO  [main:KafkaConsumer@1116] - [Consumer clientId=consumer-null-27, groupId=null] Subscribed to partition(s): primary.checkpoints.internal-0
2023-07-30 13:49:50,112 - INFO  [main:SubscriptionState@618] - [Consumer clientId=consumer-null-27, groupId=null] Seeking to EARLIEST offset of partition primary.checkpoints.internal-0
2023-07-30 13:49:50,113 - INFO  [main:Metadata@279] - [Consumer clientId=consumer-null-27, groupId=null] Cluster ID: l67hsn_BQke0DtmYXtLNog
2023-07-30 13:49:50,115 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-null-27, groupId=null] Resetting offset for partition primary.checkpoints.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:50,115 - INFO  [main:MirrorClient@177] - Consumed 0 checkpoint records for consumer-group-testReplicationWithEmptyPartition from primary.checkpoints.internal.
2023-07-30 13:49:50,115 - INFO  [main:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:50,115 - INFO  [main:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:50,115 - INFO  [main:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:50,116 - INFO  [main:AppInfoParser@83] - App info kafka.consumer for consumer-null-27 unregistered
2023-07-30 13:49:50,116 - INFO  [kafka-admin-client-thread | adminclient-80:AppInfoParser@83] - App info kafka.admin.client for adminclient-80 unregistered
2023-07-30 13:49:50,117 - INFO  [kafka-admin-client-thread | adminclient-80:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:50,117 - INFO  [kafka-admin-client-thread | adminclient-80:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:50,117 - INFO  [kafka-admin-client-thread | adminclient-80:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:50,218 - INFO  [main:AbstractConfig@361] - MirrorClientConfig values: 
	bootstrap.servers = localhost:45395
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:50,218 - INFO  [main:AbstractConfig@361] - MirrorClientConfig values: 
	bootstrap.servers = localhost:45395
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:50,219 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:45395]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:50,219 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.protocol' was supplied but isn't a known config.
2023-07-30 13:49:50,220 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.window.factor' was supplied but isn't a known config.
2023-07-30 13:49:50,220 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.min.period.seconds' was supplied but isn't a known config.
2023-07-30 13:49:50,220 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.buffer.seconds' was supplied but isn't a known config.
2023-07-30 13:49:50,220 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.enabled.protocols' was supplied but isn't a known config.
2023-07-30 13:49:50,220 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.ticket.renew.window.factor' was supplied but isn't a known config.
2023-07-30 13:49:50,220 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.kinit.cmd' was supplied but isn't a known config.
2023-07-30 13:49:50,220 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.truststore.type' was supplied but isn't a known config.
2023-07-30 13:49:50,220 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.ticket.renew.jitter' was supplied but isn't a known config.
2023-07-30 13:49:50,220 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.keystore.type' was supplied but isn't a known config.
2023-07-30 13:49:50,220 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.trustmanager.algorithm' was supplied but isn't a known config.
2023-07-30 13:49:50,221 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.keymanager.algorithm' was supplied but isn't a known config.
2023-07-30 13:49:50,221 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.min.time.before.relogin' was supplied but isn't a known config.
2023-07-30 13:49:50,221 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.window.jitter' was supplied but isn't a known config.
2023-07-30 13:49:50,221 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
2023-07-30 13:49:50,221 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:50,221 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:50,221 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739390221
2023-07-30 13:49:50,222 - INFO  [main:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:45395]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-null-28
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:49:50,223 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:50,223 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:50,223 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739390223
2023-07-30 13:49:50,223 - INFO  [main:KafkaConsumer@1116] - [Consumer clientId=consumer-null-28, groupId=null] Subscribed to partition(s): primary.checkpoints.internal-0
2023-07-30 13:49:50,223 - INFO  [main:SubscriptionState@618] - [Consumer clientId=consumer-null-28, groupId=null] Seeking to EARLIEST offset of partition primary.checkpoints.internal-0
2023-07-30 13:49:50,224 - INFO  [main:Metadata@279] - [Consumer clientId=consumer-null-28, groupId=null] Cluster ID: l67hsn_BQke0DtmYXtLNog
2023-07-30 13:49:50,226 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-null-28, groupId=null] Resetting offset for partition primary.checkpoints.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:50,226 - INFO  [main:MirrorClient@177] - Consumed 0 checkpoint records for consumer-group-testReplicationWithEmptyPartition from primary.checkpoints.internal.
2023-07-30 13:49:50,226 - INFO  [main:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:50,226 - INFO  [main:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:50,226 - INFO  [main:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:50,227 - INFO  [main:AppInfoParser@83] - App info kafka.consumer for consumer-null-28 unregistered
2023-07-30 13:49:50,227 - INFO  [kafka-admin-client-thread | adminclient-81:AppInfoParser@83] - App info kafka.admin.client for adminclient-81 unregistered
2023-07-30 13:49:50,228 - INFO  [kafka-admin-client-thread | adminclient-81:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:50,228 - INFO  [kafka-admin-client-thread | adminclient-81:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:50,228 - INFO  [kafka-admin-client-thread | adminclient-81:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:50,329 - INFO  [main:AbstractConfig@361] - MirrorClientConfig values: 
	bootstrap.servers = localhost:45395
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:50,329 - INFO  [main:AbstractConfig@361] - MirrorClientConfig values: 
	bootstrap.servers = localhost:45395
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:50,330 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:45395]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:49:50,330 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.protocol' was supplied but isn't a known config.
2023-07-30 13:49:50,331 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.window.factor' was supplied but isn't a known config.
2023-07-30 13:49:50,331 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.min.period.seconds' was supplied but isn't a known config.
2023-07-30 13:49:50,331 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.buffer.seconds' was supplied but isn't a known config.
2023-07-30 13:49:50,331 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.enabled.protocols' was supplied but isn't a known config.
2023-07-30 13:49:50,331 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.ticket.renew.window.factor' was supplied but isn't a known config.
2023-07-30 13:49:50,331 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.kinit.cmd' was supplied but isn't a known config.
2023-07-30 13:49:50,331 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.truststore.type' was supplied but isn't a known config.
2023-07-30 13:49:50,331 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.ticket.renew.jitter' was supplied but isn't a known config.
2023-07-30 13:49:50,331 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.keystore.type' was supplied but isn't a known config.
2023-07-30 13:49:50,331 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.trustmanager.algorithm' was supplied but isn't a known config.
2023-07-30 13:49:50,331 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.keymanager.algorithm' was supplied but isn't a known config.
2023-07-30 13:49:50,331 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.kerberos.min.time.before.relogin' was supplied but isn't a known config.
2023-07-30 13:49:50,332 - WARN  [main:AbstractConfig@369] - The configuration 'sasl.login.refresh.window.jitter' was supplied but isn't a known config.
2023-07-30 13:49:50,332 - WARN  [main:AbstractConfig@369] - The configuration 'ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
2023-07-30 13:49:50,332 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:50,332 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:50,332 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739390332
2023-07-30 13:49:50,332 - INFO  [main:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:45395]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-null-29
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:49:50,333 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:50,333 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:50,334 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739390333
2023-07-30 13:49:50,334 - INFO  [main:KafkaConsumer@1116] - [Consumer clientId=consumer-null-29, groupId=null] Subscribed to partition(s): primary.checkpoints.internal-0
2023-07-30 13:49:50,334 - INFO  [main:SubscriptionState@618] - [Consumer clientId=consumer-null-29, groupId=null] Seeking to EARLIEST offset of partition primary.checkpoints.internal-0
2023-07-30 13:49:50,335 - INFO  [main:Metadata@279] - [Consumer clientId=consumer-null-29, groupId=null] Cluster ID: l67hsn_BQke0DtmYXtLNog
2023-07-30 13:49:50,337 - INFO  [main:SubscriptionState@396] - [Consumer clientId=consumer-null-29, groupId=null] Resetting offset for partition primary.checkpoints.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:45395 (id: 0 rack: null)], epoch=0}}.
2023-07-30 13:49:50,808 - INFO  [Scheduler for MirrorCheckpointConnector-refreshing consumer groups:Scheduler@95] - refreshing consumer groups took 1 ms
2023-07-30 13:49:50,841 - INFO  [main:MirrorClient@177] - Consumed 10 checkpoint records for consumer-group-testReplicationWithEmptyPartition from primary.checkpoints.internal.
2023-07-30 13:49:50,842 - INFO  [main:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:50,842 - INFO  [main:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:50,842 - INFO  [main:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:50,842 - INFO  [main:AppInfoParser@83] - App info kafka.consumer for consumer-null-29 unregistered
2023-07-30 13:49:50,843 - INFO  [kafka-admin-client-thread | adminclient-82:AppInfoParser@83] - App info kafka.admin.client for adminclient-82 unregistered
2023-07-30 13:49:50,843 - INFO  [kafka-admin-client-thread | adminclient-82:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:50,843 - INFO  [kafka-admin-client-thread | adminclient-82:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:50,843 - INFO  [kafka-admin-client-thread | adminclient-82:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:50,849 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:43083/connectors is []
2023-07-30 13:49:50,854 - INFO  [main:EmbeddedConnectCluster@689] - GET response for URL=http://localhost:43163/connectors is ["MirrorCheckpointConnector","MirrorSourceConnector","MirrorHeartbeatConnector"]
2023-07-30 13:49:50,893 - INFO  [Scheduler for MirrorSourceConnector-refreshing topics:MirrorSourceConnector@225] - Found 21 topic-partitions on primary. 0 are new. 1 were removed. Previously had 21.
2023-07-30 13:49:50,893 - INFO  [Scheduler for MirrorSourceConnector-refreshing topics:Scheduler@95] - refreshing topics took 4 ms
2023-07-30 13:49:51,808 - INFO  [Scheduler for MirrorCheckpointConnector-refreshing consumer groups:Scheduler@95] - refreshing consumer groups took 1 ms
2023-07-30 13:49:51,893 - INFO  [Scheduler for MirrorSourceConnector-refreshing topics:MirrorSourceConnector@225] - Found 21 topic-partitions on primary. 0 are new. 1 were removed. Previously had 21.
2023-07-30 13:49:51,893 - INFO  [Scheduler for MirrorSourceConnector-refreshing topics:Scheduler@95] - refreshing topics took 4 ms
2023-07-30 13:49:52,236 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$HeartbeatResponseHandler@1097] - [Worker clientId=connect-5, groupId=primary-mm2] Attempt to heartbeat failed since group is rebalancing
2023-07-30 13:49:52,236 - INFO  [DistributedHerder-connect-5-1:WorkerCoordinator@225] - [Worker clientId=connect-5, groupId=primary-mm2] Rebalance started
2023-07-30 13:49:52,236 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator@534] - [Worker clientId=connect-5, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:49:52,343 - INFO  [kafka-coordinator-heartbeat-thread | primary-mm2:AbstractCoordinator$HeartbeatResponseHandler@1097] - [Worker clientId=connect-6, groupId=primary-mm2] Attempt to heartbeat failed since group is rebalancing
2023-07-30 13:49:52,808 - INFO  [Scheduler for MirrorCheckpointConnector-refreshing consumer groups:Scheduler@95] - refreshing consumer groups took 1 ms
2023-07-30 13:49:52,892 - INFO  [Scheduler for MirrorSourceConnector-refreshing topics:MirrorSourceConnector@225] - Found 21 topic-partitions on primary. 0 are new. 1 were removed. Previously had 21.
2023-07-30 13:49:52,893 - INFO  [Scheduler for MirrorSourceConnector-refreshing topics:Scheduler@95] - refreshing topics took 4 ms
2023-07-30 13:49:53,808 - INFO  [Scheduler for MirrorCheckpointConnector-refreshing consumer groups:Scheduler@95] - refreshing consumer groups took 1 ms
2023-07-30 13:49:53,893 - INFO  [Scheduler for MirrorSourceConnector-refreshing topics:MirrorSourceConnector@225] - Found 21 topic-partitions on primary. 0 are new. 1 were removed. Previously had 21.
2023-07-30 13:49:53,893 - INFO  [Scheduler for MirrorSourceConnector-refreshing topics:Scheduler@95] - refreshing topics took 4 ms
2023-07-30 13:49:54,742 - ERROR [DistributedHerder-connect-6-1:Worker@867] - Graceful stop of task MirrorSourceConnector-0 failed.
2023-07-30 13:49:54,744 - ERROR [task-thread-MirrorSourceConnector-0:OffsetStorageReaderImpl@113] - Failed to fetch offsets from namespace MirrorSourceConnector: 
org.apache.kafka.connect.errors.ConnectException: Offset reader closed while attempting to read offsets. This is likely because the task was been scheduled to stop but has taken longer than the graceful shutdown period to do so.
	at org.apache.kafka.connect.storage.OffsetStorageReaderImpl.offsets(OffsetStorageReaderImpl.java:103)
	at org.apache.kafka.connect.storage.OffsetStorageReaderImpl.offset(OffsetStorageReaderImpl.java:63)
	at org.apache.kafka.connect.mirror.MirrorSourceTask.loadOffset(MirrorSourceTask.java:227)
	at org.apache.kafka.connect.mirror.MirrorSourceTask.lambda$loadOffsets$4(MirrorSourceTask.java:222)
	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321)
	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169)
	at java.util.HashMap$KeySpliterator.forEachRemaining(HashMap.java:1556)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566)
	at org.apache.kafka.connect.mirror.MirrorSourceTask.loadOffsets(MirrorSourceTask.java:222)
	at org.apache.kafka.connect.mirror.MirrorSourceTask.start(MirrorSourceTask.java:92)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.execute(WorkerSourceTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:235)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2023-07-30 13:49:54,745 - INFO  [task-thread-MirrorSourceConnector-0:WorkerSourceTask@478] - WorkerSourceTask{id=MirrorSourceConnector-0} Committing offsets
2023-07-30 13:49:54,745 - INFO  [task-thread-MirrorSourceConnector-0:WorkerSourceTask@495] - WorkerSourceTask{id=MirrorSourceConnector-0} flushing 0 outstanding messages for offset commit
2023-07-30 13:49:54,745 - ERROR [task-thread-MirrorSourceConnector-0:WorkerTask@187] - WorkerSourceTask{id=MirrorSourceConnector-0} Task threw an uncaught and unrecoverable exception
org.apache.kafka.connect.errors.ConnectException: Failed to fetch offsets.
	at org.apache.kafka.connect.storage.OffsetStorageReaderImpl.offsets(OffsetStorageReaderImpl.java:114)
	at org.apache.kafka.connect.storage.OffsetStorageReaderImpl.offset(OffsetStorageReaderImpl.java:63)
	at org.apache.kafka.connect.mirror.MirrorSourceTask.loadOffset(MirrorSourceTask.java:227)
	at org.apache.kafka.connect.mirror.MirrorSourceTask.lambda$loadOffsets$4(MirrorSourceTask.java:222)
	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321)
	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169)
	at java.util.HashMap$KeySpliterator.forEachRemaining(HashMap.java:1556)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566)
	at org.apache.kafka.connect.mirror.MirrorSourceTask.loadOffsets(MirrorSourceTask.java:222)
	at org.apache.kafka.connect.mirror.MirrorSourceTask.start(MirrorSourceTask.java:92)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.execute(WorkerSourceTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:235)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.connect.errors.ConnectException: Offset reader closed while attempting to read offsets. This is likely because the task was been scheduled to stop but has taken longer than the graceful shutdown period to do so.
	at org.apache.kafka.connect.storage.OffsetStorageReaderImpl.offsets(OffsetStorageReaderImpl.java:103)
	... 21 more
2023-07-30 13:49:54,745 - ERROR [task-thread-MirrorSourceConnector-0:WorkerTask@188] - WorkerSourceTask{id=MirrorSourceConnector-0} Task is being killed and will not recover until manually restarted
2023-07-30 13:49:54,746 - INFO  [DistributedHerder-connect-6-1:WorkerCoordinator@225] - [Worker clientId=connect-6, groupId=primary-mm2] Rebalance started
2023-07-30 13:49:54,746 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:54,746 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:54,746 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator@534] - [Worker clientId=connect-6, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:49:54,746 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:54,746 - INFO  [task-thread-MirrorSourceConnector-0:AppInfoParser@83] - App info kafka.consumer for consumer-null-23 unregistered
2023-07-30 13:49:54,746 - INFO  [task-thread-MirrorSourceConnector-0:KafkaProducer@1193] - [Producer clientId=producer-21] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:49:54,747 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupCoordinator 0]: Stabilized group primary-mm2 generation 8 (__consumer_offsets-17)
2023-07-30 13:49:54,748 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=8, memberId='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', protocol='sessioned'}
2023-07-30 13:49:54,748 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:54,748 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:54,748 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=8, memberId='connect-6-5d9250b5-104e-4755-a5c9-fa2e5283a564', protocol='sessioned'}
2023-07-30 13:49:54,748 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:54,748 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=8, memberId='connect-4-4677c970-fed2-41c6-8269-6c02eca18a2c', protocol='sessioned'}
2023-07-30 13:49:54,748 - INFO  [task-thread-MirrorSourceConnector-0:AppInfoParser@83] - App info kafka.producer for producer-21 unregistered
2023-07-30 13:49:54,748 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:54,748 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:54,749 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:54,749 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group primary-mm2 for generation 8
2023-07-30 13:49:54,749 - INFO  [task-thread-MirrorSourceConnector-0:MirrorSourceTask@120] - Stopping task-thread-MirrorSourceConnector-0 took 3 ms.
2023-07-30 13:49:54,749 - INFO  [task-thread-MirrorSourceConnector-0:KafkaProducer@1193] - [Producer clientId=connector-producer-MirrorSourceConnector-0] Closing the Kafka producer with timeoutMillis = 30000 ms.
2023-07-30 13:49:54,749 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:54,750 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:54,750 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=8, memberId='connect-6-5d9250b5-104e-4755-a5c9-fa2e5283a564', protocol='sessioned'}
2023-07-30 13:49:54,750 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=8, memberId='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', protocol='sessioned'}
2023-07-30 13:49:54,750 - INFO  [DistributedHerder-connect-6-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-6, groupId=primary-mm2] Joined group at generation 8 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', leaderUrl='http://localhost:39987/', offset=10, connectorIds=[MirrorHeartbeatConnector], taskIds=[MirrorSourceConnector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:49:54,750 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=8, memberId='connect-4-4677c970-fed2-41c6-8269-6c02eca18a2c', protocol='sessioned'}
2023-07-30 13:49:54,750 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:54,750 - INFO  [DistributedHerder-connect-4-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-4, groupId=primary-mm2] Joined group at generation 8 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', leaderUrl='http://localhost:39987/', offset=10, connectorIds=[MirrorSourceConnector], taskIds=[MirrorHeartbeatConnector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:49:54,750 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1215] - [Worker clientId=connect-6, groupId=primary-mm2] Starting connectors and tasks using config offset 10
2023-07-30 13:49:54,750 - INFO  [DistributedHerder-connect-5-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-5, groupId=primary-mm2] Joined group at generation 8 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', leaderUrl='http://localhost:39987/', offset=10, connectorIds=[MirrorCheckpointConnector], taskIds=[MirrorCheckpointConnector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:49:54,750 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1215] - [Worker clientId=connect-4, groupId=primary-mm2] Starting connectors and tasks using config offset 10
2023-07-30 13:49:54,750 - INFO  [task-thread-MirrorSourceConnector-0:AppInfoParser@83] - App info kafka.producer for connector-producer-MirrorSourceConnector-0 unregistered
2023-07-30 13:49:54,750 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1243] - [Worker clientId=connect-4, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:49:54,750 - INFO  [StartAndStopExecutor-connect-6-3:DistributedHerder@1257] - [Worker clientId=connect-6, groupId=primary-mm2] Starting task MirrorSourceConnector-0
2023-07-30 13:49:54,750 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1215] - [Worker clientId=connect-5, groupId=primary-mm2] Starting connectors and tasks using config offset 10
2023-07-30 13:49:54,751 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1243] - [Worker clientId=connect-5, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:49:54,751 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:49:54,751 - INFO  [StartAndStopExecutor-connect-6-3:Worker@509] - Creating task MirrorSourceConnector-0
2023-07-30 13:49:54,751 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:49:54,751 - INFO  [StartAndStopExecutor-connect-6-3:AbstractConfig@361] - ConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null

2023-07-30 13:49:54,751 - INFO  [StartAndStopExecutor-connect-6-3:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null

2023-07-30 13:49:54,751 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:49:54,751 - INFO  [StartAndStopExecutor-connect-6-3:AbstractConfig@361] - TaskConfig values: 
	task.class = class org.apache.kafka.connect.mirror.MirrorSourceTask

2023-07-30 13:49:54,751 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:49:54,751 - INFO  [StartAndStopExecutor-connect-6-3:Worker@524] - Instantiated task MirrorSourceConnector-0 with version 1 of type org.apache.kafka.connect.mirror.MirrorSourceTask
2023-07-30 13:49:54,752 - INFO  [StartAndStopExecutor-connect-6-3:Worker@537] - Set up the key converter class org.apache.kafka.connect.converters.ByteArrayConverter for task MirrorSourceConnector-0 using the worker config
2023-07-30 13:49:54,752 - INFO  [StartAndStopExecutor-connect-6-3:Worker@543] - Set up the value converter class org.apache.kafka.connect.converters.ByteArrayConverter for task MirrorSourceConnector-0 using the worker config
2023-07-30 13:49:54,752 - INFO  [StartAndStopExecutor-connect-6-3:Worker@550] - Set up the header converter class org.apache.kafka.connect.converters.ByteArrayConverter for task MirrorSourceConnector-0 using the worker config
2023-07-30 13:49:54,753 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:49:54,753 - INFO  [StartAndStopExecutor-connect-6-3:AbstractConfig@361] - SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:49:54,753 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:49:54,753 - INFO  [StartAndStopExecutor-connect-6-3:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:49:54,753 - INFO  [StartAndStopExecutor-connect-6-3:Worker@606] - Initializing: org.apache.kafka.connect.runtime.TransformationChain{}
2023-07-30 13:49:54,753 - INFO  [StartAndStopExecutor-connect-6-3:AbstractConfig@361] - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:45395]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-MirrorSourceConnector-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 2147483647
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:49:54,754 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:49:54,754 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:49:54,754 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:49:54,754 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:49:54,757 - WARN  [StartAndStopExecutor-connect-6-3:AbstractConfig@369] - The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config.
2023-07-30 13:49:54,757 - WARN  [StartAndStopExecutor-connect-6-3:AbstractConfig@369] - The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config.
2023-07-30 13:49:54,758 - INFO  [StartAndStopExecutor-connect-6-3:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:54,758 - INFO  [StartAndStopExecutor-connect-6-3:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:54,758 - INFO  [StartAndStopExecutor-connect-6-3:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739394758
2023-07-30 13:49:54,758 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1243] - [Worker clientId=connect-6, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:49:54,759 - INFO  [kafka-producer-network-thread | connector-producer-MirrorSourceConnector-0:Metadata@279] - [Producer clientId=connector-producer-MirrorSourceConnector-0] Cluster ID: l67hsn_BQke0DtmYXtLNog
2023-07-30 13:49:54,761 - INFO  [task-thread-MirrorSourceConnector-0:AbstractConfig@361] - MirrorTaskConfig values: 
	admin.timeout.ms = 60000
	checkpoints.topic.replication.factor = 1
	config.action.reload = restart
	config.properties.blacklist = [follower\.replication\.throttled\.replicas, leader\.replication\.throttled\.replicas, message\.timestamp\.difference\.max\.ms, message\.timestamp\.type, unclean\.leader\.election\.enable, min\.insync\.replicas]
	config.property.filter.class = class org.apache.kafka.connect.mirror.DefaultConfigPropertyFilter
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	consumer.poll.timeout.ms = 1000
	emit.checkpoints.enabled = true
	emit.checkpoints.interval.seconds = 1
	emit.heartbeats.enabled = true
	emit.heartbeats.interval.seconds = 1
	enabled = true
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	group.filter.class = class org.apache.kafka.connect.mirror.DefaultGroupFilter
	groups = [consumer-group-.*]
	groups.blacklist = [console-consumer-.*, connect-.*, __.*]
	header.converter = null
	heartbeats.topic.replication.factor = 1
	key.converter = null
	metric.reporters = null
	name = MirrorSourceConnector
	offset-syncs.topic.replication.factor = 1
	offset.lag.max = 100
	predicates = []
	refresh.groups.enabled = true
	refresh.groups.interval.seconds = 1
	refresh.topics.enabled = true
	refresh.topics.interval.seconds = 1
	replication.factor = 1
	replication.policy.class = class org.apache.kafka.connect.mirror.DefaultReplicationPolicy
	replication.policy.separator = .
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	source.cluster.alias = primary
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	sync.group.offsets.enabled = false
	sync.group.offsets.interval.seconds = 60
	sync.topic.acls.enabled = false
	sync.topic.acls.interval.seconds = 600
	sync.topic.configs.enabled = true
	sync.topic.configs.interval.seconds = 600
	target.cluster.alias = backup
	task.assigned.groups = null
	task.assigned.partitions = [test-topic-with-empty-partition-0, test-topic-with-empty-partition-1, test-topic-with-empty-partition-2, test-topic-with-empty-partition-3, test-topic-with-empty-partition-4, test-topic-with-empty-partition-5, test-topic-with-empty-partition-6, test-topic-with-empty-partition-7, test-topic-with-empty-partition-8, test-topic-with-empty-partition-9, test-topic-1-0, test-topic-1-1, test-topic-1-2, test-topic-1-3, test-topic-1-4, test-topic-1-5, test-topic-1-6, test-topic-1-7, test-topic-1-8, test-topic-1-9, heartbeats-0]
	tasks.max = 1
	topic.filter.class = class org.apache.kafka.connect.mirror.DefaultTopicFilter
	topics = [test-topic-.*, primary.test-topic-.*, backup.test-topic-.*]
	topics.blacklist = [.*[\-\.]internal, .*\.replica, __.*]
	transforms = []
	value.converter = null

2023-07-30 13:49:54,766 - INFO  [task-thread-MirrorSourceConnector-0:AbstractConfig@361] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:35687]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-null-30
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2023-07-30 13:49:54,767 - INFO  [task-thread-MirrorSourceConnector-0:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:54,767 - INFO  [task-thread-MirrorSourceConnector-0:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:54,767 - INFO  [task-thread-MirrorSourceConnector-0:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739394767
2023-07-30 13:49:54,767 - INFO  [task-thread-MirrorSourceConnector-0:AbstractConfig@361] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:35687]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-22
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2023-07-30 13:49:54,768 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:KafkaConfigBackingStore$ConsumeCallback@578] - Successfully processed removal of connector 'MirrorCheckpointConnector'
2023-07-30 13:49:54,769 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:KafkaConfigBackingStore$ConsumeCallback@578] - Successfully processed removal of connector 'MirrorCheckpointConnector'
2023-07-30 13:49:54,769 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:KafkaConfigBackingStore$ConsumeCallback@578] - Successfully processed removal of connector 'MirrorCheckpointConnector'
2023-07-30 13:49:54,769 - INFO  [task-thread-MirrorSourceConnector-0:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:49:54,770 - INFO  [task-thread-MirrorSourceConnector-0:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:49:54,770 - INFO  [task-thread-MirrorSourceConnector-0:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739394769
2023-07-30 13:49:54,770 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1520] - [Worker clientId=connect-6, groupId=primary-mm2] Connector MirrorCheckpointConnector config removed
2023-07-30 13:49:54,770 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1520] - [Worker clientId=connect-4, groupId=primary-mm2] Connector MirrorCheckpointConnector config removed
2023-07-30 13:49:54,770 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1520] - [Worker clientId=connect-5, groupId=primary-mm2] Connector MirrorCheckpointConnector config removed
2023-07-30 13:49:54,770 - INFO  [DistributedHerder-connect-6-1:WorkerCoordinator@225] - [Worker clientId=connect-6, groupId=primary-mm2] Rebalance started
2023-07-30 13:49:54,770 - INFO  [DistributedHerder-connect-4-1:WorkerCoordinator@225] - [Worker clientId=connect-4, groupId=primary-mm2] Rebalance started
2023-07-30 13:49:54,770 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@592] - [Worker clientId=connect-5, groupId=primary-mm2] Handling connector-only config update by stopping connector MirrorCheckpointConnector
2023-07-30 13:49:54,770 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator@534] - [Worker clientId=connect-6, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:49:54,770 - INFO  [DistributedHerder-connect-5-1:Worker@387] - Stopping connector MirrorCheckpointConnector
2023-07-30 13:49:54,770 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator@534] - [Worker clientId=connect-4, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:49:54,771 - INFO  [DistributedHerder-connect-5-1:WorkerConnector@249] - Scheduled shutdown for WorkerConnector{id=MirrorCheckpointConnector}
2023-07-30 13:49:54,771 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group primary-mm2 in state PreparingRebalance with old generation 8 (__consumer_offsets-17) (reason: Updating metadata for member connect-6-5d9250b5-104e-4755-a5c9-fa2e5283a564 during Stable)
2023-07-30 13:49:54,771 - INFO  [kafka-producer-network-thread | producer-22:Metadata@279] - [Producer clientId=producer-22] Cluster ID: XnnaS7BkQ22Tt1TiwnBd0Q
2023-07-30 13:49:54,773 - INFO  [kafka-admin-client-thread | adminclient-71:AppInfoParser@83] - App info kafka.admin.client for adminclient-71 unregistered
2023-07-30 13:49:54,774 - INFO  [kafka-admin-client-thread | adminclient-71:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:54,775 - INFO  [kafka-admin-client-thread | adminclient-71:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:54,775 - INFO  [kafka-admin-client-thread | adminclient-71:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:54,775 - INFO  [connector-thread-MirrorCheckpointConnector:WorkerConnector@269] - Completed shutdown for WorkerConnector{id=MirrorCheckpointConnector}
2023-07-30 13:49:54,776 - INFO  [DistributedHerder-connect-5-1:WorkerCoordinator@225] - [Worker clientId=connect-5, groupId=primary-mm2] Rebalance started
2023-07-30 13:49:54,777 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator@534] - [Worker clientId=connect-5, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:49:54,778 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupCoordinator 0]: Stabilized group primary-mm2 generation 9 (__consumer_offsets-17)
2023-07-30 13:49:54,778 - INFO  [main:EmbeddedConnectCluster@689] - DELETE response for URL=http://localhost:43163/connectors/MirrorCheckpointConnector is empty
2023-07-30 13:49:54,779 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=9, memberId='connect-6-5d9250b5-104e-4755-a5c9-fa2e5283a564', protocol='sessioned'}
2023-07-30 13:49:54,779 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=9, memberId='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', protocol='sessioned'}
2023-07-30 13:49:54,779 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=9, memberId='connect-4-4677c970-fed2-41c6-8269-6c02eca18a2c', protocol='sessioned'}
2023-07-30 13:49:54,781 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group primary-mm2 for generation 9
2023-07-30 13:49:54,782 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=9, memberId='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', protocol='sessioned'}
2023-07-30 13:49:54,782 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=9, memberId='connect-6-5d9250b5-104e-4755-a5c9-fa2e5283a564', protocol='sessioned'}
2023-07-30 13:49:54,782 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=9, memberId='connect-4-4677c970-fed2-41c6-8269-6c02eca18a2c', protocol='sessioned'}
2023-07-30 13:49:54,783 - INFO  [DistributedHerder-connect-6-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-6, groupId=primary-mm2] Joined group at generation 9 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', leaderUrl='http://localhost:39987/', offset=12, connectorIds=[MirrorHeartbeatConnector], taskIds=[MirrorSourceConnector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:49:54,783 - INFO  [DistributedHerder-connect-4-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-4, groupId=primary-mm2] Joined group at generation 9 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', leaderUrl='http://localhost:39987/', offset=12, connectorIds=[MirrorSourceConnector], taskIds=[MirrorHeartbeatConnector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:49:54,783 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1215] - [Worker clientId=connect-6, groupId=primary-mm2] Starting connectors and tasks using config offset 12
2023-07-30 13:49:54,783 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1243] - [Worker clientId=connect-6, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:49:54,783 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1215] - [Worker clientId=connect-4, groupId=primary-mm2] Starting connectors and tasks using config offset 12
2023-07-30 13:49:54,783 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1243] - [Worker clientId=connect-4, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:49:54,784 - INFO  [StartAndStopExecutor-connect-5-3:Worker@387] - Stopping connector MirrorCheckpointConnector
2023-07-30 13:49:54,784 - INFO  [StartAndStopExecutor-connect-5-4:Worker@836] - Stopping task MirrorCheckpointConnector-0
2023-07-30 13:49:54,784 - WARN  [StartAndStopExecutor-connect-5-3:Worker@390] - Ignoring stop request for unowned connector MirrorCheckpointConnector
2023-07-30 13:49:54,785 - WARN  [StartAndStopExecutor-connect-5-3:Worker@415] - Ignoring await stop request for non-present connector MirrorCheckpointConnector
2023-07-30 13:49:54,893 - INFO  [Scheduler for MirrorSourceConnector-refreshing topics:MirrorSourceConnector@225] - Found 21 topic-partitions on primary. 0 are new. 1 were removed. Previously had 21.
2023-07-30 13:49:54,893 - INFO  [Scheduler for MirrorSourceConnector-refreshing topics:Scheduler@95] - refreshing topics took 4 ms
2023-07-30 13:49:54,893 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:49:54,893 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:49:55,282 - INFO  [task-thread-MirrorCheckpointConnector-0:WorkerSourceTask@478] - WorkerSourceTask{id=MirrorCheckpointConnector-0} Committing offsets
2023-07-30 13:49:55,282 - INFO  [StartAndStopExecutor-connect-5-4:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:55,283 - INFO  [StartAndStopExecutor-connect-5-4:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:55,283 - INFO  [StartAndStopExecutor-connect-5-4:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:55,285 - INFO  [StartAndStopExecutor-connect-5-4:AppInfoParser@83] - App info kafka.consumer for consumer-null-22 unregistered
2023-07-30 13:49:55,286 - INFO  [kafka-admin-client-thread | adminclient-74:AppInfoParser@83] - App info kafka.admin.client for adminclient-74 unregistered
2023-07-30 13:49:55,286 - INFO  [kafka-admin-client-thread | adminclient-74:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:55,286 - INFO  [kafka-admin-client-thread | adminclient-74:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:55,286 - INFO  [kafka-admin-client-thread | adminclient-74:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:55,287 - INFO  [kafka-admin-client-thread | adminclient-75:AppInfoParser@83] - App info kafka.admin.client for adminclient-75 unregistered
2023-07-30 13:49:55,287 - INFO  [kafka-admin-client-thread | adminclient-75:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:55,287 - INFO  [kafka-admin-client-thread | adminclient-75:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:55,287 - INFO  [kafka-admin-client-thread | adminclient-75:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:55,287 - INFO  [StartAndStopExecutor-connect-5-4:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:55,287 - INFO  [StartAndStopExecutor-connect-5-4:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:55,287 - INFO  [StartAndStopExecutor-connect-5-4:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:55,287 - INFO  [StartAndStopExecutor-connect-5-4:MirrorCheckpointTask@121] - Stopping StartAndStopExecutor-connect-5-4 took 503 ms.
2023-07-30 13:49:55,288 - INFO  [task-thread-MirrorCheckpointConnector-0:WorkerSourceTask@495] - WorkerSourceTask{id=MirrorCheckpointConnector-0} flushing 0 outstanding messages for offset commit
2023-07-30 13:49:55,299 - INFO  [task-thread-MirrorCheckpointConnector-0:WorkerSourceTask@574] - WorkerSourceTask{id=MirrorCheckpointConnector-0} Finished commitOffsets successfully in 17 ms
2023-07-30 13:49:55,299 - INFO  [task-thread-MirrorCheckpointConnector-0:KafkaProducer@1193] - [Producer clientId=connector-producer-MirrorCheckpointConnector-0] Closing the Kafka producer with timeoutMillis = 30000 ms.
2023-07-30 13:49:55,301 - INFO  [task-thread-MirrorCheckpointConnector-0:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:55,301 - INFO  [task-thread-MirrorCheckpointConnector-0:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:55,301 - INFO  [task-thread-MirrorCheckpointConnector-0:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:55,301 - INFO  [task-thread-MirrorCheckpointConnector-0:AppInfoParser@83] - App info kafka.producer for connector-producer-MirrorCheckpointConnector-0 unregistered
2023-07-30 13:49:55,305 - INFO  [DistributedHerder-connect-5-1:DistributedHerder$RebalanceListener@1766] - [Worker clientId=connect-5, groupId=primary-mm2] Finished stopping tasks in preparation for rebalance
2023-07-30 13:49:55,309 - INFO  [DistributedHerder-connect-5-1:DistributedHerder$RebalanceListener@1785] - [Worker clientId=connect-5, groupId=primary-mm2] Finished flushing status backing store in preparation for rebalance
2023-07-30 13:49:55,309 - INFO  [DistributedHerder-connect-5-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-5, groupId=primary-mm2] Joined group at generation 9 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', leaderUrl='http://localhost:39987/', offset=12, connectorIds=[], taskIds=[], revokedConnectorIds=[MirrorCheckpointConnector], revokedTaskIds=[MirrorCheckpointConnector-0], delay=0} with rebalance delay: 0
2023-07-30 13:49:55,310 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1215] - [Worker clientId=connect-5, groupId=primary-mm2] Starting connectors and tasks using config offset 12
2023-07-30 13:49:55,310 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1243] - [Worker clientId=connect-5, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:49:55,310 - INFO  [DistributedHerder-connect-5-1:WorkerCoordinator@225] - [Worker clientId=connect-5, groupId=primary-mm2] Rebalance started
2023-07-30 13:49:55,310 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator@534] - [Worker clientId=connect-5, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:49:55,311 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group primary-mm2 in state PreparingRebalance with old generation 9 (__consumer_offsets-17) (reason: leader connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175 re-joining group during Stable)
2023-07-30 13:49:55,893 - INFO  [Scheduler for MirrorSourceConnector-refreshing topics:MirrorSourceConnector@225] - Found 21 topic-partitions on primary. 0 are new. 1 were removed. Previously had 21.
2023-07-30 13:49:55,893 - INFO  [Scheduler for MirrorSourceConnector-refreshing topics:Scheduler@95] - refreshing topics took 4 ms
2023-07-30 13:49:55,893 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:49:55,893 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:49:56,893 - INFO  [Scheduler for MirrorSourceConnector-refreshing topics:MirrorSourceConnector@225] - Found 21 topic-partitions on primary. 0 are new. 1 were removed. Previously had 21.
2023-07-30 13:49:56,893 - INFO  [Scheduler for MirrorSourceConnector-refreshing topics:Scheduler@95] - refreshing topics took 4 ms
2023-07-30 13:49:56,894 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:49:56,894 - INFO  [DistributedHerder-connect-4-1:AbstractConfig@361] - EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = org.apache.kafka.connect.mirror.MirrorSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = MirrorSourceConnector
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null

2023-07-30 13:49:57,780 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$HeartbeatResponseHandler@1097] - [Worker clientId=connect-4, groupId=primary-mm2] Attempt to heartbeat failed since group is rebalancing
2023-07-30 13:49:57,780 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$HeartbeatResponseHandler@1097] - [Worker clientId=connect-6, groupId=primary-mm2] Attempt to heartbeat failed since group is rebalancing
2023-07-30 13:49:57,781 - INFO  [DistributedHerder-connect-4-1:WorkerCoordinator@225] - [Worker clientId=connect-4, groupId=primary-mm2] Rebalance started
2023-07-30 13:49:57,781 - INFO  [DistributedHerder-connect-6-1:WorkerCoordinator@225] - [Worker clientId=connect-6, groupId=primary-mm2] Rebalance started
2023-07-30 13:49:57,781 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator@534] - [Worker clientId=connect-6, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:49:57,781 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator@534] - [Worker clientId=connect-4, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:49:57,783 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [GroupCoordinator 0]: Stabilized group primary-mm2 generation 10 (__consumer_offsets-17)
2023-07-30 13:49:57,783 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=10, memberId='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', protocol='sessioned'}
2023-07-30 13:49:57,783 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=10, memberId='connect-4-4677c970-fed2-41c6-8269-6c02eca18a2c', protocol='sessioned'}
2023-07-30 13:49:57,783 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=10, memberId='connect-6-5d9250b5-104e-4755-a5c9-fa2e5283a564', protocol='sessioned'}
2023-07-30 13:49:57,785 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group primary-mm2 for generation 10
2023-07-30 13:49:57,785 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=10, memberId='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', protocol='sessioned'}
2023-07-30 13:49:57,785 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=10, memberId='connect-6-5d9250b5-104e-4755-a5c9-fa2e5283a564', protocol='sessioned'}
2023-07-30 13:49:57,786 - INFO  [DistributedHerder-connect-5-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-5, groupId=primary-mm2] Joined group at generation 10 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', leaderUrl='http://localhost:39987/', offset=12, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:49:57,786 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=10, memberId='connect-4-4677c970-fed2-41c6-8269-6c02eca18a2c', protocol='sessioned'}
2023-07-30 13:49:57,786 - INFO  [DistributedHerder-connect-6-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-6, groupId=primary-mm2] Joined group at generation 10 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', leaderUrl='http://localhost:39987/', offset=12, connectorIds=[MirrorHeartbeatConnector], taskIds=[MirrorSourceConnector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:49:57,786 - INFO  [DistributedHerder-connect-4-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-4, groupId=primary-mm2] Joined group at generation 10 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', leaderUrl='http://localhost:39987/', offset=12, connectorIds=[MirrorSourceConnector], taskIds=[MirrorHeartbeatConnector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:49:57,786 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1215] - [Worker clientId=connect-5, groupId=primary-mm2] Starting connectors and tasks using config offset 12
2023-07-30 13:49:57,786 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1215] - [Worker clientId=connect-4, groupId=primary-mm2] Starting connectors and tasks using config offset 12
2023-07-30 13:49:57,786 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1215] - [Worker clientId=connect-6, groupId=primary-mm2] Starting connectors and tasks using config offset 12
2023-07-30 13:49:57,786 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1243] - [Worker clientId=connect-4, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:49:57,786 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1243] - [Worker clientId=connect-5, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:49:57,786 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1243] - [Worker clientId=connect-6, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:49:57,788 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:KafkaConfigBackingStore$ConsumeCallback@578] - Successfully processed removal of connector 'MirrorSourceConnector'
2023-07-30 13:49:57,788 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1520] - [Worker clientId=connect-4, groupId=primary-mm2] Connector MirrorSourceConnector config removed
2023-07-30 13:49:57,788 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@592] - [Worker clientId=connect-4, groupId=primary-mm2] Handling connector-only config update by stopping connector MirrorSourceConnector
2023-07-30 13:49:57,788 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:KafkaConfigBackingStore$ConsumeCallback@578] - Successfully processed removal of connector 'MirrorSourceConnector'
2023-07-30 13:49:57,788 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:KafkaConfigBackingStore$ConsumeCallback@578] - Successfully processed removal of connector 'MirrorSourceConnector'
2023-07-30 13:49:57,789 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1520] - [Worker clientId=connect-5, groupId=primary-mm2] Connector MirrorSourceConnector config removed
2023-07-30 13:49:57,789 - INFO  [DistributedHerder-connect-4-1:Worker@387] - Stopping connector MirrorSourceConnector
2023-07-30 13:49:57,789 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1520] - [Worker clientId=connect-6, groupId=primary-mm2] Connector MirrorSourceConnector config removed
2023-07-30 13:49:57,789 - INFO  [DistributedHerder-connect-5-1:WorkerCoordinator@225] - [Worker clientId=connect-5, groupId=primary-mm2] Rebalance started
2023-07-30 13:49:57,789 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator@534] - [Worker clientId=connect-5, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:49:57,789 - INFO  [DistributedHerder-connect-4-1:WorkerConnector@249] - Scheduled shutdown for WorkerConnector{id=MirrorSourceConnector}
2023-07-30 13:49:57,789 - INFO  [DistributedHerder-connect-6-1:WorkerCoordinator@225] - [Worker clientId=connect-6, groupId=primary-mm2] Rebalance started
2023-07-30 13:49:57,789 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator@534] - [Worker clientId=connect-6, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:49:57,789 - INFO  [kafka-admin-client-thread | adminclient-68:AppInfoParser@83] - App info kafka.admin.client for adminclient-68 unregistered
2023-07-30 13:49:57,790 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group primary-mm2 in state PreparingRebalance with old generation 10 (__consumer_offsets-17) (reason: leader connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175 re-joining group during Stable)
2023-07-30 13:49:57,792 - INFO  [kafka-admin-client-thread | adminclient-68:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:57,792 - INFO  [kafka-admin-client-thread | adminclient-68:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:57,792 - INFO  [kafka-admin-client-thread | adminclient-68:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:57,793 - INFO  [kafka-admin-client-thread | adminclient-69:AppInfoParser@83] - App info kafka.admin.client for adminclient-69 unregistered
2023-07-30 13:49:57,793 - INFO  [kafka-admin-client-thread | adminclient-69:Metrics@668] - Metrics scheduler closed
2023-07-30 13:49:57,793 - INFO  [kafka-admin-client-thread | adminclient-69:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:49:57,794 - INFO  [kafka-admin-client-thread | adminclient-69:Metrics@678] - Metrics reporters closed
2023-07-30 13:49:57,794 - INFO  [connector-thread-MirrorSourceConnector:MirrorSourceConnector@141] - Stopping MirrorSourceConnector took 5 ms.
2023-07-30 13:49:57,794 - INFO  [connector-thread-MirrorSourceConnector:WorkerConnector@269] - Completed shutdown for WorkerConnector{id=MirrorSourceConnector}
2023-07-30 13:49:57,795 - INFO  [DistributedHerder-connect-4-1:WorkerCoordinator@225] - [Worker clientId=connect-4, groupId=primary-mm2] Rebalance started
2023-07-30 13:49:57,795 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator@534] - [Worker clientId=connect-4, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:49:57,795 - INFO  [main:EmbeddedConnectCluster@689] - DELETE response for URL=http://localhost:43163/connectors/MirrorSourceConnector is empty
2023-07-30 13:49:57,797 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Stabilized group primary-mm2 generation 11 (__consumer_offsets-17)
2023-07-30 13:49:57,797 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=11, memberId='connect-6-5d9250b5-104e-4755-a5c9-fa2e5283a564', protocol='sessioned'}
2023-07-30 13:49:57,797 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=11, memberId='connect-4-4677c970-fed2-41c6-8269-6c02eca18a2c', protocol='sessioned'}
2023-07-30 13:49:57,797 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=11, memberId='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', protocol='sessioned'}
2023-07-30 13:49:57,799 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group primary-mm2 for generation 11
2023-07-30 13:49:57,800 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=11, memberId='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', protocol='sessioned'}
2023-07-30 13:49:57,800 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=11, memberId='connect-6-5d9250b5-104e-4755-a5c9-fa2e5283a564', protocol='sessioned'}
2023-07-30 13:49:57,800 - INFO  [DistributedHerder-connect-5-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-5, groupId=primary-mm2] Joined group at generation 11 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', leaderUrl='http://localhost:39987/', offset=14, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:49:57,800 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=11, memberId='connect-4-4677c970-fed2-41c6-8269-6c02eca18a2c', protocol='sessioned'}
2023-07-30 13:49:57,800 - INFO  [StartAndStopExecutor-connect-6-4:Worker@836] - Stopping task MirrorSourceConnector-0
2023-07-30 13:49:57,801 - INFO  [StartAndStopExecutor-connect-4-3:Worker@387] - Stopping connector MirrorSourceConnector
2023-07-30 13:49:57,801 - WARN  [StartAndStopExecutor-connect-4-3:Worker@390] - Ignoring stop request for unowned connector MirrorSourceConnector
2023-07-30 13:49:57,801 - WARN  [StartAndStopExecutor-connect-4-3:Worker@415] - Ignoring await stop request for non-present connector MirrorSourceConnector
2023-07-30 13:49:57,801 - INFO  [DistributedHerder-connect-4-1:DistributedHerder$RebalanceListener@1766] - [Worker clientId=connect-4, groupId=primary-mm2] Finished stopping tasks in preparation for rebalance
2023-07-30 13:49:57,801 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1215] - [Worker clientId=connect-5, groupId=primary-mm2] Starting connectors and tasks using config offset 14
2023-07-30 13:49:57,801 - INFO  [DistributedHerder-connect-4-1:DistributedHerder$RebalanceListener@1785] - [Worker clientId=connect-4, groupId=primary-mm2] Finished flushing status backing store in preparation for rebalance
2023-07-30 13:49:57,801 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1243] - [Worker clientId=connect-5, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:49:57,801 - INFO  [DistributedHerder-connect-4-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-4, groupId=primary-mm2] Joined group at generation 11 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', leaderUrl='http://localhost:39987/', offset=14, connectorIds=[], taskIds=[MirrorHeartbeatConnector-0], revokedConnectorIds=[MirrorSourceConnector], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:49:57,802 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1215] - [Worker clientId=connect-4, groupId=primary-mm2] Starting connectors and tasks using config offset 14
2023-07-30 13:49:57,802 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1243] - [Worker clientId=connect-4, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:49:57,802 - INFO  [DistributedHerder-connect-4-1:WorkerCoordinator@225] - [Worker clientId=connect-4, groupId=primary-mm2] Rebalance started
2023-07-30 13:49:57,802 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator@534] - [Worker clientId=connect-4, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:49:57,802 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group primary-mm2 in state PreparingRebalance with old generation 11 (__consumer_offsets-17) (reason: Updating metadata for member connect-4-4677c970-fed2-41c6-8269-6c02eca18a2c during Stable)
2023-07-30 13:50:00,798 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$HeartbeatResponseHandler@1097] - [Worker clientId=connect-5, groupId=primary-mm2] Attempt to heartbeat failed since group is rebalancing
2023-07-30 13:50:00,798 - INFO  [DistributedHerder-connect-5-1:WorkerCoordinator@225] - [Worker clientId=connect-5, groupId=primary-mm2] Rebalance started
2023-07-30 13:50:00,798 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator@534] - [Worker clientId=connect-5, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:50:00,903 - INFO  [kafka-coordinator-heartbeat-thread | primary-mm2:AbstractCoordinator$HeartbeatResponseHandler@1097] - [Worker clientId=connect-6, groupId=primary-mm2] Attempt to heartbeat failed since group is rebalancing
2023-07-30 13:50:02,801 - ERROR [StartAndStopExecutor-connect-6-4:Worker@867] - Graceful stop of task MirrorSourceConnector-0 failed.
2023-07-30 13:50:02,801 - ERROR [task-thread-MirrorSourceConnector-0:OffsetStorageReaderImpl@113] - Failed to fetch offsets from namespace MirrorSourceConnector: 
org.apache.kafka.connect.errors.ConnectException: Offset reader closed while attempting to read offsets. This is likely because the task was been scheduled to stop but has taken longer than the graceful shutdown period to do so.
	at org.apache.kafka.connect.storage.OffsetStorageReaderImpl.offsets(OffsetStorageReaderImpl.java:103)
	at org.apache.kafka.connect.storage.OffsetStorageReaderImpl.offset(OffsetStorageReaderImpl.java:63)
	at org.apache.kafka.connect.mirror.MirrorSourceTask.loadOffset(MirrorSourceTask.java:227)
	at org.apache.kafka.connect.mirror.MirrorSourceTask.lambda$loadOffsets$4(MirrorSourceTask.java:222)
	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321)
	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169)
	at java.util.HashMap$KeySpliterator.forEachRemaining(HashMap.java:1556)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566)
	at org.apache.kafka.connect.mirror.MirrorSourceTask.loadOffsets(MirrorSourceTask.java:222)
	at org.apache.kafka.connect.mirror.MirrorSourceTask.start(MirrorSourceTask.java:92)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.execute(WorkerSourceTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:235)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2023-07-30 13:50:02,803 - INFO  [DistributedHerder-connect-6-1:DistributedHerder$RebalanceListener@1766] - [Worker clientId=connect-6, groupId=primary-mm2] Finished stopping tasks in preparation for rebalance
2023-07-30 13:50:02,803 - INFO  [task-thread-MirrorSourceConnector-0:WorkerSourceTask@478] - WorkerSourceTask{id=MirrorSourceConnector-0} Committing offsets
2023-07-30 13:50:02,803 - INFO  [task-thread-MirrorSourceConnector-0:WorkerSourceTask@495] - WorkerSourceTask{id=MirrorSourceConnector-0} flushing 0 outstanding messages for offset commit
2023-07-30 13:50:02,803 - INFO  [DistributedHerder-connect-6-1:DistributedHerder$RebalanceListener@1785] - [Worker clientId=connect-6, groupId=primary-mm2] Finished flushing status backing store in preparation for rebalance
2023-07-30 13:50:02,803 - ERROR [task-thread-MirrorSourceConnector-0:WorkerTask@187] - WorkerSourceTask{id=MirrorSourceConnector-0} Task threw an uncaught and unrecoverable exception
org.apache.kafka.connect.errors.ConnectException: Failed to fetch offsets.
	at org.apache.kafka.connect.storage.OffsetStorageReaderImpl.offsets(OffsetStorageReaderImpl.java:114)
	at org.apache.kafka.connect.storage.OffsetStorageReaderImpl.offset(OffsetStorageReaderImpl.java:63)
	at org.apache.kafka.connect.mirror.MirrorSourceTask.loadOffset(MirrorSourceTask.java:227)
	at org.apache.kafka.connect.mirror.MirrorSourceTask.lambda$loadOffsets$4(MirrorSourceTask.java:222)
	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321)
	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169)
	at java.util.HashMap$KeySpliterator.forEachRemaining(HashMap.java:1556)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566)
	at org.apache.kafka.connect.mirror.MirrorSourceTask.loadOffsets(MirrorSourceTask.java:222)
	at org.apache.kafka.connect.mirror.MirrorSourceTask.start(MirrorSourceTask.java:92)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.execute(WorkerSourceTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:235)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.connect.errors.ConnectException: Offset reader closed while attempting to read offsets. This is likely because the task was been scheduled to stop but has taken longer than the graceful shutdown period to do so.
	at org.apache.kafka.connect.storage.OffsetStorageReaderImpl.offsets(OffsetStorageReaderImpl.java:103)
	... 21 more
2023-07-30 13:50:02,803 - INFO  [DistributedHerder-connect-6-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-6, groupId=primary-mm2] Joined group at generation 11 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', leaderUrl='http://localhost:39987/', offset=14, connectorIds=[MirrorHeartbeatConnector], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[MirrorSourceConnector-0], delay=0} with rebalance delay: 0
2023-07-30 13:50:02,803 - ERROR [task-thread-MirrorSourceConnector-0:WorkerTask@188] - WorkerSourceTask{id=MirrorSourceConnector-0} Task is being killed and will not recover until manually restarted
2023-07-30 13:50:02,803 - INFO  [DistributedHerder-connect-6-1:WorkerCoordinator@225] - [Worker clientId=connect-6, groupId=primary-mm2] Rebalance started
2023-07-30 13:50:02,803 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator@534] - [Worker clientId=connect-6, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:50:02,803 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@668] - Metrics scheduler closed
2023-07-30 13:50:02,803 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:50:02,803 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@678] - Metrics reporters closed
2023-07-30 13:50:02,804 - INFO  [task-thread-MirrorSourceConnector-0:AppInfoParser@83] - App info kafka.consumer for consumer-null-30 unregistered
2023-07-30 13:50:02,804 - INFO  [task-thread-MirrorSourceConnector-0:KafkaProducer@1193] - [Producer clientId=producer-22] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:50:02,804 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Stabilized group primary-mm2 generation 12 (__consumer_offsets-17)
2023-07-30 13:50:02,805 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=12, memberId='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', protocol='sessioned'}
2023-07-30 13:50:02,805 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=12, memberId='connect-4-4677c970-fed2-41c6-8269-6c02eca18a2c', protocol='sessioned'}
2023-07-30 13:50:02,805 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=12, memberId='connect-6-5d9250b5-104e-4755-a5c9-fa2e5283a564', protocol='sessioned'}
2023-07-30 13:50:02,805 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@668] - Metrics scheduler closed
2023-07-30 13:50:02,805 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:50:02,805 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@678] - Metrics reporters closed
2023-07-30 13:50:02,805 - INFO  [task-thread-MirrorSourceConnector-0:AppInfoParser@83] - App info kafka.producer for producer-22 unregistered
2023-07-30 13:50:02,805 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@668] - Metrics scheduler closed
2023-07-30 13:50:02,805 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:50:02,805 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@678] - Metrics reporters closed
2023-07-30 13:50:02,805 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group primary-mm2 for generation 12
2023-07-30 13:50:02,805 - INFO  [task-thread-MirrorSourceConnector-0:MirrorSourceTask@120] - Stopping task-thread-MirrorSourceConnector-0 took 2 ms.
2023-07-30 13:50:02,806 - INFO  [task-thread-MirrorSourceConnector-0:KafkaProducer@1193] - [Producer clientId=connector-producer-MirrorSourceConnector-0] Closing the Kafka producer with timeoutMillis = 30000 ms.
2023-07-30 13:50:02,806 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@668] - Metrics scheduler closed
2023-07-30 13:50:02,807 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:50:02,807 - INFO  [task-thread-MirrorSourceConnector-0:Metrics@678] - Metrics reporters closed
2023-07-30 13:50:02,807 - INFO  [task-thread-MirrorSourceConnector-0:AppInfoParser@83] - App info kafka.producer for connector-producer-MirrorSourceConnector-0 unregistered
2023-07-30 13:50:02,807 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=12, memberId='connect-6-5d9250b5-104e-4755-a5c9-fa2e5283a564', protocol='sessioned'}
2023-07-30 13:50:02,807 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=12, memberId='connect-4-4677c970-fed2-41c6-8269-6c02eca18a2c', protocol='sessioned'}
2023-07-30 13:50:02,807 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=12, memberId='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', protocol='sessioned'}
2023-07-30 13:50:02,807 - INFO  [DistributedHerder-connect-4-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-4, groupId=primary-mm2] Joined group at generation 12 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', leaderUrl='http://localhost:39987/', offset=14, connectorIds=[], taskIds=[MirrorHeartbeatConnector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:50:02,807 - INFO  [DistributedHerder-connect-6-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-6, groupId=primary-mm2] Joined group at generation 12 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', leaderUrl='http://localhost:39987/', offset=14, connectorIds=[MirrorHeartbeatConnector], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:50:02,807 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1215] - [Worker clientId=connect-4, groupId=primary-mm2] Starting connectors and tasks using config offset 14
2023-07-30 13:50:02,807 - INFO  [DistributedHerder-connect-5-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-5, groupId=primary-mm2] Joined group at generation 12 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', leaderUrl='http://localhost:39987/', offset=14, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:50:02,808 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1243] - [Worker clientId=connect-4, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:50:02,808 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1215] - [Worker clientId=connect-6, groupId=primary-mm2] Starting connectors and tasks using config offset 14
2023-07-30 13:50:02,808 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1215] - [Worker clientId=connect-5, groupId=primary-mm2] Starting connectors and tasks using config offset 14
2023-07-30 13:50:02,808 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1243] - [Worker clientId=connect-6, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:50:02,808 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1243] - [Worker clientId=connect-5, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:50:02,819 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:KafkaConfigBackingStore$ConsumeCallback@578] - Successfully processed removal of connector 'MirrorHeartbeatConnector'
2023-07-30 13:50:02,819 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:KafkaConfigBackingStore$ConsumeCallback@578] - Successfully processed removal of connector 'MirrorHeartbeatConnector'
2023-07-30 13:50:02,819 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1520] - [Worker clientId=connect-6, groupId=primary-mm2] Connector MirrorHeartbeatConnector config removed
2023-07-30 13:50:02,819 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1520] - [Worker clientId=connect-4, groupId=primary-mm2] Connector MirrorHeartbeatConnector config removed
2023-07-30 13:50:02,819 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:KafkaConfigBackingStore$ConsumeCallback@578] - Successfully processed removal of connector 'MirrorHeartbeatConnector'
2023-07-30 13:50:02,820 - INFO  [DistributedHerder-connect-4-1:WorkerCoordinator@225] - [Worker clientId=connect-4, groupId=primary-mm2] Rebalance started
2023-07-30 13:50:02,819 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@592] - [Worker clientId=connect-6, groupId=primary-mm2] Handling connector-only config update by stopping connector MirrorHeartbeatConnector
2023-07-30 13:50:02,820 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator@534] - [Worker clientId=connect-4, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:50:02,820 - INFO  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:DistributedHerder$ConfigUpdateListener@1520] - [Worker clientId=connect-5, groupId=primary-mm2] Connector MirrorHeartbeatConnector config removed
2023-07-30 13:50:02,820 - INFO  [DistributedHerder-connect-6-1:Worker@387] - Stopping connector MirrorHeartbeatConnector
2023-07-30 13:50:02,820 - INFO  [DistributedHerder-connect-5-1:WorkerCoordinator@225] - [Worker clientId=connect-5, groupId=primary-mm2] Rebalance started
2023-07-30 13:50:02,820 - INFO  [DistributedHerder-connect-6-1:WorkerConnector@249] - Scheduled shutdown for WorkerConnector{id=MirrorHeartbeatConnector}
2023-07-30 13:50:02,820 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator@534] - [Worker clientId=connect-5, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:50:02,820 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group primary-mm2 in state PreparingRebalance with old generation 12 (__consumer_offsets-17) (reason: Updating metadata for member connect-4-4677c970-fed2-41c6-8269-6c02eca18a2c during Stable)
2023-07-30 13:50:02,820 - INFO  [connector-thread-MirrorHeartbeatConnector:WorkerConnector@269] - Completed shutdown for WorkerConnector{id=MirrorHeartbeatConnector}
2023-07-30 13:50:02,822 - INFO  [DistributedHerder-connect-6-1:WorkerCoordinator@225] - [Worker clientId=connect-6, groupId=primary-mm2] Rebalance started
2023-07-30 13:50:02,822 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator@534] - [Worker clientId=connect-6, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:50:02,825 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupCoordinator 0]: Stabilized group primary-mm2 generation 13 (__consumer_offsets-17)
2023-07-30 13:50:02,825 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=13, memberId='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', protocol='sessioned'}
2023-07-30 13:50:02,825 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=13, memberId='connect-4-4677c970-fed2-41c6-8269-6c02eca18a2c', protocol='sessioned'}
2023-07-30 13:50:02,825 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=13, memberId='connect-6-5d9250b5-104e-4755-a5c9-fa2e5283a564', protocol='sessioned'}
2023-07-30 13:50:02,826 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group primary-mm2 for generation 13
2023-07-30 13:50:02,827 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=13, memberId='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', protocol='sessioned'}
2023-07-30 13:50:02,827 - INFO  [DistributedHerder-connect-5-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-5, groupId=primary-mm2] Joined group at generation 13 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', leaderUrl='http://localhost:39987/', offset=16, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:50:02,827 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=13, memberId='connect-6-5d9250b5-104e-4755-a5c9-fa2e5283a564', protocol='sessioned'}
2023-07-30 13:50:02,828 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=13, memberId='connect-4-4677c970-fed2-41c6-8269-6c02eca18a2c', protocol='sessioned'}
2023-07-30 13:50:02,828 - INFO  [main:EmbeddedConnectCluster@689] - DELETE response for URL=http://localhost:43163/connectors/MirrorHeartbeatConnector is empty
2023-07-30 13:50:02,828 - INFO  [StartAndStopExecutor-connect-6-5:Worker@387] - Stopping connector MirrorHeartbeatConnector
2023-07-30 13:50:02,828 - WARN  [StartAndStopExecutor-connect-6-5:Worker@390] - Ignoring stop request for unowned connector MirrorHeartbeatConnector
2023-07-30 13:50:02,828 - INFO  [StartAndStopExecutor-connect-4-4:Worker@836] - Stopping task MirrorHeartbeatConnector-0
2023-07-30 13:50:02,828 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1215] - [Worker clientId=connect-5, groupId=primary-mm2] Starting connectors and tasks using config offset 16
2023-07-30 13:50:02,828 - INFO  [task-thread-MirrorHeartbeatConnector-0:WorkerSourceTask@478] - WorkerSourceTask{id=MirrorHeartbeatConnector-0} Committing offsets
2023-07-30 13:50:02,828 - WARN  [StartAndStopExecutor-connect-6-5:Worker@415] - Ignoring await stop request for non-present connector MirrorHeartbeatConnector
2023-07-30 13:50:02,828 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:35687]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:50:02,828 - INFO  [task-thread-MirrorHeartbeatConnector-0:WorkerSourceTask@495] - WorkerSourceTask{id=MirrorHeartbeatConnector-0} flushing 0 outstanding messages for offset commit
2023-07-30 13:50:02,828 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1243] - [Worker clientId=connect-5, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:50:02,828 - INFO  [DistributedHerder-connect-6-1:DistributedHerder$RebalanceListener@1766] - [Worker clientId=connect-6, groupId=primary-mm2] Finished stopping tasks in preparation for rebalance
2023-07-30 13:50:02,830 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:50:02,830 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:50:02,830 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739402830
2023-07-30 13:50:02,830 - INFO  [DistributedHerder-connect-6-1:DistributedHerder$RebalanceListener@1785] - [Worker clientId=connect-6, groupId=primary-mm2] Finished flushing status backing store in preparation for rebalance
2023-07-30 13:50:02,830 - INFO  [DistributedHerder-connect-6-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-6, groupId=primary-mm2] Joined group at generation 13 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', leaderUrl='http://localhost:39987/', offset=16, connectorIds=[], taskIds=[], revokedConnectorIds=[MirrorHeartbeatConnector], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:50:02,830 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1215] - [Worker clientId=connect-6, groupId=primary-mm2] Starting connectors and tasks using config offset 16
2023-07-30 13:50:02,830 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1243] - [Worker clientId=connect-6, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:50:02,831 - INFO  [DistributedHerder-connect-6-1:WorkerCoordinator@225] - [Worker clientId=connect-6, groupId=primary-mm2] Rebalance started
2023-07-30 13:50:02,831 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator@534] - [Worker clientId=connect-6, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:50:02,831 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group primary-mm2 in state PreparingRebalance with old generation 13 (__consumer_offsets-17) (reason: Updating metadata for member connect-6-5d9250b5-104e-4755-a5c9-fa2e5283a564 during Stable)
2023-07-30 13:50:02,835 - INFO  [task-thread-MirrorHeartbeatConnector-0:WorkerSourceTask@574] - WorkerSourceTask{id=MirrorHeartbeatConnector-0} Finished commitOffsets successfully in 7 ms
2023-07-30 13:50:02,835 - INFO  [task-thread-MirrorHeartbeatConnector-0:KafkaProducer@1193] - [Producer clientId=connector-producer-MirrorHeartbeatConnector-0] Closing the Kafka producer with timeoutMillis = 30000 ms.
2023-07-30 13:50:02,836 - INFO  [task-thread-MirrorHeartbeatConnector-0:Metrics@668] - Metrics scheduler closed
2023-07-30 13:50:02,836 - INFO  [task-thread-MirrorHeartbeatConnector-0:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:50:02,836 - INFO  [task-thread-MirrorHeartbeatConnector-0:Metrics@678] - Metrics reporters closed
2023-07-30 13:50:02,836 - INFO  [task-thread-MirrorHeartbeatConnector-0:AppInfoParser@83] - App info kafka.producer for connector-producer-MirrorHeartbeatConnector-0 unregistered
2023-07-30 13:50:02,839 - INFO  [DistributedHerder-connect-4-1:DistributedHerder$RebalanceListener@1766] - [Worker clientId=connect-4, groupId=primary-mm2] Finished stopping tasks in preparation for rebalance
2023-07-30 13:50:02,839 - INFO  [main:AbstractConfig@361] - AdminClientConfig values: 
	bootstrap.servers = [localhost:45395]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-07-30 13:50:02,840 - INFO  [DistributedHerder-connect-4-1:DistributedHerder$RebalanceListener@1785] - [Worker clientId=connect-4, groupId=primary-mm2] Finished flushing status backing store in preparation for rebalance
2023-07-30 13:50:02,840 - INFO  [DistributedHerder-connect-4-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-4, groupId=primary-mm2] Joined group at generation 13 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', leaderUrl='http://localhost:39987/', offset=16, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[MirrorHeartbeatConnector-0], delay=0} with rebalance delay: 0
2023-07-30 13:50:02,840 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1215] - [Worker clientId=connect-4, groupId=primary-mm2] Starting connectors and tasks using config offset 16
2023-07-30 13:50:02,840 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1243] - [Worker clientId=connect-4, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:50:02,840 - INFO  [DistributedHerder-connect-4-1:WorkerCoordinator@225] - [Worker clientId=connect-4, groupId=primary-mm2] Rebalance started
2023-07-30 13:50:02,840 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator@534] - [Worker clientId=connect-4, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:50:02,840 - INFO  [main:AppInfoParser$AppInfo@119] - Kafka version: unknown
2023-07-30 13:50:02,840 - INFO  [main:AppInfoParser$AppInfo@120] - Kafka commitId: unknown
2023-07-30 13:50:02,840 - INFO  [main:AppInfoParser$AppInfo@121] - Kafka startTimeMs: 1690739402840
2023-07-30 13:50:02,844 - INFO  [main:EmbeddedConnectCluster@205] - Stopping worker WorkerHandle{workerName='connect-worker-0'workerURL='http://localhost:43083/'}
2023-07-30 13:50:02,844 - INFO  [main:Connect@67] - Kafka Connect stopping
2023-07-30 13:50:02,844 - INFO  [main:RestServer@327] - Stopping REST server
2023-07-30 13:50:02,847 - INFO  [main:AbstractConnector@381] - Stopped http_localhost0@3113a37{HTTP/1.1, (http/1.1)}{localhost:0}
2023-07-30 13:50:02,847 - INFO  [main:HouseKeeper@158] - node0 Stopped scavenging
2023-07-30 13:50:02,848 - INFO  [main:RestServer@344] - REST server stopped
2023-07-30 13:50:02,848 - INFO  [main:DistributedHerder@681] - [Worker clientId=connect-1, groupId=backup-mm2] Herder stopping
2023-07-30 13:50:02,849 - INFO  [DistributedHerder-connect-1-1:DistributedHerder@655] - [Worker clientId=connect-1, groupId=backup-mm2] Stopping connectors and tasks that are still assigned to this worker.
2023-07-30 13:50:02,849 - INFO  [DistributedHerder-connect-1-1:AbstractCoordinator@1016] - [Worker clientId=connect-1, groupId=backup-mm2] Member connect-1-e0f7ca44-8c98-43fc-a034-dae0d9034b88 sending LeaveGroup request to coordinator localhost:35687 (id: 2147483647 rack: null) due to the consumer is being closed
2023-07-30 13:50:02,849 - WARN  [DistributedHerder-connect-1-1:AbstractCoordinator@997] - [Worker clientId=connect-1, groupId=backup-mm2] Close timed out with 1 pending requests to coordinator, terminating client connections
2023-07-30 13:50:02,849 - INFO  [DistributedHerder-connect-1-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:50:02,849 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Member[group.instance.id None, member.id connect-1-e0f7ca44-8c98-43fc-a034-dae0d9034b88] in group backup-mm2 has left, removing it from the group
2023-07-30 13:50:02,849 - INFO  [DistributedHerder-connect-1-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:50:02,849 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group backup-mm2 in state PreparingRebalance with old generation 2 (__consumer_offsets-5) (reason: removing member connect-1-e0f7ca44-8c98-43fc-a034-dae0d9034b88 on LeaveGroup)
2023-07-30 13:50:02,849 - INFO  [DistributedHerder-connect-1-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:50:02,850 - INFO  [DistributedHerder-connect-1-1:AppInfoParser@83] - App info kafka.connect for connect-1 unregistered
2023-07-30 13:50:02,850 - INFO  [DistributedHerder-connect-1-1:KafkaBasedLog@167] - Stopping KafkaBasedLog for topic mm2-status.backup.internal
2023-07-30 13:50:02,850 - INFO  [DistributedHerder-connect-1-1:KafkaProducer@1193] - [Producer clientId=producer-3] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:50:02,851 - INFO  [DistributedHerder-connect-1-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:50:02,851 - INFO  [DistributedHerder-connect-1-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:50:02,851 - INFO  [DistributedHerder-connect-1-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:50:02,851 - INFO  [DistributedHerder-connect-1-1:AppInfoParser@83] - App info kafka.producer for producer-3 unregistered
2023-07-30 13:50:02,852 - INFO  [DistributedHerder-connect-1-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:50:02,852 - INFO  [DistributedHerder-connect-1-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:50:02,852 - INFO  [DistributedHerder-connect-1-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:50:02,853 - INFO  [DistributedHerder-connect-1-1:AppInfoParser@83] - App info kafka.consumer for consumer-backup-mm2-2 unregistered
2023-07-30 13:50:02,853 - INFO  [DistributedHerder-connect-1-1:KafkaBasedLog@193] - Stopped KafkaBasedLog for topic mm2-status.backup.internal
2023-07-30 13:50:02,853 - INFO  [DistributedHerder-connect-1-1:KafkaConfigBackingStore@285] - Closing KafkaConfigBackingStore
2023-07-30 13:50:02,853 - INFO  [DistributedHerder-connect-1-1:KafkaBasedLog@167] - Stopping KafkaBasedLog for topic mm2-configs.backup.internal
2023-07-30 13:50:02,853 - INFO  [DistributedHerder-connect-1-1:KafkaProducer@1193] - [Producer clientId=producer-4] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:50:02,854 - INFO  [DistributedHerder-connect-1-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:50:02,854 - INFO  [DistributedHerder-connect-1-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:50:02,854 - INFO  [DistributedHerder-connect-1-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:50:02,854 - INFO  [DistributedHerder-connect-1-1:AppInfoParser@83] - App info kafka.producer for producer-4 unregistered
2023-07-30 13:50:02,855 - INFO  [DistributedHerder-connect-1-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:50:02,855 - INFO  [DistributedHerder-connect-1-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:50:02,856 - INFO  [DistributedHerder-connect-1-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:50:02,856 - INFO  [DistributedHerder-connect-1-1:AppInfoParser@83] - App info kafka.consumer for consumer-backup-mm2-3 unregistered
2023-07-30 13:50:02,856 - INFO  [DistributedHerder-connect-1-1:KafkaBasedLog@193] - Stopped KafkaBasedLog for topic mm2-configs.backup.internal
2023-07-30 13:50:02,856 - INFO  [DistributedHerder-connect-1-1:KafkaConfigBackingStore@287] - Closed KafkaConfigBackingStore
2023-07-30 13:50:02,856 - INFO  [DistributedHerder-connect-1-1:Worker@209] - Worker stopping
2023-07-30 13:50:02,857 - INFO  [DistributedHerder-connect-1-1:KafkaOffsetBackingStore@134] - Stopping KafkaOffsetBackingStore
2023-07-30 13:50:02,857 - INFO  [DistributedHerder-connect-1-1:KafkaBasedLog@167] - Stopping KafkaBasedLog for topic mm2-offsets.backup.internal
2023-07-30 13:50:02,857 - INFO  [DistributedHerder-connect-1-1:KafkaProducer@1193] - [Producer clientId=producer-2] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:50:02,857 - INFO  [DistributedHerder-connect-1-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:50:02,857 - INFO  [DistributedHerder-connect-1-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:50:02,858 - INFO  [DistributedHerder-connect-1-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:50:02,858 - INFO  [DistributedHerder-connect-1-1:AppInfoParser@83] - App info kafka.producer for producer-2 unregistered
2023-07-30 13:50:02,859 - INFO  [DistributedHerder-connect-1-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:50:02,859 - INFO  [DistributedHerder-connect-1-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:50:02,859 - INFO  [DistributedHerder-connect-1-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:50:02,859 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [GroupCoordinator 0]: Removed 0 offsets associated with deleted partitions: mm2-configs.backup.internal-0.
2023-07-30 13:50:02,859 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [GroupCoordinator 0]: Removed 0 offsets associated with deleted partitions: mm2-status.primary.internal-1, mm2-status.primary.internal-0, mm2-status.primary.internal-3, mm2-status.primary.internal-2, mm2-status.primary.internal-4.
2023-07-30 13:50:02,860 - INFO  [DistributedHerder-connect-1-1:AppInfoParser@83] - App info kafka.consumer for consumer-backup-mm2-1 unregistered
2023-07-30 13:50:02,860 - INFO  [DistributedHerder-connect-1-1:KafkaBasedLog@193] - Stopped KafkaBasedLog for topic mm2-offsets.backup.internal
2023-07-30 13:50:02,860 - INFO  [DistributedHerder-connect-1-1:KafkaOffsetBackingStore@136] - Stopped KafkaOffsetBackingStore
2023-07-30 13:50:02,860 - INFO  [DistributedHerder-connect-1-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:50:02,860 - INFO  [DistributedHerder-connect-1-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:50:02,860 - INFO  [DistributedHerder-connect-1-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:50:02,860 - INFO  [DistributedHerder-connect-1-1:AppInfoParser@83] - App info kafka.connect for localhost:43083 unregistered
2023-07-30 13:50:02,860 - INFO  [DistributedHerder-connect-1-1:Worker@230] - Worker stopped
2023-07-30 13:50:02,860 - INFO  [DistributedHerder-connect-1-1:DistributedHerder@299] - [Worker clientId=connect-1, groupId=backup-mm2] Herder stopped
2023-07-30 13:50:02,861 - INFO  [main:DistributedHerder@701] - [Worker clientId=connect-1, groupId=backup-mm2] Herder stopped
2023-07-30 13:50:02,861 - INFO  [main:Connect@72] - Kafka Connect stopped
2023-07-30 13:50:02,861 - INFO  [main:EmbeddedConnectCluster@205] - Stopping worker WorkerHandle{workerName='connect-worker-1'workerURL='http://localhost:33591/'}
2023-07-30 13:50:02,861 - INFO  [main:Connect@67] - Kafka Connect stopping
2023-07-30 13:50:02,861 - INFO  [main:RestServer@327] - Stopping REST server
2023-07-30 13:50:02,863 - INFO  [main:AbstractConnector@381] - Stopped http_localhost0@181e72d3{HTTP/1.1, (http/1.1)}{localhost:0}
2023-07-30 13:50:02,863 - INFO  [main:HouseKeeper@158] - node0 Stopped scavenging
2023-07-30 13:50:02,864 - INFO  [main:RestServer@344] - REST server stopped
2023-07-30 13:50:02,864 - INFO  [main:DistributedHerder@681] - [Worker clientId=connect-2, groupId=backup-mm2] Herder stopping
2023-07-30 13:50:02,864 - INFO  [DistributedHerder-connect-2-1:DistributedHerder@655] - [Worker clientId=connect-2, groupId=backup-mm2] Stopping connectors and tasks that are still assigned to this worker.
2023-07-30 13:50:02,864 - INFO  [DistributedHerder-connect-2-1:AbstractCoordinator@1016] - [Worker clientId=connect-2, groupId=backup-mm2] Member connect-2-12409f17-1c6b-4299-8d44-e2b23de6867f sending LeaveGroup request to coordinator localhost:35687 (id: 2147483647 rack: null) due to the consumer is being closed
2023-07-30 13:50:02,864 - WARN  [DistributedHerder-connect-2-1:AbstractCoordinator@997] - [Worker clientId=connect-2, groupId=backup-mm2] Close timed out with 1 pending requests to coordinator, terminating client connections
2023-07-30 13:50:02,864 - INFO  [DistributedHerder-connect-2-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:50:02,865 - INFO  [DistributedHerder-connect-2-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:50:02,865 - INFO  [DistributedHerder-connect-2-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:50:02,865 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [GroupCoordinator 0]: Member[group.instance.id None, member.id connect-2-12409f17-1c6b-4299-8d44-e2b23de6867f] in group backup-mm2 has left, removing it from the group
2023-07-30 13:50:02,865 - INFO  [DistributedHerder-connect-2-1:AppInfoParser@83] - App info kafka.connect for connect-2 unregistered
2023-07-30 13:50:02,865 - INFO  [DistributedHerder-connect-2-1:KafkaBasedLog@167] - Stopping KafkaBasedLog for topic mm2-status.backup.internal
2023-07-30 13:50:02,865 - INFO  [DistributedHerder-connect-2-1:KafkaProducer@1193] - [Producer clientId=producer-6] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:50:02,866 - INFO  [DistributedHerder-connect-2-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:50:02,866 - INFO  [DistributedHerder-connect-2-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:50:02,866 - INFO  [DistributedHerder-connect-2-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:50:02,866 - INFO  [DistributedHerder-connect-2-1:AppInfoParser@83] - App info kafka.producer for producer-6 unregistered
2023-07-30 13:50:02,867 - INFO  [DistributedHerder-connect-2-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:50:02,868 - INFO  [DistributedHerder-connect-2-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:50:02,869 - INFO  [DistributedHerder-connect-2-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:50:02,869 - INFO  [DistributedHerder-connect-2-1:AppInfoParser@83] - App info kafka.consumer for consumer-backup-mm2-5 unregistered
2023-07-30 13:50:02,869 - INFO  [DistributedHerder-connect-2-1:KafkaBasedLog@193] - Stopped KafkaBasedLog for topic mm2-status.backup.internal
2023-07-30 13:50:02,869 - INFO  [DistributedHerder-connect-2-1:KafkaConfigBackingStore@285] - Closing KafkaConfigBackingStore
2023-07-30 13:50:02,869 - INFO  [DistributedHerder-connect-2-1:KafkaBasedLog@167] - Stopping KafkaBasedLog for topic mm2-configs.backup.internal
2023-07-30 13:50:02,869 - INFO  [DistributedHerder-connect-2-1:KafkaProducer@1193] - [Producer clientId=producer-7] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:50:02,870 - INFO  [DistributedHerder-connect-2-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:50:02,870 - INFO  [DistributedHerder-connect-2-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:50:02,870 - INFO  [DistributedHerder-connect-2-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:50:02,870 - INFO  [DistributedHerder-connect-2-1:AppInfoParser@83] - App info kafka.producer for producer-7 unregistered
2023-07-30 13:50:02,871 - INFO  [DistributedHerder-connect-2-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:50:02,871 - INFO  [DistributedHerder-connect-2-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:50:02,871 - INFO  [DistributedHerder-connect-2-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:50:02,871 - INFO  [DistributedHerder-connect-2-1:AppInfoParser@83] - App info kafka.consumer for consumer-backup-mm2-6 unregistered
2023-07-30 13:50:02,871 - INFO  [DistributedHerder-connect-2-1:KafkaBasedLog@193] - Stopped KafkaBasedLog for topic mm2-configs.backup.internal
2023-07-30 13:50:02,872 - INFO  [DistributedHerder-connect-2-1:KafkaConfigBackingStore@287] - Closed KafkaConfigBackingStore
2023-07-30 13:50:02,872 - INFO  [DistributedHerder-connect-2-1:Worker@209] - Worker stopping
2023-07-30 13:50:02,872 - INFO  [DistributedHerder-connect-2-1:KafkaOffsetBackingStore@134] - Stopping KafkaOffsetBackingStore
2023-07-30 13:50:02,872 - INFO  [DistributedHerder-connect-2-1:KafkaBasedLog@167] - Stopping KafkaBasedLog for topic mm2-offsets.backup.internal
2023-07-30 13:50:02,873 - INFO  [DistributedHerder-connect-2-1:KafkaProducer@1193] - [Producer clientId=producer-5] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:50:02,873 - INFO  [DistributedHerder-connect-2-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:50:02,873 - INFO  [DistributedHerder-connect-2-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:50:02,873 - INFO  [DistributedHerder-connect-2-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:50:02,874 - INFO  [DistributedHerder-connect-2-1:AppInfoParser@83] - App info kafka.producer for producer-5 unregistered
2023-07-30 13:50:02,874 - INFO  [DistributedHerder-connect-2-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:50:02,874 - INFO  [DistributedHerder-connect-2-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:50:02,874 - INFO  [DistributedHerder-connect-2-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:50:02,876 - INFO  [DistributedHerder-connect-2-1:AppInfoParser@83] - App info kafka.consumer for consumer-backup-mm2-4 unregistered
2023-07-30 13:50:02,877 - INFO  [DistributedHerder-connect-2-1:KafkaBasedLog@193] - Stopped KafkaBasedLog for topic mm2-offsets.backup.internal
2023-07-30 13:50:02,877 - INFO  [DistributedHerder-connect-2-1:KafkaOffsetBackingStore@136] - Stopped KafkaOffsetBackingStore
2023-07-30 13:50:02,877 - INFO  [DistributedHerder-connect-2-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:50:02,877 - INFO  [DistributedHerder-connect-2-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:50:02,877 - INFO  [DistributedHerder-connect-2-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:50:02,877 - INFO  [DistributedHerder-connect-2-1:AppInfoParser@83] - App info kafka.connect for localhost:33591 unregistered
2023-07-30 13:50:02,877 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(mm2-configs.backup.internal-0)
2023-07-30 13:50:02,877 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(mm2-status.primary.internal-1, mm2-status.primary.internal-0, mm2-status.primary.internal-3, mm2-status.primary.internal-2, mm2-status.primary.internal-4)
2023-07-30 13:50:02,877 - INFO  [DistributedHerder-connect-2-1:Worker@230] - Worker stopped
2023-07-30 13:50:02,877 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [ReplicaAlterLogDirsManager on broker 0] Removed fetcher for partitions Set(mm2-status.primary.internal-1, mm2-status.primary.internal-0, mm2-status.primary.internal-3, mm2-status.primary.internal-2, mm2-status.primary.internal-4)
2023-07-30 13:50:02,878 - INFO  [DistributedHerder-connect-2-1:DistributedHerder@299] - [Worker clientId=connect-2, groupId=backup-mm2] Herder stopped
2023-07-30 13:50:02,877 - INFO  [data-plane-kafka-request-handler-2:Logging@66] - [ReplicaAlterLogDirsManager on broker 0] Removed fetcher for partitions Set(mm2-configs.backup.internal-0)
2023-07-30 13:50:02,879 - INFO  [main:DistributedHerder@701] - [Worker clientId=connect-2, groupId=backup-mm2] Herder stopped
2023-07-30 13:50:02,879 - INFO  [main:Connect@72] - Kafka Connect stopped
2023-07-30 13:50:02,879 - INFO  [main:EmbeddedConnectCluster@205] - Stopping worker WorkerHandle{workerName='connect-worker-2'workerURL='http://localhost:37073/'}
2023-07-30 13:50:02,879 - INFO  [main:Connect@67] - Kafka Connect stopping
2023-07-30 13:50:02,879 - INFO  [main:RestServer@327] - Stopping REST server
2023-07-30 13:50:02,882 - INFO  [main:AbstractConnector@381] - Stopped http_localhost0@28da7d11{HTTP/1.1, (http/1.1)}{localhost:0}
2023-07-30 13:50:02,883 - INFO  [main:HouseKeeper@158] - node0 Stopped scavenging
2023-07-30 13:50:02,883 - INFO  [main:RestServer@344] - REST server stopped
2023-07-30 13:50:02,883 - INFO  [main:DistributedHerder@681] - [Worker clientId=connect-3, groupId=backup-mm2] Herder stopping
2023-07-30 13:50:02,884 - INFO  [DistributedHerder-connect-3-1:DistributedHerder@655] - [Worker clientId=connect-3, groupId=backup-mm2] Stopping connectors and tasks that are still assigned to this worker.
2023-07-30 13:50:02,884 - INFO  [DistributedHerder-connect-3-1:AbstractCoordinator@1016] - [Worker clientId=connect-3, groupId=backup-mm2] Member connect-3-e9f825fd-ae6e-40a7-b695-08f8d5226443 sending LeaveGroup request to coordinator localhost:35687 (id: 2147483647 rack: null) due to the consumer is being closed
2023-07-30 13:50:02,884 - WARN  [DistributedHerder-connect-3-1:AbstractCoordinator@997] - [Worker clientId=connect-3, groupId=backup-mm2] Close timed out with 1 pending requests to coordinator, terminating client connections
2023-07-30 13:50:02,884 - INFO  [DistributedHerder-connect-3-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:50:02,884 - INFO  [DistributedHerder-connect-3-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:50:02,885 - INFO  [DistributedHerder-connect-3-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:50:02,885 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [GroupCoordinator 0]: Member[group.instance.id None, member.id connect-3-e9f825fd-ae6e-40a7-b695-08f8d5226443] in group backup-mm2 has left, removing it from the group
2023-07-30 13:50:02,885 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [GroupCoordinator 0]: Group backup-mm2 with generation 3 is now empty (__consumer_offsets-5)
2023-07-30 13:50:02,886 - INFO  [DistributedHerder-connect-3-1:AppInfoParser@83] - App info kafka.connect for connect-3 unregistered
2023-07-30 13:50:02,886 - INFO  [DistributedHerder-connect-3-1:KafkaBasedLog@167] - Stopping KafkaBasedLog for topic mm2-status.backup.internal
2023-07-30 13:50:02,886 - INFO  [DistributedHerder-connect-3-1:KafkaProducer@1193] - [Producer clientId=producer-9] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:50:02,887 - INFO  [DistributedHerder-connect-3-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:50:02,887 - INFO  [DistributedHerder-connect-3-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:50:02,887 - INFO  [DistributedHerder-connect-3-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:50:02,887 - INFO  [DistributedHerder-connect-3-1:AppInfoParser@83] - App info kafka.producer for producer-9 unregistered
2023-07-30 13:50:02,888 - INFO  [DistributedHerder-connect-3-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:50:02,888 - INFO  [DistributedHerder-connect-3-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:50:02,888 - INFO  [DistributedHerder-connect-3-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:50:02,889 - INFO  [DistributedHerder-connect-3-1:AppInfoParser@83] - App info kafka.consumer for consumer-backup-mm2-8 unregistered
2023-07-30 13:50:02,889 - INFO  [DistributedHerder-connect-3-1:KafkaBasedLog@193] - Stopped KafkaBasedLog for topic mm2-status.backup.internal
2023-07-30 13:50:02,889 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(mm2-configs.backup.internal-0)
2023-07-30 13:50:02,889 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(mm2-status.primary.internal-1, mm2-status.primary.internal-0, mm2-status.primary.internal-3, mm2-status.primary.internal-2, mm2-status.primary.internal-4)
2023-07-30 13:50:02,889 - INFO  [DistributedHerder-connect-3-1:KafkaConfigBackingStore@285] - Closing KafkaConfigBackingStore
2023-07-30 13:50:02,890 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [ReplicaAlterLogDirsManager on broker 0] Removed fetcher for partitions Set(mm2-status.primary.internal-1, mm2-status.primary.internal-0, mm2-status.primary.internal-3, mm2-status.primary.internal-2, mm2-status.primary.internal-4)
2023-07-30 13:50:02,890 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [ReplicaAlterLogDirsManager on broker 0] Removed fetcher for partitions Set(mm2-configs.backup.internal-0)
2023-07-30 13:50:02,890 - INFO  [DistributedHerder-connect-3-1:KafkaBasedLog@167] - Stopping KafkaBasedLog for topic mm2-configs.backup.internal
2023-07-30 13:50:02,891 - INFO  [DistributedHerder-connect-3-1:KafkaProducer@1193] - [Producer clientId=producer-10] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:50:02,891 - INFO  [DistributedHerder-connect-3-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:50:02,892 - INFO  [DistributedHerder-connect-3-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:50:02,892 - INFO  [DistributedHerder-connect-3-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:50:02,892 - INFO  [DistributedHerder-connect-3-1:AppInfoParser@83] - App info kafka.producer for producer-10 unregistered
2023-07-30 13:50:02,892 - INFO  [DistributedHerder-connect-3-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:50:02,892 - INFO  [DistributedHerder-connect-3-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:50:02,892 - INFO  [DistributedHerder-connect-3-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:50:02,893 - INFO  [DistributedHerder-connect-3-1:AppInfoParser@83] - App info kafka.consumer for consumer-backup-mm2-9 unregistered
2023-07-30 13:50:02,893 - INFO  [DistributedHerder-connect-3-1:KafkaBasedLog@193] - Stopped KafkaBasedLog for topic mm2-configs.backup.internal
2023-07-30 13:50:02,893 - INFO  [DistributedHerder-connect-3-1:KafkaConfigBackingStore@287] - Closed KafkaConfigBackingStore
2023-07-30 13:50:02,893 - INFO  [DistributedHerder-connect-3-1:Worker@209] - Worker stopping
2023-07-30 13:50:02,894 - INFO  [DistributedHerder-connect-3-1:KafkaOffsetBackingStore@134] - Stopping KafkaOffsetBackingStore
2023-07-30 13:50:02,895 - INFO  [DistributedHerder-connect-3-1:KafkaBasedLog@167] - Stopping KafkaBasedLog for topic mm2-offsets.backup.internal
2023-07-30 13:50:02,895 - INFO  [DistributedHerder-connect-3-1:KafkaProducer@1193] - [Producer clientId=producer-8] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:50:02,896 - INFO  [DistributedHerder-connect-3-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:50:02,896 - INFO  [DistributedHerder-connect-3-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:50:02,896 - INFO  [DistributedHerder-connect-3-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:50:02,896 - INFO  [DistributedHerder-connect-3-1:AppInfoParser@83] - App info kafka.producer for producer-8 unregistered
2023-07-30 13:50:02,897 - INFO  [DistributedHerder-connect-3-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:50:02,897 - INFO  [DistributedHerder-connect-3-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:50:02,897 - INFO  [DistributedHerder-connect-3-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:50:02,898 - INFO  [DistributedHerder-connect-3-1:AppInfoParser@83] - App info kafka.consumer for consumer-backup-mm2-7 unregistered
2023-07-30 13:50:02,898 - INFO  [DistributedHerder-connect-3-1:KafkaBasedLog@193] - Stopped KafkaBasedLog for topic mm2-offsets.backup.internal
2023-07-30 13:50:02,898 - INFO  [DistributedHerder-connect-3-1:KafkaOffsetBackingStore@136] - Stopped KafkaOffsetBackingStore
2023-07-30 13:50:02,898 - INFO  [DistributedHerder-connect-3-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:50:02,899 - INFO  [DistributedHerder-connect-3-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:50:02,899 - INFO  [DistributedHerder-connect-3-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:50:02,902 - INFO  [DistributedHerder-connect-3-1:AppInfoParser@83] - App info kafka.connect for localhost:37073 unregistered
2023-07-30 13:50:02,902 - INFO  [DistributedHerder-connect-3-1:Worker@230] - Worker stopped
2023-07-30 13:50:02,903 - INFO  [DistributedHerder-connect-3-1:DistributedHerder@299] - [Worker clientId=connect-3, groupId=backup-mm2] Herder stopped
2023-07-30 13:50:02,904 - INFO  [main:DistributedHerder@701] - [Worker clientId=connect-3, groupId=backup-mm2] Herder stopped
2023-07-30 13:50:02,904 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 66 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:02,904 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 66 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:02,904 - INFO  [main:Connect@72] - Kafka Connect stopped
2023-07-30 13:50:02,904 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 70 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:02,905 - INFO  [main:KafkaProducer@1193] - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:50:02,905 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Log for partition mm2-status.primary.internal-1 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/mm2-status.primary.internal-1.47338a7f4cec40bcba1785ad6955c74e-delete and is scheduled for deletion
2023-07-30 13:50:02,905 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-configs.backup.internal-0 is renamed to /tmp/junit6293896813403981889/junit8934247217114872089/mm2-configs.backup.internal-0.ccf2fdfca1a94ccf8cf68c0d2876e4dc-delete and is scheduled for deletion
2023-07-30 13:50:02,906 - INFO  [main:Metrics@668] - Metrics scheduler closed
2023-07-30 13:50:02,906 - INFO  [main:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:50:02,906 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Log for partition mm2-status.primary.internal-0 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/mm2-status.primary.internal-0.5eee9175566a474086bbcae938a00863-delete and is scheduled for deletion
2023-07-30 13:50:02,906 - INFO  [main:Metrics@678] - Metrics reporters closed
2023-07-30 13:50:02,907 - INFO  [main:AppInfoParser@83] - App info kafka.producer for producer-1 unregistered
2023-07-30 13:50:02,907 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Log for partition mm2-status.primary.internal-3 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/mm2-status.primary.internal-3.2306897d1d1a4bdeb0638d9302d83070-delete and is scheduled for deletion
2023-07-30 13:50:02,908 - INFO  [main:Logging@66] - [KafkaServer id=0] shutting down
2023-07-30 13:50:02,908 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Log for partition mm2-status.primary.internal-2 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/mm2-status.primary.internal-2.6f5869f020884da6b635fe289b8d55b0-delete and is scheduled for deletion
2023-07-30 13:50:02,909 - INFO  [main:Logging@66] - [KafkaServer id=0] Starting controlled shutdown
2023-07-30 13:50:02,909 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - Log for partition mm2-status.primary.internal-4 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/mm2-status.primary.internal-4.75a24aad5efe4be4bd00e94e377697d1-delete and is scheduled for deletion
2023-07-30 13:50:02,930 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Group consumer-group-testReplicationWithEmptyPartition transitioned to Dead in generation 2
2023-07-30 13:50:02,932 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Group backup-mm2 transitioned to Dead in generation 3
2023-07-30 13:50:02,933 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupMetadataManager brokerId=0] Group consumer-group-dummy transitioned to Dead in generation 2
2023-07-30 13:50:02,933 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Removed 20 offsets associated with deleted partitions: heartbeats-0, test-topic-with-empty-partition-3, test-topic-with-empty-partition-7, test-topic-with-empty-partition-2, test-topic-with-empty-partition-6, test-topic-with-empty-partition-5, test-topic-with-empty-partition-1, test-topic-with-empty-partition-9, test-topic-with-empty-partition-4, test-topic-with-empty-partition-0, test-topic-with-empty-partition-8, mm2-status.backup.internal-2, mm2-status.backup.internal-3, mm2-status.backup.internal-0, mm2-status.backup.internal-4, mm2-status.backup.internal-1, mm2-offset-syncs.backup.internal-0, backup.test-topic-1-0, test-topic-1-3, test-topic-1-7, test-topic-1-2, test-topic-1-6, test-topic-1-5, test-topic-1-9, test-topic-1-1, test-topic-1-4, test-topic-1-8, test-topic-1-0, mm2-offsets.backup.internal-18, mm2-offsets.backup.internal-22, mm2-offsets.backup.internal-1, mm2-offsets.backup.internal-2, mm2-offsets.backup.internal-6, mm2-offsets.backup.internal-10, mm2-offsets.backup.internal-14, mm2-offsets.backup.internal-21, mm2-offsets.backup.internal-0, mm2-offsets.backup.internal-5, mm2-offsets.backup.internal-9, mm2-offsets.backup.internal-13, mm2-offsets.backup.internal-17, mm2-offsets.backup.internal-20, mm2-offsets.backup.internal-24, mm2-offsets.backup.internal-4, mm2-offsets.backup.internal-8, mm2-offsets.backup.internal-12, mm2-offsets.backup.internal-16, mm2-offsets.backup.internal-19, mm2-offsets.backup.internal-23, mm2-offsets.backup.internal-3, mm2-offsets.backup.internal-7, mm2-offsets.backup.internal-11, mm2-offsets.backup.internal-15.
2023-07-30 13:50:02,936 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [GroupMetadataManager brokerId=0] Group consumer-group-dummy transitioned to Dead in generation 2
2023-07-30 13:50:02,937 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [GroupCoordinator 0]: Removed 10 offsets associated with deleted partitions: mm2-offsets.primary.internal-2, mm2-offsets.primary.internal-23, mm2-offsets.primary.internal-18, mm2-offsets.primary.internal-14, mm2-offsets.primary.internal-10, mm2-offsets.primary.internal-6, mm2-offsets.primary.internal-1, mm2-offsets.primary.internal-22, mm2-offsets.primary.internal-17, mm2-offsets.primary.internal-13, mm2-offsets.primary.internal-9, mm2-offsets.primary.internal-5, mm2-offsets.primary.internal-0, mm2-offsets.primary.internal-21, mm2-offsets.primary.internal-16, mm2-offsets.primary.internal-12, mm2-offsets.primary.internal-8, mm2-offsets.primary.internal-4, mm2-offsets.primary.internal-3, mm2-offsets.primary.internal-24, mm2-offsets.primary.internal-20, mm2-offsets.primary.internal-19, mm2-offsets.primary.internal-15, mm2-offsets.primary.internal-11, mm2-offsets.primary.internal-7, heartbeats-0, primary.test-topic-1-9, primary.test-topic-1-0, primary.test-topic-1-4, primary.test-topic-1-6, primary.test-topic-1-1, primary.test-topic-1-5, primary.test-topic-1-7, primary.test-topic-1-2, primary.test-topic-1-8, primary.test-topic-1-3, mm2-configs.primary.internal-0, primary.checkpoints.internal-0, primary.test-topic-with-empty-partition-9, primary.test-topic-with-empty-partition-5, primary.test-topic-with-empty-partition-1, primary.test-topic-with-empty-partition-6, primary.test-topic-with-empty-partition-2, primary.test-topic-with-empty-partition-7, primary.test-topic-with-empty-partition-3, primary.test-topic-with-empty-partition-8, primary.test-topic-with-empty-partition-4, primary.test-topic-with-empty-partition-0, primary.heartbeats-0, test-topic-1-3, test-topic-1-7, test-topic-1-2, test-topic-1-6, test-topic-1-5, test-topic-1-9, test-topic-1-1, test-topic-1-4, test-topic-1-8, test-topic-1-0.
2023-07-30 13:50:02,941 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(mm2-offsets.backup.internal-18, mm2-offsets.backup.internal-22, heartbeats-0, mm2-offsets.backup.internal-1, test-topic-1-3, test-topic-with-empty-partition-3, mm2-offsets.backup.internal-2, test-topic-1-7, mm2-offsets.backup.internal-6, mm2-status.backup.internal-2, mm2-offsets.backup.internal-10, test-topic-with-empty-partition-7, mm2-offsets.backup.internal-14, mm2-offsets.backup.internal-21, backup.test-topic-1-0, mm2-offsets.backup.internal-0, test-topic-1-2, test-topic-with-empty-partition-2, mm2-offsets.backup.internal-5, test-topic-1-6, mm2-offsets.backup.internal-9, mm2-status.backup.internal-3, mm2-offsets.backup.internal-13, test-topic-with-empty-partition-6, mm2-offsets.backup.internal-17, mm2-offsets.backup.internal-20, mm2-offsets.backup.internal-24, test-topic-1-5, test-topic-with-empty-partition-5, mm2-offsets.backup.internal-4, test-topic-1-9, mm2-status.backup.internal-0, test-topic-with-empty-partition-1, mm2-offsets.backup.internal-8, mm2-status.backup.internal-4, mm2-offsets.backup.internal-12, test-topic-1-1, test-topic-with-empty-partition-9, mm2-offsets.backup.internal-16, mm2-offsets.backup.internal-19, mm2-offsets.backup.internal-23, mm2-offset-syncs.backup.internal-0, mm2-offsets.backup.internal-3, test-topic-1-4, test-topic-with-empty-partition-4, mm2-offsets.backup.internal-7, test-topic-1-8, mm2-status.backup.internal-1, test-topic-with-empty-partition-0, mm2-offsets.backup.internal-11, mm2-offsets.backup.internal-15, test-topic-1-0, test-topic-with-empty-partition-8)
2023-07-30 13:50:02,941 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [ReplicaAlterLogDirsManager on broker 0] Removed fetcher for partitions Set(mm2-offsets.backup.internal-18, mm2-offsets.backup.internal-22, heartbeats-0, mm2-offsets.backup.internal-1, test-topic-1-3, test-topic-with-empty-partition-3, mm2-offsets.backup.internal-2, test-topic-1-7, mm2-offsets.backup.internal-6, mm2-status.backup.internal-2, mm2-offsets.backup.internal-10, test-topic-with-empty-partition-7, mm2-offsets.backup.internal-14, mm2-offsets.backup.internal-21, backup.test-topic-1-0, mm2-offsets.backup.internal-0, test-topic-1-2, test-topic-with-empty-partition-2, mm2-offsets.backup.internal-5, test-topic-1-6, mm2-offsets.backup.internal-9, mm2-status.backup.internal-3, mm2-offsets.backup.internal-13, test-topic-with-empty-partition-6, mm2-offsets.backup.internal-17, mm2-offsets.backup.internal-20, mm2-offsets.backup.internal-24, test-topic-1-5, test-topic-with-empty-partition-5, mm2-offsets.backup.internal-4, test-topic-1-9, mm2-status.backup.internal-0, test-topic-with-empty-partition-1, mm2-offsets.backup.internal-8, mm2-status.backup.internal-4, mm2-offsets.backup.internal-12, test-topic-1-1, test-topic-with-empty-partition-9, mm2-offsets.backup.internal-16, mm2-offsets.backup.internal-19, mm2-offsets.backup.internal-23, mm2-offset-syncs.backup.internal-0, mm2-offsets.backup.internal-3, test-topic-1-4, test-topic-with-empty-partition-4, mm2-offsets.backup.internal-7, test-topic-1-8, mm2-status.backup.internal-1, test-topic-with-empty-partition-0, mm2-offsets.backup.internal-11, mm2-offsets.backup.internal-15, test-topic-1-0, test-topic-with-empty-partition-8)
2023-07-30 13:50:02,942 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(mm2-offsets.primary.internal-2, primary.test-topic-with-empty-partition-9, primary.test-topic-with-empty-partition-5, primary.test-topic-1-9, mm2-offsets.primary.internal-23, heartbeats-0, test-topic-1-3, mm2-offsets.primary.internal-18, primary.test-topic-1-0, test-topic-1-7, mm2-offsets.primary.internal-14, primary.test-topic-1-4, mm2-offsets.primary.internal-10, primary.test-topic-with-empty-partition-1, mm2-offsets.primary.internal-6, primary.test-topic-with-empty-partition-6, mm2-offsets.primary.internal-1, primary.test-topic-with-empty-partition-2, primary.test-topic-1-6, mm2-offsets.primary.internal-22, test-topic-1-2, mm2-offsets.primary.internal-17, primary.test-topic-1-1, test-topic-1-6, mm2-offsets.primary.internal-13, primary.test-topic-1-5, mm2-offsets.primary.internal-9, mm2-offsets.primary.internal-5, primary.test-topic-with-empty-partition-7, mm2-offsets.primary.internal-0, primary.checkpoints.internal-0, primary.test-topic-with-empty-partition-3, mm2-configs.primary.internal-0, primary.test-topic-1-7, mm2-offsets.primary.internal-21, test-topic-1-5, mm2-offsets.primary.internal-16, primary.test-topic-1-2, test-topic-1-9, mm2-offsets.primary.internal-12, mm2-offsets.primary.internal-8, test-topic-1-1, mm2-offsets.primary.internal-4, mm2-offsets.primary.internal-3, primary.test-topic-with-empty-partition-8, primary.test-topic-with-empty-partition-4, primary.test-topic-1-8, mm2-offsets.primary.internal-24, mm2-offsets.primary.internal-20, mm2-offsets.primary.internal-19, test-topic-1-4, mm2-offsets.primary.internal-15, primary.test-topic-1-3, test-topic-1-8, mm2-offsets.primary.internal-11, primary.heartbeats-0, primary.test-topic-with-empty-partition-0, mm2-offsets.primary.internal-7, test-topic-1-0)
2023-07-30 13:50:02,942 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [ReplicaAlterLogDirsManager on broker 0] Removed fetcher for partitions Set(mm2-offsets.primary.internal-2, primary.test-topic-with-empty-partition-9, primary.test-topic-with-empty-partition-5, primary.test-topic-1-9, mm2-offsets.primary.internal-23, heartbeats-0, test-topic-1-3, mm2-offsets.primary.internal-18, primary.test-topic-1-0, test-topic-1-7, mm2-offsets.primary.internal-14, primary.test-topic-1-4, mm2-offsets.primary.internal-10, primary.test-topic-with-empty-partition-1, mm2-offsets.primary.internal-6, primary.test-topic-with-empty-partition-6, mm2-offsets.primary.internal-1, primary.test-topic-with-empty-partition-2, primary.test-topic-1-6, mm2-offsets.primary.internal-22, test-topic-1-2, mm2-offsets.primary.internal-17, primary.test-topic-1-1, test-topic-1-6, mm2-offsets.primary.internal-13, primary.test-topic-1-5, mm2-offsets.primary.internal-9, mm2-offsets.primary.internal-5, primary.test-topic-with-empty-partition-7, mm2-offsets.primary.internal-0, primary.checkpoints.internal-0, primary.test-topic-with-empty-partition-3, mm2-configs.primary.internal-0, primary.test-topic-1-7, mm2-offsets.primary.internal-21, test-topic-1-5, mm2-offsets.primary.internal-16, primary.test-topic-1-2, test-topic-1-9, mm2-offsets.primary.internal-12, mm2-offsets.primary.internal-8, test-topic-1-1, mm2-offsets.primary.internal-4, mm2-offsets.primary.internal-3, primary.test-topic-with-empty-partition-8, primary.test-topic-with-empty-partition-4, primary.test-topic-1-8, mm2-offsets.primary.internal-24, mm2-offsets.primary.internal-20, mm2-offsets.primary.internal-19, test-topic-1-4, mm2-offsets.primary.internal-15, primary.test-topic-1-3, test-topic-1-8, mm2-offsets.primary.internal-11, primary.heartbeats-0, primary.test-topic-with-empty-partition-0, mm2-offsets.primary.internal-7, test-topic-1-0)
2023-07-30 13:50:02,945 - INFO  [main:Logging@66] - [KafkaServer id=0] Controlled shutdown succeeded
2023-07-30 13:50:02,947 - INFO  [main:Logging@66] - [/config/changes-event-process-thread]: Shutting down
2023-07-30 13:50:02,948 - INFO  [main:Logging@66] - [/config/changes-event-process-thread]: Shutdown completed
2023-07-30 13:50:02,948 - INFO  [/config/changes-event-process-thread:Logging@66] - [/config/changes-event-process-thread]: Stopped
2023-07-30 13:50:02,949 - INFO  [main:Logging@66] - [SocketServer brokerId=0] Stopping socket server request processors
2023-07-30 13:50:02,949 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(mm2-offsets.primary.internal-2, primary.test-topic-with-empty-partition-9, primary.test-topic-with-empty-partition-5, primary.test-topic-1-9, mm2-offsets.primary.internal-23, heartbeats-0, test-topic-1-3, mm2-offsets.primary.internal-18, primary.test-topic-1-0, test-topic-1-7, mm2-offsets.primary.internal-14, primary.test-topic-1-4, mm2-offsets.primary.internal-10, primary.test-topic-with-empty-partition-1, mm2-offsets.primary.internal-6, primary.test-topic-with-empty-partition-6, mm2-offsets.primary.internal-1, primary.test-topic-with-empty-partition-2, primary.test-topic-1-6, mm2-offsets.primary.internal-22, test-topic-1-2, mm2-offsets.primary.internal-17, primary.test-topic-1-1, test-topic-1-6, mm2-offsets.primary.internal-13, primary.test-topic-1-5, mm2-offsets.primary.internal-9, mm2-offsets.primary.internal-5, primary.test-topic-with-empty-partition-7, mm2-offsets.primary.internal-0, primary.checkpoints.internal-0, primary.test-topic-with-empty-partition-3, mm2-configs.primary.internal-0, primary.test-topic-1-7, mm2-offsets.primary.internal-21, test-topic-1-5, mm2-offsets.primary.internal-16, primary.test-topic-1-2, test-topic-1-9, mm2-offsets.primary.internal-12, mm2-offsets.primary.internal-8, test-topic-1-1, mm2-offsets.primary.internal-4, mm2-offsets.primary.internal-3, primary.test-topic-with-empty-partition-8, primary.test-topic-with-empty-partition-4, primary.test-topic-1-8, mm2-offsets.primary.internal-24, mm2-offsets.primary.internal-20, mm2-offsets.primary.internal-19, test-topic-1-4, mm2-offsets.primary.internal-15, primary.test-topic-1-3, test-topic-1-8, mm2-offsets.primary.internal-11, primary.heartbeats-0, primary.test-topic-with-empty-partition-0, mm2-offsets.primary.internal-7, test-topic-1-0)
2023-07-30 13:50:02,949 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(mm2-offsets.backup.internal-18, mm2-offsets.backup.internal-22, heartbeats-0, mm2-offsets.backup.internal-1, test-topic-1-3, test-topic-with-empty-partition-3, mm2-offsets.backup.internal-2, test-topic-1-7, mm2-offsets.backup.internal-6, mm2-status.backup.internal-2, mm2-offsets.backup.internal-10, test-topic-with-empty-partition-7, mm2-offsets.backup.internal-14, mm2-offsets.backup.internal-21, backup.test-topic-1-0, mm2-offsets.backup.internal-0, test-topic-1-2, test-topic-with-empty-partition-2, mm2-offsets.backup.internal-5, test-topic-1-6, mm2-offsets.backup.internal-9, mm2-status.backup.internal-3, mm2-offsets.backup.internal-13, test-topic-with-empty-partition-6, mm2-offsets.backup.internal-17, mm2-offsets.backup.internal-20, mm2-offsets.backup.internal-24, test-topic-1-5, test-topic-with-empty-partition-5, mm2-offsets.backup.internal-4, test-topic-1-9, mm2-status.backup.internal-0, test-topic-with-empty-partition-1, mm2-offsets.backup.internal-8, mm2-status.backup.internal-4, mm2-offsets.backup.internal-12, test-topic-1-1, test-topic-with-empty-partition-9, mm2-offsets.backup.internal-16, mm2-offsets.backup.internal-19, mm2-offsets.backup.internal-23, mm2-offset-syncs.backup.internal-0, mm2-offsets.backup.internal-3, test-topic-1-4, test-topic-with-empty-partition-4, mm2-offsets.backup.internal-7, test-topic-1-8, mm2-status.backup.internal-1, test-topic-with-empty-partition-0, mm2-offsets.backup.internal-11, mm2-offsets.backup.internal-15, test-topic-1-0, test-topic-with-empty-partition-8)
2023-07-30 13:50:02,949 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - [ReplicaAlterLogDirsManager on broker 0] Removed fetcher for partitions Set(mm2-offsets.primary.internal-2, primary.test-topic-with-empty-partition-9, primary.test-topic-with-empty-partition-5, primary.test-topic-1-9, mm2-offsets.primary.internal-23, heartbeats-0, test-topic-1-3, mm2-offsets.primary.internal-18, primary.test-topic-1-0, test-topic-1-7, mm2-offsets.primary.internal-14, primary.test-topic-1-4, mm2-offsets.primary.internal-10, primary.test-topic-with-empty-partition-1, mm2-offsets.primary.internal-6, primary.test-topic-with-empty-partition-6, mm2-offsets.primary.internal-1, primary.test-topic-with-empty-partition-2, primary.test-topic-1-6, mm2-offsets.primary.internal-22, test-topic-1-2, mm2-offsets.primary.internal-17, primary.test-topic-1-1, test-topic-1-6, mm2-offsets.primary.internal-13, primary.test-topic-1-5, mm2-offsets.primary.internal-9, mm2-offsets.primary.internal-5, primary.test-topic-with-empty-partition-7, mm2-offsets.primary.internal-0, primary.checkpoints.internal-0, primary.test-topic-with-empty-partition-3, mm2-configs.primary.internal-0, primary.test-topic-1-7, mm2-offsets.primary.internal-21, test-topic-1-5, mm2-offsets.primary.internal-16, primary.test-topic-1-2, test-topic-1-9, mm2-offsets.primary.internal-12, mm2-offsets.primary.internal-8, test-topic-1-1, mm2-offsets.primary.internal-4, mm2-offsets.primary.internal-3, primary.test-topic-with-empty-partition-8, primary.test-topic-with-empty-partition-4, primary.test-topic-1-8, mm2-offsets.primary.internal-24, mm2-offsets.primary.internal-20, mm2-offsets.primary.internal-19, test-topic-1-4, mm2-offsets.primary.internal-15, primary.test-topic-1-3, test-topic-1-8, mm2-offsets.primary.internal-11, primary.heartbeats-0, primary.test-topic-with-empty-partition-0, mm2-offsets.primary.internal-7, test-topic-1-0)
2023-07-30 13:50:02,950 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - [ReplicaAlterLogDirsManager on broker 0] Removed fetcher for partitions Set(mm2-offsets.backup.internal-18, mm2-offsets.backup.internal-22, heartbeats-0, mm2-offsets.backup.internal-1, test-topic-1-3, test-topic-with-empty-partition-3, mm2-offsets.backup.internal-2, test-topic-1-7, mm2-offsets.backup.internal-6, mm2-status.backup.internal-2, mm2-offsets.backup.internal-10, test-topic-with-empty-partition-7, mm2-offsets.backup.internal-14, mm2-offsets.backup.internal-21, backup.test-topic-1-0, mm2-offsets.backup.internal-0, test-topic-1-2, test-topic-with-empty-partition-2, mm2-offsets.backup.internal-5, test-topic-1-6, mm2-offsets.backup.internal-9, mm2-status.backup.internal-3, mm2-offsets.backup.internal-13, test-topic-with-empty-partition-6, mm2-offsets.backup.internal-17, mm2-offsets.backup.internal-20, mm2-offsets.backup.internal-24, test-topic-1-5, test-topic-with-empty-partition-5, mm2-offsets.backup.internal-4, test-topic-1-9, mm2-status.backup.internal-0, test-topic-with-empty-partition-1, mm2-offsets.backup.internal-8, mm2-status.backup.internal-4, mm2-offsets.backup.internal-12, test-topic-1-1, test-topic-with-empty-partition-9, mm2-offsets.backup.internal-16, mm2-offsets.backup.internal-19, mm2-offsets.backup.internal-23, mm2-offset-syncs.backup.internal-0, mm2-offsets.backup.internal-3, test-topic-1-4, test-topic-with-empty-partition-4, mm2-offsets.backup.internal-7, test-topic-1-8, mm2-status.backup.internal-1, test-topic-with-empty-partition-0, mm2-offsets.backup.internal-11, mm2-offsets.backup.internal-15, test-topic-1-0, test-topic-with-empty-partition-8)
2023-07-30 13:50:02,955 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 51 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:02,956 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 52 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:02,967 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 79 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:02,967 - INFO  [main:Logging@66] - [SocketServer brokerId=0] Stopped socket server request processors
2023-07-30 13:50:02,968 - INFO  [main:Logging@66] - [data-plane Kafka Request Handler on Broker 0], shutting down
2023-07-30 13:50:02,976 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Log for partition mm2-offsets.backup.internal-18 is renamed to /tmp/junit6293896813403981889/junit8934247217114872089/mm2-offsets.backup.internal-18.b17e0f1af292413cb52b0c351665ec0c-delete and is scheduled for deletion
2023-07-30 13:50:02,977 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Log for partition mm2-offsets.backup.internal-22 is renamed to /tmp/junit6293896813403981889/junit8934247217114872089/mm2-offsets.backup.internal-22.6bd95c2713e74202bc7efbc2686c13f3-delete and is scheduled for deletion
2023-07-30 13:50:02,978 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Log for partition heartbeats-0 is renamed to /tmp/junit6293896813403981889/junit8934247217114872089/heartbeats-0.0e537599d25549098afccea18512f7cc-delete and is scheduled for deletion
2023-07-30 13:50:02,979 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Log for partition mm2-offsets.backup.internal-1 is renamed to /tmp/junit6293896813403981889/junit8934247217114872089/mm2-offsets.backup.internal-1.7da23674be3a4b7e896bd1c2cea9823c-delete and is scheduled for deletion
2023-07-30 13:50:02,980 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Log for partition test-topic-1-3 is renamed to /tmp/junit6293896813403981889/junit8934247217114872089/test-topic-1-3.a57ce486f9db4df4816feb94926c9369-delete and is scheduled for deletion
2023-07-30 13:50:02,981 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Log for partition test-topic-with-empty-partition-3 is renamed to /tmp/junit6293896813403981889/junit8934247217114872089/test-topic-with-empty-partition-3.fbbd76e2583a4a239ed1a160a8007ac9-delete and is scheduled for deletion
2023-07-30 13:50:02,981 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 76 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:02,982 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Log for partition mm2-offsets.backup.internal-2 is renamed to /tmp/junit6293896813403981889/junit8934247217114872089/mm2-offsets.backup.internal-2.5c8aa9ac11c24fc0923e8f7addc0b67a-delete and is scheduled for deletion
2023-07-30 13:50:02,982 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Log for partition test-topic-1-7 is renamed to /tmp/junit6293896813403981889/junit8934247217114872089/test-topic-1-7.cf9e38900f144e53896f0fec0d91a942-delete and is scheduled for deletion
2023-07-30 13:50:02,982 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 60 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:02,983 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 60 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:02,983 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Log for partition mm2-offsets.backup.internal-6 is renamed to /tmp/junit6293896813403981889/junit8934247217114872089/mm2-offsets.backup.internal-6.97576620de7f4304b3c2f55347f1ea28-delete and is scheduled for deletion
2023-07-30 13:50:02,984 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Log for partition mm2-status.backup.internal-2 is renamed to /tmp/junit6293896813403981889/junit8934247217114872089/mm2-status.backup.internal-2.1dba568ec7964a50b085411c0a8112b7-delete and is scheduled for deletion
2023-07-30 13:50:02,985 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Log for partition mm2-offsets.backup.internal-10 is renamed to /tmp/junit6293896813403981889/junit8934247217114872089/mm2-offsets.backup.internal-10.7a38f5c4d8684ea3beb3927a1860480e-delete and is scheduled for deletion
2023-07-30 13:50:02,985 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Log for partition test-topic-with-empty-partition-7 is renamed to /tmp/junit6293896813403981889/junit8934247217114872089/test-topic-with-empty-partition-7.b82c9cc9857e49de83ca3b624bf93677-delete and is scheduled for deletion
2023-07-30 13:50:02,986 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Log for partition mm2-offsets.backup.internal-14 is renamed to /tmp/junit6293896813403981889/junit8934247217114872089/mm2-offsets.backup.internal-14.e660bc839d4744e9ac3656dbaa75d5a7-delete and is scheduled for deletion
2023-07-30 13:50:02,986 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Log for partition mm2-offsets.backup.internal-21 is renamed to /tmp/junit6293896813403981889/junit8934247217114872089/mm2-offsets.backup.internal-21.0e5d8259226340b8b9428b2337a0a05b-delete and is scheduled for deletion
2023-07-30 13:50:02,988 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Log for partition backup.test-topic-1-0 is renamed to /tmp/junit6293896813403981889/junit8934247217114872089/backup.test-topic-1-0.5e6d79c5aa074ac08c279bbd3e9816c5-delete and is scheduled for deletion
2023-07-30 13:50:02,989 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-offsets.primary.internal-2 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/mm2-offsets.primary.internal-2.772cbff32fe34209bc50f019a7720e8f-delete and is scheduled for deletion
2023-07-30 13:50:02,990 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Log for partition mm2-offsets.backup.internal-0 is renamed to /tmp/junit6293896813403981889/junit8934247217114872089/mm2-offsets.backup.internal-0.ca793f3c23734bc8b29409ef44f19626-delete and is scheduled for deletion
2023-07-30 13:50:02,991 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Log for partition test-topic-1-2 is renamed to /tmp/junit6293896813403981889/junit8934247217114872089/test-topic-1-2.98a842f2ed9249e2b1a890680d7d0445-delete and is scheduled for deletion
2023-07-30 13:50:02,991 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition primary.test-topic-with-empty-partition-9 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/primary.test-topic-with-empty-partition-9.dc4889347b1b4f6286c0d9eed887f48a-delete and is scheduled for deletion
2023-07-30 13:50:02,992 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Log for partition test-topic-with-empty-partition-2 is renamed to /tmp/junit6293896813403981889/junit8934247217114872089/test-topic-with-empty-partition-2.2769afc27ca24ccfba04daa235c8b85d-delete and is scheduled for deletion
2023-07-30 13:50:02,992 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Log for partition mm2-offsets.backup.internal-5 is renamed to /tmp/junit6293896813403981889/junit8934247217114872089/mm2-offsets.backup.internal-5.f78792542107426b98a406f0108ce75a-delete and is scheduled for deletion
2023-07-30 13:50:02,992 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition primary.test-topic-with-empty-partition-5 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/primary.test-topic-with-empty-partition-5.f41e51fb68ef4b09b07e1aae71417bfc-delete and is scheduled for deletion
2023-07-30 13:50:02,993 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Log for partition test-topic-1-6 is renamed to /tmp/junit6293896813403981889/junit8934247217114872089/test-topic-1-6.75502948c05648ee85ca853c842a3fe4-delete and is scheduled for deletion
2023-07-30 13:50:02,993 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Log for partition mm2-offsets.backup.internal-9 is renamed to /tmp/junit6293896813403981889/junit8934247217114872089/mm2-offsets.backup.internal-9.39406bec428a46cfb3d32064148e85d2-delete and is scheduled for deletion
2023-07-30 13:50:02,994 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Log for partition mm2-status.backup.internal-3 is renamed to /tmp/junit6293896813403981889/junit8934247217114872089/mm2-status.backup.internal-3.2d721f79dbb74f2aac9f54da6ac614aa-delete and is scheduled for deletion
2023-07-30 13:50:02,994 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Log for partition mm2-offsets.backup.internal-13 is renamed to /tmp/junit6293896813403981889/junit8934247217114872089/mm2-offsets.backup.internal-13.3069790baf4e4887bb62a39f6f39d2ea-delete and is scheduled for deletion
2023-07-30 13:50:02,995 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Log for partition test-topic-with-empty-partition-6 is renamed to /tmp/junit6293896813403981889/junit8934247217114872089/test-topic-with-empty-partition-6.ff4ddbacce7f4d2c808f95c1906693c5-delete and is scheduled for deletion
2023-07-30 13:50:02,995 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Log for partition mm2-offsets.backup.internal-17 is renamed to /tmp/junit6293896813403981889/junit8934247217114872089/mm2-offsets.backup.internal-17.2ba7f245fae247a3972662f92707d321-delete and is scheduled for deletion
2023-07-30 13:50:02,996 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Log for partition mm2-offsets.backup.internal-20 is renamed to /tmp/junit6293896813403981889/junit8934247217114872089/mm2-offsets.backup.internal-20.7930e67e32564e2aa2426d4cc99ea6c8-delete and is scheduled for deletion
2023-07-30 13:50:02,996 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Log for partition mm2-offsets.backup.internal-24 is renamed to /tmp/junit6293896813403981889/junit8934247217114872089/mm2-offsets.backup.internal-24.38f5b8282b5e4f6ea4a343518cd6720d-delete and is scheduled for deletion
2023-07-30 13:50:02,997 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Log for partition test-topic-1-5 is renamed to /tmp/junit6293896813403981889/junit8934247217114872089/test-topic-1-5.b5273b7219fc4fe7810aefc6013157c8-delete and is scheduled for deletion
2023-07-30 13:50:02,997 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Log for partition test-topic-with-empty-partition-5 is renamed to /tmp/junit6293896813403981889/junit8934247217114872089/test-topic-with-empty-partition-5.d53f6ec1e88b47558439617bb35928b1-delete and is scheduled for deletion
2023-07-30 13:50:02,997 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition primary.test-topic-1-9 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/primary.test-topic-1-9.ce9056f432dd43769e6da7a67128bba1-delete and is scheduled for deletion
2023-07-30 13:50:02,997 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Log for partition mm2-offsets.backup.internal-4 is renamed to /tmp/junit6293896813403981889/junit8934247217114872089/mm2-offsets.backup.internal-4.be5202618cc947e0915b9947af5725f7-delete and is scheduled for deletion
2023-07-30 13:50:02,998 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-offsets.primary.internal-23 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/mm2-offsets.primary.internal-23.1e3775603fdd433ba4d5fc9ad4ed742b-delete and is scheduled for deletion
2023-07-30 13:50:02,998 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Log for partition test-topic-1-9 is renamed to /tmp/junit6293896813403981889/junit8934247217114872089/test-topic-1-9.db2efa2fc489486e940ac1ef7e9334bd-delete and is scheduled for deletion
2023-07-30 13:50:02,999 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition heartbeats-0 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/heartbeats-0.173186845ea8455fb250163c6cddca61-delete and is scheduled for deletion
2023-07-30 13:50:02,999 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Log for partition mm2-status.backup.internal-0 is renamed to /tmp/junit6293896813403981889/junit8934247217114872089/mm2-status.backup.internal-0.0da628b7b54c4069b49eb5234b7177b4-delete and is scheduled for deletion
2023-07-30 13:50:02,999 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition test-topic-1-3 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/test-topic-1-3.41259ce83fb64c2f90eaabb5228398fe-delete and is scheduled for deletion
2023-07-30 13:50:02,999 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Log for partition test-topic-with-empty-partition-1 is renamed to /tmp/junit6293896813403981889/junit8934247217114872089/test-topic-with-empty-partition-1.50c7db97ffb847499db09fc8ecd1825c-delete and is scheduled for deletion
2023-07-30 13:50:02,999 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-offsets.primary.internal-18 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/mm2-offsets.primary.internal-18.42fffe5373c34afbae89c7e0f8b49cd2-delete and is scheduled for deletion
2023-07-30 13:50:03,000 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Log for partition mm2-offsets.backup.internal-8 is renamed to /tmp/junit6293896813403981889/junit8934247217114872089/mm2-offsets.backup.internal-8.8ab461913d62478ca198e718b84a2c6e-delete and is scheduled for deletion
2023-07-30 13:50:03,000 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition primary.test-topic-1-0 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/primary.test-topic-1-0.a67515305c2d4f59b63258637baf17eb-delete and is scheduled for deletion
2023-07-30 13:50:03,000 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Log for partition mm2-status.backup.internal-4 is renamed to /tmp/junit6293896813403981889/junit8934247217114872089/mm2-status.backup.internal-4.a868e42c381d47abae1488e39f91bfb5-delete and is scheduled for deletion
2023-07-30 13:50:03,000 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition test-topic-1-7 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/test-topic-1-7.330497806961489dbef02a8d16e73baf-delete and is scheduled for deletion
2023-07-30 13:50:03,001 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Log for partition mm2-offsets.backup.internal-12 is renamed to /tmp/junit6293896813403981889/junit8934247217114872089/mm2-offsets.backup.internal-12.3c7e89fd4b24488c9373827880c590e0-delete and is scheduled for deletion
2023-07-30 13:50:03,002 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-offsets.primary.internal-14 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/mm2-offsets.primary.internal-14.11c58aa3d0704a8688e6e6c4715edef5-delete and is scheduled for deletion
2023-07-30 13:50:03,002 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Log for partition test-topic-1-1 is renamed to /tmp/junit6293896813403981889/junit8934247217114872089/test-topic-1-1.4b0293c7529b4d69b5c3d9022aa4a4ac-delete and is scheduled for deletion
2023-07-30 13:50:03,003 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition primary.test-topic-1-4 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/primary.test-topic-1-4.2abfcf482212471c951540fedffc0300-delete and is scheduled for deletion
2023-07-30 13:50:03,003 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Log for partition test-topic-with-empty-partition-9 is renamed to /tmp/junit6293896813403981889/junit8934247217114872089/test-topic-with-empty-partition-9.41f4ae711c914924bc755c9874f45683-delete and is scheduled for deletion
2023-07-30 13:50:03,003 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-offsets.primary.internal-10 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/mm2-offsets.primary.internal-10.476a32ab40914ee9824bd6f6e9f3263e-delete and is scheduled for deletion
2023-07-30 13:50:03,004 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Log for partition mm2-offsets.backup.internal-16 is renamed to /tmp/junit6293896813403981889/junit8934247217114872089/mm2-offsets.backup.internal-16.5021f4a343f54201bd49e0d17c405cca-delete and is scheduled for deletion
2023-07-30 13:50:03,004 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition primary.test-topic-with-empty-partition-1 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/primary.test-topic-with-empty-partition-1.ec9a037346594f8d9e304f269d20adbd-delete and is scheduled for deletion
2023-07-30 13:50:03,004 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Log for partition mm2-offsets.backup.internal-19 is renamed to /tmp/junit6293896813403981889/junit8934247217114872089/mm2-offsets.backup.internal-19.b158ad8f01aa4313a917010916cb3dd5-delete and is scheduled for deletion
2023-07-30 13:50:03,005 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 184 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,005 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Log for partition mm2-offsets.backup.internal-23 is renamed to /tmp/junit6293896813403981889/junit8934247217114872089/mm2-offsets.backup.internal-23.aa0b091839ad4d42964d4929c7fbafa2-delete and is scheduled for deletion
2023-07-30 13:50:03,005 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-offsets.primary.internal-6 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/mm2-offsets.primary.internal-6.079b4ebc89c44e4da47585718bb3160f-delete and is scheduled for deletion
2023-07-30 13:50:03,005 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 188 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,005 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Log for partition mm2-offset-syncs.backup.internal-0 is renamed to /tmp/junit6293896813403981889/junit8934247217114872089/mm2-offset-syncs.backup.internal-0.4f0142f558be435f86e54a141cf9e7c4-delete and is scheduled for deletion
2023-07-30 13:50:03,005 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition primary.test-topic-with-empty-partition-6 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/primary.test-topic-with-empty-partition-6.35240100d03c4b4e98bec27e0f96f760-delete and is scheduled for deletion
2023-07-30 13:50:03,006 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Log for partition mm2-offsets.backup.internal-3 is renamed to /tmp/junit6293896813403981889/junit8934247217114872089/mm2-offsets.backup.internal-3.ba7dbd759e114b4a95f35359e0580902-delete and is scheduled for deletion
2023-07-30 13:50:03,006 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-offsets.primary.internal-1 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/mm2-offsets.primary.internal-1.4ea7eb550d29482fbb9933cf7d13f08c-delete and is scheduled for deletion
2023-07-30 13:50:03,007 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Log for partition test-topic-1-4 is renamed to /tmp/junit6293896813403981889/junit8934247217114872089/test-topic-1-4.0d1a0699c6bd4866817bfe5403fc78af-delete and is scheduled for deletion
2023-07-30 13:50:03,007 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition primary.test-topic-with-empty-partition-2 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/primary.test-topic-with-empty-partition-2.19211430611a48b69e351cdf2a7f46bb-delete and is scheduled for deletion
2023-07-30 13:50:03,007 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Log for partition test-topic-with-empty-partition-4 is renamed to /tmp/junit6293896813403981889/junit8934247217114872089/test-topic-with-empty-partition-4.c025479136354217a51bbb8da1c90a79-delete and is scheduled for deletion
2023-07-30 13:50:03,007 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition primary.test-topic-1-6 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/primary.test-topic-1-6.ff8b49d0f5e94f5ea95b7ab5ad6007bf-delete and is scheduled for deletion
2023-07-30 13:50:03,007 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 177 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,008 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Log for partition mm2-offsets.backup.internal-7 is renamed to /tmp/junit6293896813403981889/junit8934247217114872089/mm2-offsets.backup.internal-7.89dd0347260d4ddd80a6f74f76276736-delete and is scheduled for deletion
2023-07-30 13:50:03,008 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-offsets.primary.internal-22 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/mm2-offsets.primary.internal-22.fbcaf36a1f9f442c9633c456ea473064-delete and is scheduled for deletion
2023-07-30 13:50:03,009 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Log for partition test-topic-1-8 is renamed to /tmp/junit6293896813403981889/junit8934247217114872089/test-topic-1-8.cdbf8e4ff77b409e82a21e3516fd8878-delete and is scheduled for deletion
2023-07-30 13:50:03,009 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition test-topic-1-2 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/test-topic-1-2.9c5d91e73f764b4581ec8cd002fb0b71-delete and is scheduled for deletion
2023-07-30 13:50:03,009 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Log for partition mm2-status.backup.internal-1 is renamed to /tmp/junit6293896813403981889/junit8934247217114872089/mm2-status.backup.internal-1.a85b72c7019c41ad8187754688eb5269-delete and is scheduled for deletion
2023-07-30 13:50:03,010 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-offsets.primary.internal-17 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/mm2-offsets.primary.internal-17.875ba74e557c4506826f868987e021ff-delete and is scheduled for deletion
2023-07-30 13:50:03,010 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Log for partition test-topic-with-empty-partition-0 is renamed to /tmp/junit6293896813403981889/junit8934247217114872089/test-topic-with-empty-partition-0.8c82d2b9eb2944ba92c24931a0fffc72-delete and is scheduled for deletion
2023-07-30 13:50:03,010 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition primary.test-topic-1-1 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/primary.test-topic-1-1.f0360a39c6ce493f856327f29d7dfb8c-delete and is scheduled for deletion
2023-07-30 13:50:03,010 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Log for partition mm2-offsets.backup.internal-11 is renamed to /tmp/junit6293896813403981889/junit8934247217114872089/mm2-offsets.backup.internal-11.71c4fb22faa546e49f8635bb36d5a911-delete and is scheduled for deletion
2023-07-30 13:50:03,011 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition test-topic-1-6 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/test-topic-1-6.fae92b6c7a2c4f64a616661e8a6d6e92-delete and is scheduled for deletion
2023-07-30 13:50:03,011 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Log for partition mm2-offsets.backup.internal-15 is renamed to /tmp/junit6293896813403981889/junit8934247217114872089/mm2-offsets.backup.internal-15.678bbfb9e74b4f00bca145aee3eb1b51-delete and is scheduled for deletion
2023-07-30 13:50:03,011 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-offsets.primary.internal-13 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/mm2-offsets.primary.internal-13.07811834cdb54a64be87cf713d4744a1-delete and is scheduled for deletion
2023-07-30 13:50:03,011 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Log for partition test-topic-1-0 is renamed to /tmp/junit6293896813403981889/junit8934247217114872089/test-topic-1-0.58957175880a40489b626221259dc86b-delete and is scheduled for deletion
2023-07-30 13:50:03,012 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition primary.test-topic-1-5 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/primary.test-topic-1-5.0b829716e89d46ecbffc971230371594-delete and is scheduled for deletion
2023-07-30 13:50:03,012 - INFO  [data-plane-kafka-request-handler-6:Logging@66] - Log for partition test-topic-with-empty-partition-8 is renamed to /tmp/junit6293896813403981889/junit8934247217114872089/test-topic-with-empty-partition-8.901d4607f9f7443eb5e422b7e948b4eb-delete and is scheduled for deletion
2023-07-30 13:50:03,012 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-offsets.primary.internal-9 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/mm2-offsets.primary.internal-9.1ce3c951c59847bc8fddafc8474ff2e3-delete and is scheduled for deletion
2023-07-30 13:50:03,014 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-offsets.primary.internal-5 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/mm2-offsets.primary.internal-5.d44fb6aba0c1459498f509ac5a1e0066-delete and is scheduled for deletion
2023-07-30 13:50:03,015 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition primary.test-topic-with-empty-partition-7 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/primary.test-topic-with-empty-partition-7.374f5385b73a4b1293399125552dcce1-delete and is scheduled for deletion
2023-07-30 13:50:03,015 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-offsets.primary.internal-0 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/mm2-offsets.primary.internal-0.b509978dd605403e90cfbf5c112b2487-delete and is scheduled for deletion
2023-07-30 13:50:03,016 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition primary.checkpoints.internal-0 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/primary.checkpoints.internal-0.6b46ebde53f2438c9fe30b7ad0874e7b-delete and is scheduled for deletion
2023-07-30 13:50:03,016 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition primary.test-topic-with-empty-partition-3 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/primary.test-topic-with-empty-partition-3.3a1a24b296db4769b04c383f20b12eee-delete and is scheduled for deletion
2023-07-30 13:50:03,017 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-configs.primary.internal-0 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/mm2-configs.primary.internal-0.3d59f3b1bb304ebe833fec1c5abc6e3a-delete and is scheduled for deletion
2023-07-30 13:50:03,017 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition primary.test-topic-1-7 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/primary.test-topic-1-7.8f430c5059954aa481bfe50166df4c1a-delete and is scheduled for deletion
2023-07-30 13:50:03,018 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-offsets.primary.internal-21 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/mm2-offsets.primary.internal-21.60193c42b0e340f5a5bc03865a58b6f8-delete and is scheduled for deletion
2023-07-30 13:50:03,018 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition test-topic-1-5 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/test-topic-1-5.eb7cda1e47324409ba1bbe2fa5dd26f3-delete and is scheduled for deletion
2023-07-30 13:50:03,019 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-offsets.primary.internal-16 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/mm2-offsets.primary.internal-16.dfab8b4b90164c47b5b1538e5bf11c7e-delete and is scheduled for deletion
2023-07-30 13:50:03,019 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition primary.test-topic-1-2 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/primary.test-topic-1-2.263002f2c49c46ca9347e86fe6f508c9-delete and is scheduled for deletion
2023-07-30 13:50:03,020 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition test-topic-1-9 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/test-topic-1-9.b919b059817c406e917213925698dc6d-delete and is scheduled for deletion
2023-07-30 13:50:03,020 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-offsets.primary.internal-12 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/mm2-offsets.primary.internal-12.e8d05c9126d443f49ebc0cbff2e42d3d-delete and is scheduled for deletion
2023-07-30 13:50:03,021 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-offsets.primary.internal-8 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/mm2-offsets.primary.internal-8.51ad2fcde8554fa9970dfc4bf321617d-delete and is scheduled for deletion
2023-07-30 13:50:03,021 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition test-topic-1-1 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/test-topic-1-1.634f1265e3294fcd8af65b5656edc7b1-delete and is scheduled for deletion
2023-07-30 13:50:03,022 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-offsets.primary.internal-4 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/mm2-offsets.primary.internal-4.a8d3193f0c8f4d2293a9d62ebe4a141a-delete and is scheduled for deletion
2023-07-30 13:50:03,022 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-offsets.primary.internal-3 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/mm2-offsets.primary.internal-3.4ae1b0557aee4d479ed2498379e1d636-delete and is scheduled for deletion
2023-07-30 13:50:03,023 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition primary.test-topic-with-empty-partition-8 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/primary.test-topic-with-empty-partition-8.38c20a5824d34ab288ea3ed63a5e4cbd-delete and is scheduled for deletion
2023-07-30 13:50:03,024 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition primary.test-topic-with-empty-partition-4 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/primary.test-topic-with-empty-partition-4.8f13b0f2b4eb4b1abca17e9001c84259-delete and is scheduled for deletion
2023-07-30 13:50:03,024 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition primary.test-topic-1-8 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/primary.test-topic-1-8.b6fbecdb12f646459bea7efc54a87cfd-delete and is scheduled for deletion
2023-07-30 13:50:03,025 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-offsets.primary.internal-24 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/mm2-offsets.primary.internal-24.47f209be5a984e0f9b195816ce032dbf-delete and is scheduled for deletion
2023-07-30 13:50:03,025 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-offsets.primary.internal-20 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/mm2-offsets.primary.internal-20.0e74886cb7ef47e6acf12914e40774c1-delete and is scheduled for deletion
2023-07-30 13:50:03,026 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-offsets.primary.internal-19 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/mm2-offsets.primary.internal-19.c88aadb44adf4452bb0f59bbec7258de-delete and is scheduled for deletion
2023-07-30 13:50:03,026 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition test-topic-1-4 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/test-topic-1-4.359c6db18bba4f2692446658fff890bd-delete and is scheduled for deletion
2023-07-30 13:50:03,027 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-offsets.primary.internal-15 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/mm2-offsets.primary.internal-15.18d5dd671c07457a966ffc188de8f5f9-delete and is scheduled for deletion
2023-07-30 13:50:03,027 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition primary.test-topic-1-3 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/primary.test-topic-1-3.bdf71e3e00284374a7b29f12e05ea8d7-delete and is scheduled for deletion
2023-07-30 13:50:03,028 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition test-topic-1-8 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/test-topic-1-8.14b31d66785b4a5d820c66eb2318f9a1-delete and is scheduled for deletion
2023-07-30 13:50:03,028 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-offsets.primary.internal-11 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/mm2-offsets.primary.internal-11.1ae7f73f4e87499a85f875887d50c4d1-delete and is scheduled for deletion
2023-07-30 13:50:03,029 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition primary.heartbeats-0 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/primary.heartbeats-0.bd64cd8faa8b4c6ab2a5e7f67536b21d-delete and is scheduled for deletion
2023-07-30 13:50:03,029 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition primary.test-topic-with-empty-partition-0 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/primary.test-topic-with-empty-partition-0.210b8c91df714d529c61547203eb658e-delete and is scheduled for deletion
2023-07-30 13:50:03,030 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition mm2-offsets.primary.internal-7 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/mm2-offsets.primary.internal-7.8660b9a8ba68459fb100c7b4e892aaa7-delete and is scheduled for deletion
2023-07-30 13:50:03,030 - INFO  [data-plane-kafka-request-handler-0:Logging@66] - Log for partition test-topic-1-0 is renamed to /tmp/junit7660765717748375067/junit866559250785637651/test-topic-1-0.32c1d2a6cec24f6ab4793106bfe3e344-delete and is scheduled for deletion
2023-07-30 13:50:03,033 - INFO  [main:Logging@66] - [data-plane Kafka Request Handler on Broker 0], shut down completely
2023-07-30 13:50:03,037 - INFO  [main:Logging@66] - [ExpirationReaper-0-AlterAcls]: Shutting down
2023-07-30 13:50:03,056 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 110 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,056 - WARN  [Controller-0-to-broker-0-send-thread:NetworkClient@780] - [Controller id=0, targetBrokerId=0] Connection to node 0 (localhost/127.0.0.1:35687) could not be established. Broker may not be available.
2023-07-30 13:50:03,056 - WARN  [kafka-admin-client-thread | adminclient-83:NetworkClient@780] - [AdminClient clientId=adminclient-83] Connection to node 0 (localhost/127.0.0.1:35687) could not be established. Broker may not be available.
2023-07-30 13:50:03,058 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 118 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,069 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 139 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,080 - INFO  [ExpirationReaper-0-AlterAcls:Logging@66] - [ExpirationReaper-0-AlterAcls]: Stopped
2023-07-30 13:50:03,080 - INFO  [main:Logging@66] - [ExpirationReaper-0-AlterAcls]: Shutdown completed
2023-07-30 13:50:03,081 - INFO  [main:Logging@66] - [KafkaApi-0] Shutdown complete.
2023-07-30 13:50:03,082 - INFO  [main:Logging@66] - [ExpirationReaper-0-topic]: Shutting down
2023-07-30 13:50:03,082 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 172 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,083 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 188 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,084 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 189 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,099 - INFO  [ExpirationReaper-0-topic:Logging@66] - [ExpirationReaper-0-topic]: Stopped
2023-07-30 13:50:03,099 - INFO  [main:Logging@66] - [ExpirationReaper-0-topic]: Shutdown completed
2023-07-30 13:50:03,101 - INFO  [main:Logging@66] - [TransactionCoordinator id=0] Shutting down.
2023-07-30 13:50:03,102 - INFO  [main:Logging@66] - [ProducerId Manager 0]: Shutdown complete: last producerId assigned 0
2023-07-30 13:50:03,103 - INFO  [main:Logging@66] - [Transaction State Manager 0]: Shutdown complete
2023-07-30 13:50:03,103 - INFO  [main:Logging@66] - [Transaction Marker Channel Manager 0]: Shutting down
2023-07-30 13:50:03,104 - INFO  [TxnMarkerSenderThread-0:Logging@66] - [Transaction Marker Channel Manager 0]: Stopped
2023-07-30 13:50:03,104 - INFO  [main:Logging@66] - [Transaction Marker Channel Manager 0]: Shutdown completed
2023-07-30 13:50:03,105 - INFO  [main:Logging@66] - [TransactionCoordinator id=0] Shutdown complete.
2023-07-30 13:50:03,106 - INFO  [main:Logging@66] - [GroupCoordinator 0]: Shutting down.
2023-07-30 13:50:03,105 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 332 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,106 - INFO  [main:Logging@66] - [ExpirationReaper-0-Heartbeat]: Shutting down
2023-07-30 13:50:03,106 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 330 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,108 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 310 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,157 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 198 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,157 - WARN  [kafka-admin-client-thread | adminclient-83:NetworkClient@780] - [AdminClient clientId=adminclient-83] Connection to node 0 (localhost/127.0.0.1:35687) could not be established. Broker may not be available.
2023-07-30 13:50:03,157 - WARN  [Controller-0-to-broker-0-send-thread:NetworkClient@780] - [Controller id=0, targetBrokerId=0] Connection to node 0 (localhost/127.0.0.1:35687) could not be established. Broker may not be available.
2023-07-30 13:50:03,159 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 199 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,171 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 219 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,183 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 280 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,184 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 358 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,185 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 363 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,206 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 479 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,206 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 488 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,207 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 469 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,229 - INFO  [ExpirationReaper-0-Heartbeat:Logging@66] - [ExpirationReaper-0-Heartbeat]: Stopped
2023-07-30 13:50:03,229 - INFO  [main:Logging@66] - [ExpirationReaper-0-Heartbeat]: Shutdown completed
2023-07-30 13:50:03,229 - INFO  [main:Logging@66] - [ExpirationReaper-0-Rebalance]: Shutting down
2023-07-30 13:50:03,275 - INFO  [main:Logging@66] - [ExpirationReaper-0-Rebalance]: Shutdown completed
2023-07-30 13:50:03,276 - WARN  [Controller-0-to-broker-0-send-thread:NetworkClient@780] - [Controller id=0, targetBrokerId=0] Connection to node 0 (localhost/127.0.0.1:35687) could not be established. Broker may not be available.
2023-07-30 13:50:03,275 - INFO  [ExpirationReaper-0-Rebalance:Logging@66] - [ExpirationReaper-0-Rebalance]: Stopped
2023-07-30 13:50:03,277 - INFO  [main:Logging@66] - [GroupCoordinator 0]: Shutdown complete.
2023-07-30 13:50:03,278 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 284 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,278 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 288 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,278 - INFO  [main:Logging@66] - [ReplicaManager broker=0] Shutting down
2023-07-30 13:50:03,278 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 276 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,279 - INFO  [main:Logging@66] - [LogDirFailureHandler]: Shutting down
2023-07-30 13:50:03,279 - INFO  [LogDirFailureHandler:Logging@66] - [LogDirFailureHandler]: Stopped
2023-07-30 13:50:03,279 - INFO  [main:Logging@66] - [LogDirFailureHandler]: Shutdown completed
2023-07-30 13:50:03,280 - INFO  [main:Logging@66] - [ReplicaFetcherManager on broker 0] shutting down
2023-07-30 13:50:03,281 - INFO  [main:Logging@66] - [ReplicaFetcherManager on broker 0] shutdown completed
2023-07-30 13:50:03,282 - INFO  [main:Logging@66] - [ReplicaAlterLogDirsManager on broker 0] shutting down
2023-07-30 13:50:03,282 - INFO  [main:Logging@66] - [ReplicaAlterLogDirsManager on broker 0] shutdown completed
2023-07-30 13:50:03,282 - INFO  [main:Logging@66] - [ExpirationReaper-0-Fetch]: Shutting down
2023-07-30 13:50:03,284 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 495 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,285 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 366 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,285 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 500 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,306 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 633 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,307 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 624 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,307 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 611 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,376 - WARN  [kafka-admin-client-thread | adminclient-83:NetworkClient@780] - [AdminClient clientId=adminclient-83] Connection to node 0 (localhost/127.0.0.1:35687) could not be established. Broker may not be available.
2023-07-30 13:50:03,377 - WARN  [Controller-0-to-broker-0-send-thread:NetworkClient@780] - [Controller id=0, targetBrokerId=0] Connection to node 0 (localhost/127.0.0.1:35687) could not be established. Broker may not be available.
2023-07-30 13:50:03,379 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 377 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,379 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 400 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,379 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 380 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,385 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 710 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,385 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 720 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,386 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 490 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,407 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 859 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,407 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 838 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,407 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 835 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,427 - INFO  [ExpirationReaper-0-Fetch:Logging@66] - [ExpirationReaper-0-Fetch]: Stopped
2023-07-30 13:50:03,427 - INFO  [main:Logging@66] - [ExpirationReaper-0-Fetch]: Shutdown completed
2023-07-30 13:50:03,428 - INFO  [main:Logging@66] - [ExpirationReaper-0-Produce]: Shutting down
2023-07-30 13:50:03,478 - WARN  [Controller-0-to-broker-0-send-thread:NetworkClient@780] - [Controller id=0, targetBrokerId=0] Connection to node 0 (localhost/127.0.0.1:35687) could not be established. Broker may not be available.
2023-07-30 13:50:03,479 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 464 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,479 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 472 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,480 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 496 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,480 - INFO  [ExpirationReaper-0-Produce:Logging@66] - [ExpirationReaper-0-Produce]: Stopped
2023-07-30 13:50:03,480 - INFO  [main:Logging@66] - [ExpirationReaper-0-Produce]: Shutdown completed
2023-07-30 13:50:03,480 - INFO  [main:Logging@66] - [ExpirationReaper-0-DeleteRecords]: Shutting down
2023-07-30 13:50:03,485 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 919 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,485 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 936 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,487 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 606 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,506 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 1069 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,507 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 1054 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,507 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 1051 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,579 - WARN  [Controller-0-to-broker-0-send-thread:NetworkClient@780] - [Controller id=0, targetBrokerId=0] Connection to node 0 (localhost/127.0.0.1:35687) could not be established. Broker may not be available.
2023-07-30 13:50:03,579 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 572 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,580 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 561 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,581 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 609 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,585 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 1152 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,585 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 1169 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,587 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 730 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,606 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 1304 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,607 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 1295 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,607 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 1275 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,680 - WARN  [Controller-0-to-broker-0-send-thread:NetworkClient@780] - [Controller id=0, targetBrokerId=0] Connection to node 0 (localhost/127.0.0.1:35687) could not be established. Broker may not be available.
2023-07-30 13:50:03,680 - INFO  [ExpirationReaper-0-DeleteRecords:Logging@66] - [ExpirationReaper-0-DeleteRecords]: Stopped
2023-07-30 13:50:03,680 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 669 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,680 - INFO  [main:Logging@66] - [ExpirationReaper-0-DeleteRecords]: Shutdown completed
2023-07-30 13:50:03,681 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 663 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,681 - INFO  [main:Logging@66] - [ExpirationReaper-0-ElectLeader]: Shutting down
2023-07-30 13:50:03,681 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 721 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,686 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 1374 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,686 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 1401 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,688 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 854 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,706 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 1545 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,707 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 1504 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,708 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 1536 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,781 - WARN  [Controller-0-to-broker-0-send-thread:NetworkClient@780] - [Controller id=0, targetBrokerId=0] Connection to node 0 (localhost/127.0.0.1:35687) could not be established. Broker may not be available.
2023-07-30 13:50:03,782 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 761 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,782 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 773 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,783 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 835 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,786 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 1604 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,787 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 1640 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,789 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 979 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,807 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 1782 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,807 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 1731 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,808 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 1784 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,878 - WARN  [kafka-admin-client-thread | adminclient-83:NetworkClient@780] - [AdminClient clientId=adminclient-83] Connection to node 0 (localhost/127.0.0.1:35687) could not be established. Broker may not be available.
2023-07-30 13:50:03,880 - INFO  [ExpirationReaper-0-ElectLeader:Logging@66] - [ExpirationReaper-0-ElectLeader]: Stopped
2023-07-30 13:50:03,880 - INFO  [main:Logging@66] - [ExpirationReaper-0-ElectLeader]: Shutdown completed
2023-07-30 13:50:03,882 - WARN  [Controller-0-to-broker-0-send-thread:NetworkClient@780] - [Controller id=0, targetBrokerId=0] Connection to node 0 (localhost/127.0.0.1:35687) could not be established. Broker may not be available.
2023-07-30 13:50:03,883 - INFO  [main:Logging@66] - [ReplicaManager broker=0] Shut down completely
2023-07-30 13:50:03,883 - INFO  [main:Logging@66] - [broker-0-to-controller-send-thread]: Shutting down
2023-07-30 13:50:03,884 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 951 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,884 - INFO  [broker-0-to-controller-send-thread:Logging@66] - [broker-0-to-controller-send-thread]: Stopped
2023-07-30 13:50:03,884 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 890 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,884 - INFO  [main:Logging@66] - [broker-0-to-controller-send-thread]: Shutdown completed
2023-07-30 13:50:03,884 - INFO  [main:Logging@66] - [broker-0-to-controller-send-thread]: Shutdown completed
2023-07-30 13:50:03,885 - INFO  [main:Logging@66] - Shutting down.
2023-07-30 13:50:03,887 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 1858 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,884 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 868 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,887 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 1906 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,890 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 1108 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,906 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 2036 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,907 - INFO  [pool-37-thread-1:Logging@66] - [ProducerStateManager partition=__consumer_offsets-35] Writing producer snapshot at offset 23
2023-07-30 13:50:03,907 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 1975 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,908 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 2049 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,922 - INFO  [pool-37-thread-1:Logging@66] - [ProducerStateManager partition=__consumer_offsets-5] Writing producer snapshot at offset 4
2023-07-30 13:50:03,925 - INFO  [pool-37-thread-1:Logging@66] - [ProducerStateManager partition=__consumer_offsets-34] Writing producer snapshot at offset 23
2023-07-30 13:50:03,940 - INFO  [main:Logging@66] - Shutdown complete.
2023-07-30 13:50:03,951 - INFO  [main:Logging@66] - [feature-zk-node-event-process-thread]: Shutting down
2023-07-30 13:50:03,951 - INFO  [feature-zk-node-event-process-thread:Logging@66] - [feature-zk-node-event-process-thread]: Stopped
2023-07-30 13:50:03,951 - INFO  [main:Logging@66] - [feature-zk-node-event-process-thread]: Shutdown completed
2023-07-30 13:50:03,953 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Closing.
2023-07-30 13:50:03,984 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 1002 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,984 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 1061 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,984 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 974 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,987 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 2180 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,988 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 2122 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:03,990 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 1240 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,006 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 2274 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,007 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 2198 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,008 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 2345 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,056 - INFO  [main:ZooKeeper@1422] - Session: 0x1087f31955f0000 closed
2023-07-30 13:50:04,056 - INFO  [main-EventThread:ClientCnxn$EventThread@524] - EventThread shut down for session: 0x1087f31955f0000
2023-07-30 13:50:04,057 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Closed.
2023-07-30 13:50:04,058 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Fetch]: Shutting down
2023-07-30 13:50:04,084 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 1181 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,085 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 1106 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,085 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 1083 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,087 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 2449 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,090 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 2392 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,092 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 1373 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,106 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 2558 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,107 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 2476 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,108 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 2599 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,185 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 1275 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,185 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 1195 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,186 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 1173 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,187 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 2679 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,190 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 2612 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,192 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 1484 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,206 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 2795 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,207 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 2698 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,211 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 2832 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,286 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 1399 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,286 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 1311 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,287 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 1283 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,287 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 2965 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,290 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 2891 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,293 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 1622 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,306 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 3090 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,307 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 2985 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,310 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 3137 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,387 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 1436 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,387 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 1530 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,387 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 1401 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,387 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 3314 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,390 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 3225 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,392 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 1758 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,406 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 3453 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,407 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 3332 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,410 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 3503 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,487 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 3712 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,487 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 1523 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,487 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 1669 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,487 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 1568 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,490 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 3606 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,492 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 1908 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,506 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 3845 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,507 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 3734 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,510 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 3918 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,587 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 4149 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,587 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 1658 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,588 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 1709 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,587 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 1819 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,590 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 4025 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,592 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 2074 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,606 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 4289 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,607 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 4170 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,610 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 4374 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,687 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 4620 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,687 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 1973 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,687 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 1797 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,687 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 1852 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,690 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 4475 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,693 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 2245 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,706 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 4777 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,707 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 4646 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,710 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 4859 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,780 - WARN  [kafka-admin-client-thread | adminclient-83:NetworkClient@780] - [AdminClient clientId=adminclient-83] Connection to node 0 (localhost/127.0.0.1:35687) could not be established. Broker may not be available.
2023-07-30 13:50:04,787 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 5026 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,787 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 1981 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,787 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 1925 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,787 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 2116 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,790 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 4865 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,793 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 2400 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,806 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 5194 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,807 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 5051 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,810 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 5278 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,886 - INFO  [ThrottledChannelReaper-Fetch:Logging@66] - [ThrottledChannelReaper-Fetch]: Stopped
2023-07-30 13:50:04,886 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Fetch]: Shutdown completed
2023-07-30 13:50:04,886 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Produce]: Shutting down
2023-07-30 13:50:04,887 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 5496 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,887 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 2276 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,887 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 2131 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,887 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 2067 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,890 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 5322 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,893 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 2571 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,906 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 5674 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,907 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 5519 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,910 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 5758 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,987 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 5932 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,987 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 2271 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,987 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 2201 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,987 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 2425 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,990 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 5753 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:04,994 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 2734 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,006 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 6130 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,007 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 5959 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,010 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 6202 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,087 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 6390 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,087 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 2425 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,087 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 2346 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,087 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 2586 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,090 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 6197 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,094 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 2904 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,106 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 6604 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,107 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 6412 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,110 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 6683 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,187 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 6868 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,187 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 2745 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,187 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 2490 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,187 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 2574 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,190 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 6658 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,194 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 3076 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,206 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 7086 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,207 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 6883 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,210 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 7166 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,287 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 7323 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,287 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 2627 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,287 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 2719 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,287 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 2900 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,290 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 7102 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,295 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 3244 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,306 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 7564 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,307 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 7345 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,310 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 7626 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,387 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 3026 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,387 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 2835 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,387 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 7691 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,387 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 2740 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,390 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 7459 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,395 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 3380 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,406 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 7939 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,407 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 7710 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,410 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 8008 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,487 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 8129 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,487 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 3176 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,488 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 2976 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,488 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 2872 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,490 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 7887 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,496 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 3541 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,506 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 8427 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,507 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 8187 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,510 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 8478 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,587 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 8605 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,587 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 3342 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,587 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 3128 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,589 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 3017 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,590 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 8357 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,597 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 3718 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,606 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 8908 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,607 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 8655 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,610 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 8968 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,687 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 9067 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,687 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 3486 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,687 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 3263 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,689 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 3148 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,690 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 8799 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,698 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 3872 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,706 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 9385 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,707 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 9118 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,710 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 9453 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,787 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 9538 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,787 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 3409 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,787 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 3640 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,790 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 3285 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,790 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 9247 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,799 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 4044 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,806 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 9868 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,807 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 9583 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,810 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 9937 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,826 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$HeartbeatResponseHandler@1097] - [Worker clientId=connect-5, groupId=primary-mm2] Attempt to heartbeat failed since group is rebalancing
2023-07-30 13:50:05,826 - INFO  [DistributedHerder-connect-5-1:WorkerCoordinator@225] - [Worker clientId=connect-5, groupId=primary-mm2] Rebalance started
2023-07-30 13:50:05,826 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator@534] - [Worker clientId=connect-5, groupId=primary-mm2] (Re-)joining group
2023-07-30 13:50:05,828 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [GroupCoordinator 0]: Stabilized group primary-mm2 generation 14 (__consumer_offsets-17)
2023-07-30 13:50:05,828 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=14, memberId='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', protocol='sessioned'}
2023-07-30 13:50:05,828 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=14, memberId='connect-4-4677c970-fed2-41c6-8269-6c02eca18a2c', protocol='sessioned'}
2023-07-30 13:50:05,828 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$JoinGroupResponseHandler@590] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully joined group with generation Generation{generationId=14, memberId='connect-6-5d9250b5-104e-4755-a5c9-fa2e5283a564', protocol='sessioned'}
2023-07-30 13:50:05,829 - INFO  [data-plane-kafka-request-handler-5:Logging@66] - [GroupCoordinator 0]: Assignment received from leader for group primary-mm2 for generation 14
2023-07-30 13:50:05,830 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-5, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=14, memberId='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', protocol='sessioned'}
2023-07-30 13:50:05,830 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-6, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=14, memberId='connect-6-5d9250b5-104e-4755-a5c9-fa2e5283a564', protocol='sessioned'}
2023-07-30 13:50:05,831 - INFO  [DistributedHerder-connect-5-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-5, groupId=primary-mm2] Joined group at generation 14 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', leaderUrl='http://localhost:39987/', offset=16, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:50:05,831 - INFO  [DistributedHerder-connect-6-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-6, groupId=primary-mm2] Joined group at generation 14 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', leaderUrl='http://localhost:39987/', offset=16, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:50:05,831 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator$SyncGroupResponseHandler@750] - [Worker clientId=connect-4, groupId=primary-mm2] Successfully synced group in generation Generation{generationId=14, memberId='connect-4-4677c970-fed2-41c6-8269-6c02eca18a2c', protocol='sessioned'}
2023-07-30 13:50:05,831 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1215] - [Worker clientId=connect-6, groupId=primary-mm2] Starting connectors and tasks using config offset 16
2023-07-30 13:50:05,831 - INFO  [DistributedHerder-connect-4-1:DistributedHerder$RebalanceListener@1688] - [Worker clientId=connect-4, groupId=primary-mm2] Joined group at generation 14 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175', leaderUrl='http://localhost:39987/', offset=16, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0
2023-07-30 13:50:05,831 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1215] - [Worker clientId=connect-5, groupId=primary-mm2] Starting connectors and tasks using config offset 16
2023-07-30 13:50:05,831 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1215] - [Worker clientId=connect-4, groupId=primary-mm2] Starting connectors and tasks using config offset 16
2023-07-30 13:50:05,831 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@1243] - [Worker clientId=connect-6, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:50:05,831 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@1243] - [Worker clientId=connect-4, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:50:05,831 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@1243] - [Worker clientId=connect-5, groupId=primary-mm2] Finished starting connectors and tasks
2023-07-30 13:50:05,886 - INFO  [ThrottledChannelReaper-Produce:Logging@66] - [ThrottledChannelReaper-Produce]: Stopped
2023-07-30 13:50:05,886 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Produce]: Shutdown completed
2023-07-30 13:50:05,886 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Request]: Shutting down
2023-07-30 13:50:05,887 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-15, groupId=primary-mm2] Error while fetching metadata with correlation id 10007 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,887 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-11, groupId=primary-mm2] Error while fetching metadata with correlation id 3798 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,888 - INFO  [ThrottledChannelReaper-Request:Logging@66] - [ThrottledChannelReaper-Request]: Stopped
2023-07-30 13:50:05,888 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Request]: Shutdown completed
2023-07-30 13:50:05,888 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-10, groupId=primary-mm2] Error while fetching metadata with correlation id 3556 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,888 - INFO  [main:Logging@66] - [ThrottledChannelReaper-ControllerMutation]: Shutting down
2023-07-30 13:50:05,889 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 3425 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,890 - INFO  [ThrottledChannelReaper-ControllerMutation:Logging@66] - [ThrottledChannelReaper-ControllerMutation]: Stopped
2023-07-30 13:50:05,890 - INFO  [main:Logging@66] - [ThrottledChannelReaper-ControllerMutation]: Shutdown completed
2023-07-30 13:50:05,890 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 9699 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,891 - INFO  [main:Logging@66] - [SocketServer brokerId=0] Shutting down socket server
2023-07-30 13:50:05,900 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-14, groupId=primary-mm2] Error while fetching metadata with correlation id 4212 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,903 - INFO  [main:Logging@66] - [SocketServer brokerId=0] Shutdown completed
2023-07-30 13:50:05,904 - INFO  [main:Metrics@668] - Metrics scheduler closed
2023-07-30 13:50:05,904 - INFO  [main:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:50:05,904 - INFO  [main:Metrics@678] - Metrics reporters closed
2023-07-30 13:50:05,906 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-13, groupId=primary-mm2] Error while fetching metadata with correlation id 10330 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,906 - INFO  [main:Logging@66] - Broker and topic stats closed
2023-07-30 13:50:05,907 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-12, groupId=primary-mm2] Error while fetching metadata with correlation id 10029 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,907 - INFO  [main:AppInfoParser@83] - App info kafka.server for 0 unregistered
2023-07-30 13:50:05,908 - INFO  [main:Logging@66] - [KafkaServer id=0] shut down completed
2023-07-30 13:50:05,908 - INFO  [main:EmbeddedKafkaCluster@198] - Cleaning up kafka log dirs at ArraySeq(/tmp/junit6293896813403981889/junit8934247217114872089)
2023-07-30 13:50:05,910 - WARN  [KafkaBasedLog Work Thread - mm2-status.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-17, groupId=primary-mm2] Error while fetching metadata with correlation id 10397 : {mm2-status.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,932 - INFO  [ConnnectionExpirer:NIOServerCnxnFactory$ConnectionExpirerThread@583] - ConnnectionExpirerThread interrupted
2023-07-30 13:50:05,933 - INFO  [NIOServerCxnFactory.SelectorThread-0:NIOServerCnxnFactory$SelectorThread@420] - selector thread exitted run method
2023-07-30 13:50:05,934 - INFO  [NIOServerCxnFactory.SelectorThread-2:NIOServerCnxnFactory$SelectorThread@420] - selector thread exitted run method
2023-07-30 13:50:05,934 - INFO  [NIOServerCxnFactory.SelectorThread-1:NIOServerCnxnFactory$SelectorThread@420] - selector thread exitted run method
2023-07-30 13:50:05,934 - INFO  [NIOServerCxnFactory.AcceptThread:/127.0.0.1:0:NIOServerCnxnFactory$AcceptThread@219] - accept thread exitted run method
2023-07-30 13:50:05,935 - INFO  [main:ZooKeeperServer@558] - shutting down
2023-07-30 13:50:05,935 - INFO  [main:SessionTrackerImpl@237] - Shutting down
2023-07-30 13:50:05,935 - INFO  [main:PrepRequestProcessor@1007] - Shutting down
2023-07-30 13:50:05,936 - INFO  [main:SyncRequestProcessor@191] - Shutting down
2023-07-30 13:50:05,936 - INFO  [ProcessThread(sid:0 cport:44067)::PrepRequestProcessor@155] - PrepRequestProcessor exited loop!
2023-07-30 13:50:05,936 - INFO  [SyncThread:0:SyncRequestProcessor@169] - SyncRequestProcessor exited!
2023-07-30 13:50:05,936 - INFO  [main:FinalRequestProcessor@514] - shutdown of request processor complete
2023-07-30 13:50:05,945 - INFO  [main:EmbeddedConnectCluster@205] - Stopping worker WorkerHandle{workerName='connect-worker-0'workerURL='http://localhost:43163/'}
2023-07-30 13:50:05,945 - INFO  [main:Connect@67] - Kafka Connect stopping
2023-07-30 13:50:05,945 - INFO  [main:RestServer@327] - Stopping REST server
2023-07-30 13:50:05,947 - INFO  [main:AbstractConnector@381] - Stopped http_localhost0@630b6190{HTTP/1.1, (http/1.1)}{localhost:0}
2023-07-30 13:50:05,947 - INFO  [main:HouseKeeper@158] - node0 Stopped scavenging
2023-07-30 13:50:05,948 - INFO  [main:RestServer@344] - REST server stopped
2023-07-30 13:50:05,948 - INFO  [main:DistributedHerder@681] - [Worker clientId=connect-4, groupId=primary-mm2] Herder stopping
2023-07-30 13:50:05,949 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@655] - [Worker clientId=connect-4, groupId=primary-mm2] Stopping connectors and tasks that are still assigned to this worker.
2023-07-30 13:50:05,949 - INFO  [DistributedHerder-connect-4-1:AbstractCoordinator@1016] - [Worker clientId=connect-4, groupId=primary-mm2] Member connect-4-4677c970-fed2-41c6-8269-6c02eca18a2c sending LeaveGroup request to coordinator localhost:45395 (id: 2147483647 rack: null) due to the consumer is being closed
2023-07-30 13:50:05,949 - WARN  [DistributedHerder-connect-4-1:AbstractCoordinator@997] - [Worker clientId=connect-4, groupId=primary-mm2] Close timed out with 1 pending requests to coordinator, terminating client connections
2023-07-30 13:50:05,949 - INFO  [DistributedHerder-connect-4-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:50:05,949 - INFO  [DistributedHerder-connect-4-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:50:05,949 - INFO  [DistributedHerder-connect-4-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:50:05,950 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [GroupCoordinator 0]: Member[group.instance.id None, member.id connect-4-4677c970-fed2-41c6-8269-6c02eca18a2c] in group primary-mm2 has left, removing it from the group
2023-07-30 13:50:05,950 - INFO  [data-plane-kafka-request-handler-7:Logging@66] - [GroupCoordinator 0]: Preparing to rebalance group primary-mm2 in state PreparingRebalance with old generation 14 (__consumer_offsets-17) (reason: removing member connect-4-4677c970-fed2-41c6-8269-6c02eca18a2c on LeaveGroup)
2023-07-30 13:50:05,950 - INFO  [DistributedHerder-connect-4-1:AppInfoParser@83] - App info kafka.connect for connect-4 unregistered
2023-07-30 13:50:05,950 - INFO  [DistributedHerder-connect-4-1:KafkaBasedLog@167] - Stopping KafkaBasedLog for topic mm2-status.primary.internal
2023-07-30 13:50:05,950 - INFO  [DistributedHerder-connect-4-1:KafkaProducer@1193] - [Producer clientId=producer-14] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:50:05,951 - INFO  [DistributedHerder-connect-4-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:50:05,951 - INFO  [DistributedHerder-connect-4-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:50:05,952 - INFO  [DistributedHerder-connect-4-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:50:05,952 - INFO  [DistributedHerder-connect-4-1:AppInfoParser@83] - App info kafka.producer for producer-14 unregistered
2023-07-30 13:50:05,952 - INFO  [DistributedHerder-connect-4-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:50:05,953 - INFO  [DistributedHerder-connect-4-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:50:05,953 - INFO  [DistributedHerder-connect-4-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:50:05,953 - INFO  [DistributedHerder-connect-4-1:AppInfoParser@83] - App info kafka.consumer for consumer-primary-mm2-12 unregistered
2023-07-30 13:50:05,953 - INFO  [DistributedHerder-connect-4-1:KafkaBasedLog@193] - Stopped KafkaBasedLog for topic mm2-status.primary.internal
2023-07-30 13:50:05,953 - INFO  [DistributedHerder-connect-4-1:KafkaConfigBackingStore@285] - Closing KafkaConfigBackingStore
2023-07-30 13:50:05,954 - INFO  [DistributedHerder-connect-4-1:KafkaBasedLog@167] - Stopping KafkaBasedLog for topic mm2-configs.primary.internal
2023-07-30 13:50:05,954 - INFO  [DistributedHerder-connect-4-1:KafkaProducer@1193] - [Producer clientId=producer-17] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:50:05,954 - INFO  [DistributedHerder-connect-4-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:50:05,954 - INFO  [DistributedHerder-connect-4-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:50:05,954 - INFO  [DistributedHerder-connect-4-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:50:05,955 - INFO  [DistributedHerder-connect-4-1:AppInfoParser@83] - App info kafka.producer for producer-17 unregistered
2023-07-30 13:50:05,955 - INFO  [DistributedHerder-connect-4-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:50:05,955 - INFO  [DistributedHerder-connect-4-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:50:05,955 - INFO  [DistributedHerder-connect-4-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:50:05,956 - INFO  [DistributedHerder-connect-4-1:AppInfoParser@83] - App info kafka.consumer for consumer-primary-mm2-15 unregistered
2023-07-30 13:50:05,956 - INFO  [DistributedHerder-connect-4-1:KafkaBasedLog@193] - Stopped KafkaBasedLog for topic mm2-configs.primary.internal
2023-07-30 13:50:05,956 - INFO  [DistributedHerder-connect-4-1:KafkaConfigBackingStore@287] - Closed KafkaConfigBackingStore
2023-07-30 13:50:05,957 - INFO  [DistributedHerder-connect-4-1:Worker@209] - Worker stopping
2023-07-30 13:50:05,957 - INFO  [DistributedHerder-connect-4-1:KafkaOffsetBackingStore@134] - Stopping KafkaOffsetBackingStore
2023-07-30 13:50:05,957 - INFO  [DistributedHerder-connect-4-1:KafkaBasedLog@167] - Stopping KafkaBasedLog for topic mm2-offsets.primary.internal
2023-07-30 13:50:05,957 - INFO  [DistributedHerder-connect-4-1:KafkaProducer@1193] - [Producer clientId=producer-13] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:50:05,958 - INFO  [DistributedHerder-connect-4-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:50:05,958 - INFO  [DistributedHerder-connect-4-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:50:05,958 - INFO  [DistributedHerder-connect-4-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:50:05,959 - INFO  [DistributedHerder-connect-4-1:AppInfoParser@83] - App info kafka.producer for producer-13 unregistered
2023-07-30 13:50:05,959 - INFO  [DistributedHerder-connect-4-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:50:05,959 - INFO  [DistributedHerder-connect-4-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:50:05,959 - INFO  [DistributedHerder-connect-4-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:50:05,960 - INFO  [DistributedHerder-connect-4-1:AppInfoParser@83] - App info kafka.consumer for consumer-primary-mm2-11 unregistered
2023-07-30 13:50:05,960 - INFO  [DistributedHerder-connect-4-1:KafkaBasedLog@193] - Stopped KafkaBasedLog for topic mm2-offsets.primary.internal
2023-07-30 13:50:05,960 - INFO  [DistributedHerder-connect-4-1:KafkaOffsetBackingStore@136] - Stopped KafkaOffsetBackingStore
2023-07-30 13:50:05,960 - INFO  [DistributedHerder-connect-4-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:50:05,961 - INFO  [DistributedHerder-connect-4-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:50:05,961 - INFO  [DistributedHerder-connect-4-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:50:05,961 - INFO  [DistributedHerder-connect-4-1:AppInfoParser@83] - App info kafka.connect for localhost:43163 unregistered
2023-07-30 13:50:05,961 - INFO  [DistributedHerder-connect-4-1:Worker@230] - Worker stopped
2023-07-30 13:50:05,961 - INFO  [DistributedHerder-connect-4-1:DistributedHerder@299] - [Worker clientId=connect-4, groupId=primary-mm2] Herder stopped
2023-07-30 13:50:05,962 - INFO  [main:DistributedHerder@701] - [Worker clientId=connect-4, groupId=primary-mm2] Herder stopped
2023-07-30 13:50:05,962 - INFO  [main:Connect@72] - Kafka Connect stopped
2023-07-30 13:50:05,963 - INFO  [main:EmbeddedConnectCluster@205] - Stopping worker WorkerHandle{workerName='connect-worker-1'workerURL='http://localhost:39987/'}
2023-07-30 13:50:05,963 - INFO  [main:Connect@67] - Kafka Connect stopping
2023-07-30 13:50:05,963 - INFO  [main:RestServer@327] - Stopping REST server
2023-07-30 13:50:05,965 - INFO  [main:AbstractConnector@381] - Stopped http_localhost0@25d0cb3a{HTTP/1.1, (http/1.1)}{localhost:0}
2023-07-30 13:50:05,965 - INFO  [main:HouseKeeper@158] - node0 Stopped scavenging
2023-07-30 13:50:05,966 - INFO  [main:RestServer@344] - REST server stopped
2023-07-30 13:50:05,966 - INFO  [main:DistributedHerder@681] - [Worker clientId=connect-5, groupId=primary-mm2] Herder stopping
2023-07-30 13:50:05,966 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@655] - [Worker clientId=connect-5, groupId=primary-mm2] Stopping connectors and tasks that are still assigned to this worker.
2023-07-30 13:50:05,966 - INFO  [DistributedHerder-connect-5-1:AbstractCoordinator@1016] - [Worker clientId=connect-5, groupId=primary-mm2] Member connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175 sending LeaveGroup request to coordinator localhost:45395 (id: 2147483647 rack: null) due to the consumer is being closed
2023-07-30 13:50:05,967 - WARN  [DistributedHerder-connect-5-1:AbstractCoordinator@997] - [Worker clientId=connect-5, groupId=primary-mm2] Close timed out with 1 pending requests to coordinator, terminating client connections
2023-07-30 13:50:05,967 - INFO  [DistributedHerder-connect-5-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:50:05,967 - INFO  [DistributedHerder-connect-5-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:50:05,967 - INFO  [DistributedHerder-connect-5-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:50:05,967 - INFO  [DistributedHerder-connect-5-1:AppInfoParser@83] - App info kafka.connect for connect-5 unregistered
2023-07-30 13:50:05,967 - INFO  [DistributedHerder-connect-5-1:KafkaBasedLog@167] - Stopping KafkaBasedLog for topic mm2-status.primary.internal
2023-07-30 13:50:05,968 - INFO  [DistributedHerder-connect-5-1:KafkaProducer@1193] - [Producer clientId=producer-15] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:50:05,968 - INFO  [data-plane-kafka-request-handler-1:Logging@66] - [GroupCoordinator 0]: Member[group.instance.id None, member.id connect-5-e4e7af97-8639-4494-90e0-2f5327ad4175] in group primary-mm2 has left, removing it from the group
2023-07-30 13:50:05,968 - INFO  [DistributedHerder-connect-5-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:50:05,968 - INFO  [DistributedHerder-connect-5-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:50:05,968 - INFO  [DistributedHerder-connect-5-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:50:05,968 - INFO  [DistributedHerder-connect-5-1:AppInfoParser@83] - App info kafka.producer for producer-15 unregistered
2023-07-30 13:50:05,969 - INFO  [DistributedHerder-connect-5-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:50:05,969 - INFO  [DistributedHerder-connect-5-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:50:05,969 - INFO  [DistributedHerder-connect-5-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:50:05,970 - INFO  [DistributedHerder-connect-5-1:AppInfoParser@83] - App info kafka.consumer for consumer-primary-mm2-13 unregistered
2023-07-30 13:50:05,970 - INFO  [DistributedHerder-connect-5-1:KafkaBasedLog@193] - Stopped KafkaBasedLog for topic mm2-status.primary.internal
2023-07-30 13:50:05,970 - INFO  [DistributedHerder-connect-5-1:KafkaConfigBackingStore@285] - Closing KafkaConfigBackingStore
2023-07-30 13:50:05,970 - INFO  [DistributedHerder-connect-5-1:KafkaBasedLog@167] - Stopping KafkaBasedLog for topic mm2-configs.primary.internal
2023-07-30 13:50:05,970 - INFO  [DistributedHerder-connect-5-1:KafkaProducer@1193] - [Producer clientId=producer-16] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:50:05,971 - INFO  [DistributedHerder-connect-5-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:50:05,971 - INFO  [DistributedHerder-connect-5-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:50:05,971 - INFO  [DistributedHerder-connect-5-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:50:05,971 - INFO  [DistributedHerder-connect-5-1:AppInfoParser@83] - App info kafka.producer for producer-16 unregistered
2023-07-30 13:50:05,971 - INFO  [DistributedHerder-connect-5-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:50:05,971 - INFO  [DistributedHerder-connect-5-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:50:05,972 - INFO  [DistributedHerder-connect-5-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:50:05,972 - INFO  [DistributedHerder-connect-5-1:AppInfoParser@83] - App info kafka.consumer for consumer-primary-mm2-14 unregistered
2023-07-30 13:50:05,972 - INFO  [DistributedHerder-connect-5-1:KafkaBasedLog@193] - Stopped KafkaBasedLog for topic mm2-configs.primary.internal
2023-07-30 13:50:05,972 - INFO  [DistributedHerder-connect-5-1:KafkaConfigBackingStore@287] - Closed KafkaConfigBackingStore
2023-07-30 13:50:05,972 - INFO  [DistributedHerder-connect-5-1:Worker@209] - Worker stopping
2023-07-30 13:50:05,972 - INFO  [DistributedHerder-connect-5-1:KafkaOffsetBackingStore@134] - Stopping KafkaOffsetBackingStore
2023-07-30 13:50:05,972 - INFO  [DistributedHerder-connect-5-1:KafkaBasedLog@167] - Stopping KafkaBasedLog for topic mm2-offsets.primary.internal
2023-07-30 13:50:05,973 - INFO  [DistributedHerder-connect-5-1:KafkaProducer@1193] - [Producer clientId=producer-12] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:50:05,986 - INFO  [DistributedHerder-connect-5-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:50:05,986 - INFO  [DistributedHerder-connect-5-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:50:05,986 - INFO  [DistributedHerder-connect-5-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:50:05,986 - WARN  [kafka-admin-client-thread | adminclient-83:NetworkClient@780] - [AdminClient clientId=adminclient-83] Connection to node 0 (localhost/127.0.0.1:35687) could not be established. Broker may not be available.
2023-07-30 13:50:05,987 - INFO  [DistributedHerder-connect-5-1:AppInfoParser@83] - App info kafka.producer for producer-12 unregistered
2023-07-30 13:50:05,987 - INFO  [DistributedHerder-connect-5-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:50:05,987 - INFO  [DistributedHerder-connect-5-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:50:05,988 - INFO  [DistributedHerder-connect-5-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:50:05,988 - INFO  [DistributedHerder-connect-5-1:AppInfoParser@83] - App info kafka.consumer for consumer-primary-mm2-10 unregistered
2023-07-30 13:50:05,988 - INFO  [DistributedHerder-connect-5-1:KafkaBasedLog@193] - Stopped KafkaBasedLog for topic mm2-offsets.primary.internal
2023-07-30 13:50:05,988 - INFO  [DistributedHerder-connect-5-1:KafkaOffsetBackingStore@136] - Stopped KafkaOffsetBackingStore
2023-07-30 13:50:05,988 - INFO  [DistributedHerder-connect-5-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:50:05,988 - INFO  [DistributedHerder-connect-5-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:50:05,988 - INFO  [DistributedHerder-connect-5-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:50:05,988 - INFO  [DistributedHerder-connect-5-1:AppInfoParser@83] - App info kafka.connect for localhost:39987 unregistered
2023-07-30 13:50:05,989 - INFO  [DistributedHerder-connect-5-1:Worker@230] - Worker stopped
2023-07-30 13:50:05,989 - INFO  [DistributedHerder-connect-5-1:DistributedHerder@299] - [Worker clientId=connect-5, groupId=primary-mm2] Herder stopped
2023-07-30 13:50:05,990 - WARN  [KafkaBasedLog Work Thread - mm2-offsets.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-16, groupId=primary-mm2] Error while fetching metadata with correlation id 3524 : {mm2-offsets.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,990 - INFO  [main:DistributedHerder@701] - [Worker clientId=connect-5, groupId=primary-mm2] Herder stopped
2023-07-30 13:50:05,990 - INFO  [main:Connect@72] - Kafka Connect stopped
2023-07-30 13:50:05,990 - WARN  [KafkaBasedLog Work Thread - mm2-configs.primary.internal:NetworkClient$DefaultMetadataUpdater@1117] - [Consumer clientId=consumer-primary-mm2-18, groupId=primary-mm2] Error while fetching metadata with correlation id 9962 : {mm2-configs.primary.internal=UNKNOWN_TOPIC_OR_PARTITION}
2023-07-30 13:50:05,990 - INFO  [main:EmbeddedConnectCluster@205] - Stopping worker WorkerHandle{workerName='connect-worker-2'workerURL='http://localhost:41733/'}
2023-07-30 13:50:05,991 - INFO  [main:Connect@67] - Kafka Connect stopping
2023-07-30 13:50:05,991 - INFO  [main:RestServer@327] - Stopping REST server
2023-07-30 13:50:05,992 - INFO  [main:AbstractConnector@381] - Stopped http_localhost0@1984212d{HTTP/1.1, (http/1.1)}{localhost:0}
2023-07-30 13:50:05,992 - INFO  [main:HouseKeeper@158] - node0 Stopped scavenging
2023-07-30 13:50:05,993 - INFO  [main:RestServer@344] - REST server stopped
2023-07-30 13:50:05,993 - INFO  [main:DistributedHerder@681] - [Worker clientId=connect-6, groupId=primary-mm2] Herder stopping
2023-07-30 13:50:05,993 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@655] - [Worker clientId=connect-6, groupId=primary-mm2] Stopping connectors and tasks that are still assigned to this worker.
2023-07-30 13:50:05,993 - INFO  [DistributedHerder-connect-6-1:AbstractCoordinator@1016] - [Worker clientId=connect-6, groupId=primary-mm2] Member connect-6-5d9250b5-104e-4755-a5c9-fa2e5283a564 sending LeaveGroup request to coordinator localhost:45395 (id: 2147483647 rack: null) due to the consumer is being closed
2023-07-30 13:50:05,993 - WARN  [DistributedHerder-connect-6-1:AbstractCoordinator@997] - [Worker clientId=connect-6, groupId=primary-mm2] Close timed out with 1 pending requests to coordinator, terminating client connections
2023-07-30 13:50:05,994 - INFO  [DistributedHerder-connect-6-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:50:05,994 - INFO  [DistributedHerder-connect-6-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:50:05,994 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [GroupCoordinator 0]: Member[group.instance.id None, member.id connect-6-5d9250b5-104e-4755-a5c9-fa2e5283a564] in group primary-mm2 has left, removing it from the group
2023-07-30 13:50:05,994 - INFO  [DistributedHerder-connect-6-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:50:05,994 - INFO  [data-plane-kafka-request-handler-4:Logging@66] - [GroupCoordinator 0]: Group primary-mm2 with generation 15 is now empty (__consumer_offsets-17)
2023-07-30 13:50:05,995 - INFO  [DistributedHerder-connect-6-1:AppInfoParser@83] - App info kafka.connect for connect-6 unregistered
2023-07-30 13:50:05,995 - INFO  [DistributedHerder-connect-6-1:KafkaBasedLog@167] - Stopping KafkaBasedLog for topic mm2-status.primary.internal
2023-07-30 13:50:05,995 - INFO  [DistributedHerder-connect-6-1:KafkaProducer@1193] - [Producer clientId=producer-19] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:50:05,996 - INFO  [DistributedHerder-connect-6-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:50:05,996 - INFO  [DistributedHerder-connect-6-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:50:05,996 - INFO  [DistributedHerder-connect-6-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:50:05,996 - INFO  [DistributedHerder-connect-6-1:AppInfoParser@83] - App info kafka.producer for producer-19 unregistered
2023-07-30 13:50:05,996 - INFO  [DistributedHerder-connect-6-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:50:05,997 - INFO  [DistributedHerder-connect-6-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:50:05,997 - INFO  [DistributedHerder-connect-6-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:50:05,997 - INFO  [DistributedHerder-connect-6-1:AppInfoParser@83] - App info kafka.consumer for consumer-primary-mm2-17 unregistered
2023-07-30 13:50:05,998 - INFO  [DistributedHerder-connect-6-1:KafkaBasedLog@193] - Stopped KafkaBasedLog for topic mm2-status.primary.internal
2023-07-30 13:50:05,998 - INFO  [DistributedHerder-connect-6-1:KafkaConfigBackingStore@285] - Closing KafkaConfigBackingStore
2023-07-30 13:50:05,998 - INFO  [DistributedHerder-connect-6-1:KafkaBasedLog@167] - Stopping KafkaBasedLog for topic mm2-configs.primary.internal
2023-07-30 13:50:05,998 - INFO  [DistributedHerder-connect-6-1:KafkaProducer@1193] - [Producer clientId=producer-20] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:50:05,998 - INFO  [DistributedHerder-connect-6-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:50:05,998 - INFO  [DistributedHerder-connect-6-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:50:05,998 - INFO  [DistributedHerder-connect-6-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:50:05,998 - INFO  [DistributedHerder-connect-6-1:AppInfoParser@83] - App info kafka.producer for producer-20 unregistered
2023-07-30 13:50:05,999 - INFO  [DistributedHerder-connect-6-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:50:05,999 - INFO  [DistributedHerder-connect-6-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:50:05,999 - INFO  [DistributedHerder-connect-6-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:50:06,000 - INFO  [DistributedHerder-connect-6-1:AppInfoParser@83] - App info kafka.consumer for consumer-primary-mm2-18 unregistered
2023-07-30 13:50:06,000 - INFO  [DistributedHerder-connect-6-1:KafkaBasedLog@193] - Stopped KafkaBasedLog for topic mm2-configs.primary.internal
2023-07-30 13:50:06,000 - INFO  [DistributedHerder-connect-6-1:KafkaConfigBackingStore@287] - Closed KafkaConfigBackingStore
2023-07-30 13:50:06,000 - INFO  [DistributedHerder-connect-6-1:Worker@209] - Worker stopping
2023-07-30 13:50:06,000 - INFO  [DistributedHerder-connect-6-1:KafkaOffsetBackingStore@134] - Stopping KafkaOffsetBackingStore
2023-07-30 13:50:06,000 - INFO  [DistributedHerder-connect-6-1:KafkaBasedLog@167] - Stopping KafkaBasedLog for topic mm2-offsets.primary.internal
2023-07-30 13:50:06,001 - INFO  [DistributedHerder-connect-6-1:KafkaProducer@1193] - [Producer clientId=producer-18] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:50:06,001 - INFO  [DistributedHerder-connect-6-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:50:06,001 - INFO  [DistributedHerder-connect-6-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:50:06,001 - INFO  [DistributedHerder-connect-6-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:50:06,001 - INFO  [DistributedHerder-connect-6-1:AppInfoParser@83] - App info kafka.producer for producer-18 unregistered
2023-07-30 13:50:06,002 - INFO  [DistributedHerder-connect-6-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:50:06,002 - INFO  [DistributedHerder-connect-6-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:50:06,002 - INFO  [DistributedHerder-connect-6-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:50:06,002 - INFO  [DistributedHerder-connect-6-1:AppInfoParser@83] - App info kafka.consumer for consumer-primary-mm2-16 unregistered
2023-07-30 13:50:06,002 - INFO  [DistributedHerder-connect-6-1:KafkaBasedLog@193] - Stopped KafkaBasedLog for topic mm2-offsets.primary.internal
2023-07-30 13:50:06,002 - INFO  [DistributedHerder-connect-6-1:KafkaOffsetBackingStore@136] - Stopped KafkaOffsetBackingStore
2023-07-30 13:50:06,002 - INFO  [DistributedHerder-connect-6-1:Metrics@668] - Metrics scheduler closed
2023-07-30 13:50:06,002 - INFO  [DistributedHerder-connect-6-1:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:50:06,003 - INFO  [DistributedHerder-connect-6-1:Metrics@678] - Metrics reporters closed
2023-07-30 13:50:06,003 - INFO  [DistributedHerder-connect-6-1:AppInfoParser@83] - App info kafka.connect for localhost:41733 unregistered
2023-07-30 13:50:06,003 - INFO  [DistributedHerder-connect-6-1:Worker@230] - Worker stopped
2023-07-30 13:50:06,003 - INFO  [DistributedHerder-connect-6-1:DistributedHerder@299] - [Worker clientId=connect-6, groupId=primary-mm2] Herder stopped
2023-07-30 13:50:06,004 - INFO  [main:DistributedHerder@701] - [Worker clientId=connect-6, groupId=primary-mm2] Herder stopped
2023-07-30 13:50:06,004 - INFO  [main:Connect@72] - Kafka Connect stopped
2023-07-30 13:50:06,004 - INFO  [main:KafkaProducer@1193] - [Producer clientId=producer-11] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2023-07-30 13:50:06,005 - INFO  [main:Metrics@668] - Metrics scheduler closed
2023-07-30 13:50:06,005 - INFO  [main:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:50:06,005 - INFO  [main:Metrics@678] - Metrics reporters closed
2023-07-30 13:50:06,005 - INFO  [main:AppInfoParser@83] - App info kafka.producer for producer-11 unregistered
2023-07-30 13:50:06,005 - INFO  [main:Logging@66] - [KafkaServer id=0] shutting down
2023-07-30 13:50:06,006 - INFO  [main:Logging@66] - [KafkaServer id=0] Starting controlled shutdown
2023-07-30 13:50:06,009 - INFO  [main:Logging@66] - [KafkaServer id=0] Controlled shutdown succeeded
2023-07-30 13:50:06,010 - INFO  [main:Logging@66] - [/config/changes-event-process-thread]: Shutting down
2023-07-30 13:50:06,010 - INFO  [main:Logging@66] - [/config/changes-event-process-thread]: Shutdown completed
2023-07-30 13:50:06,010 - INFO  [/config/changes-event-process-thread:Logging@66] - [/config/changes-event-process-thread]: Stopped
2023-07-30 13:50:06,010 - INFO  [main:Logging@66] - [SocketServer brokerId=0] Stopping socket server request processors
2023-07-30 13:50:06,012 - INFO  [main:Logging@66] - [SocketServer brokerId=0] Stopped socket server request processors
2023-07-30 13:50:06,012 - INFO  [main:Logging@66] - [data-plane Kafka Request Handler on Broker 0], shutting down
2023-07-30 13:50:06,012 - INFO  [main:Logging@66] - [data-plane Kafka Request Handler on Broker 0], shut down completely
2023-07-30 13:50:06,013 - INFO  [main:Logging@66] - [ExpirationReaper-0-AlterAcls]: Shutting down
2023-07-30 13:50:06,043 - INFO  [ExpirationReaper-0-AlterAcls:Logging@66] - [ExpirationReaper-0-AlterAcls]: Stopped
2023-07-30 13:50:06,043 - INFO  [main:Logging@66] - [ExpirationReaper-0-AlterAcls]: Shutdown completed
2023-07-30 13:50:06,044 - INFO  [main:Logging@66] - [KafkaApi-0] Shutdown complete.
2023-07-30 13:50:06,044 - INFO  [main:Logging@66] - [ExpirationReaper-0-topic]: Shutting down
2023-07-30 13:50:06,111 - WARN  [kafka-admin-client-thread | adminclient-76:NetworkClient@780] - [AdminClient clientId=adminclient-76] Connection to node 0 (localhost/127.0.0.1:45395) could not be established. Broker may not be available.
2023-07-30 13:50:06,112 - WARN  [kafka-admin-client-thread | adminclient-84:NetworkClient@780] - [AdminClient clientId=adminclient-84] Connection to node 0 (localhost/127.0.0.1:45395) could not be established. Broker may not be available.
2023-07-30 13:50:06,212 - WARN  [kafka-admin-client-thread | adminclient-84:NetworkClient@780] - [AdminClient clientId=adminclient-84] Connection to node 0 (localhost/127.0.0.1:45395) could not be established. Broker may not be available.
2023-07-30 13:50:06,226 - INFO  [SessionTracker:SessionTrackerImpl@163] - SessionTrackerImpl exited loop!
2023-07-30 13:50:06,244 - INFO  [ExpirationReaper-0-topic:Logging@66] - [ExpirationReaper-0-topic]: Stopped
2023-07-30 13:50:06,244 - INFO  [main:Logging@66] - [ExpirationReaper-0-topic]: Shutdown completed
2023-07-30 13:50:06,244 - INFO  [main:Logging@66] - [TransactionCoordinator id=0] Shutting down.
2023-07-30 13:50:06,245 - INFO  [main:Logging@66] - [ProducerId Manager 0]: Shutdown complete: last producerId assigned 0
2023-07-30 13:50:06,245 - INFO  [main:Logging@66] - [Transaction State Manager 0]: Shutdown complete
2023-07-30 13:50:06,245 - INFO  [main:Logging@66] - [Transaction Marker Channel Manager 0]: Shutting down
2023-07-30 13:50:06,245 - INFO  [main:Logging@66] - [Transaction Marker Channel Manager 0]: Shutdown completed
2023-07-30 13:50:06,245 - INFO  [TxnMarkerSenderThread-0:Logging@66] - [Transaction Marker Channel Manager 0]: Stopped
2023-07-30 13:50:06,245 - INFO  [main:Logging@66] - [TransactionCoordinator id=0] Shutdown complete.
2023-07-30 13:50:06,245 - INFO  [main:Logging@66] - [GroupCoordinator 0]: Shutting down.
2023-07-30 13:50:06,245 - INFO  [main:Logging@66] - [ExpirationReaper-0-Heartbeat]: Shutting down
2023-07-30 13:50:06,312 - WARN  [kafka-admin-client-thread | adminclient-76:NetworkClient@780] - [AdminClient clientId=adminclient-76] Connection to node 0 (localhost/127.0.0.1:45395) could not be established. Broker may not be available.
2023-07-30 13:50:06,413 - WARN  [kafka-admin-client-thread | adminclient-84:NetworkClient@780] - [AdminClient clientId=adminclient-84] Connection to node 0 (localhost/127.0.0.1:45395) could not be established. Broker may not be available.
2023-07-30 13:50:06,427 - INFO  [ExpirationReaper-0-Heartbeat:Logging@66] - [ExpirationReaper-0-Heartbeat]: Stopped
2023-07-30 13:50:06,427 - INFO  [main:Logging@66] - [ExpirationReaper-0-Heartbeat]: Shutdown completed
2023-07-30 13:50:06,427 - INFO  [main:Logging@66] - [ExpirationReaper-0-Rebalance]: Shutting down
2023-07-30 13:50:06,510 - INFO  [ExpirationReaper-0-Rebalance:Logging@66] - [ExpirationReaper-0-Rebalance]: Stopped
2023-07-30 13:50:06,510 - INFO  [main:Logging@66] - [ExpirationReaper-0-Rebalance]: Shutdown completed
2023-07-30 13:50:06,510 - INFO  [main:Logging@66] - [GroupCoordinator 0]: Shutdown complete.
2023-07-30 13:50:06,511 - INFO  [main:Logging@66] - [ReplicaManager broker=0] Shutting down
2023-07-30 13:50:06,511 - INFO  [main:Logging@66] - [LogDirFailureHandler]: Shutting down
2023-07-30 13:50:06,511 - INFO  [LogDirFailureHandler:Logging@66] - [LogDirFailureHandler]: Stopped
2023-07-30 13:50:06,511 - INFO  [main:Logging@66] - [LogDirFailureHandler]: Shutdown completed
2023-07-30 13:50:06,511 - INFO  [main:Logging@66] - [ReplicaFetcherManager on broker 0] shutting down
2023-07-30 13:50:06,511 - INFO  [main:Logging@66] - [ReplicaFetcherManager on broker 0] shutdown completed
2023-07-30 13:50:06,511 - INFO  [main:Logging@66] - [ReplicaAlterLogDirsManager on broker 0] shutting down
2023-07-30 13:50:06,511 - INFO  [main:Logging@66] - [ReplicaAlterLogDirsManager on broker 0] shutdown completed
2023-07-30 13:50:06,511 - INFO  [main:Logging@66] - [ExpirationReaper-0-Fetch]: Shutting down
2023-07-30 13:50:06,613 - WARN  [kafka-admin-client-thread | adminclient-76:NetworkClient@780] - [AdminClient clientId=adminclient-76] Connection to node 0 (localhost/127.0.0.1:45395) could not be established. Broker may not be available.
2023-07-30 13:50:06,628 - INFO  [ExpirationReaper-0-Fetch:Logging@66] - [ExpirationReaper-0-Fetch]: Stopped
2023-07-30 13:50:06,628 - INFO  [main:Logging@66] - [ExpirationReaper-0-Fetch]: Shutdown completed
2023-07-30 13:50:06,628 - INFO  [main:Logging@66] - [ExpirationReaper-0-Produce]: Shutting down
2023-07-30 13:50:06,644 - INFO  [ExpirationReaper-0-Produce:Logging@66] - [ExpirationReaper-0-Produce]: Stopped
2023-07-30 13:50:06,644 - INFO  [main:Logging@66] - [ExpirationReaper-0-Produce]: Shutdown completed
2023-07-30 13:50:06,644 - INFO  [main:Logging@66] - [ExpirationReaper-0-DeleteRecords]: Shutting down
2023-07-30 13:50:06,844 - INFO  [ExpirationReaper-0-DeleteRecords:Logging@66] - [ExpirationReaper-0-DeleteRecords]: Stopped
2023-07-30 13:50:06,844 - INFO  [main:Logging@66] - [ExpirationReaper-0-DeleteRecords]: Shutdown completed
2023-07-30 13:50:06,844 - INFO  [main:Logging@66] - [ExpirationReaper-0-ElectLeader]: Shutting down
2023-07-30 13:50:06,914 - WARN  [kafka-admin-client-thread | adminclient-84:NetworkClient@780] - [AdminClient clientId=adminclient-84] Connection to node 0 (localhost/127.0.0.1:45395) could not be established. Broker may not be available.
2023-07-30 13:50:06,988 - WARN  [kafka-admin-client-thread | adminclient-83:NetworkClient@780] - [AdminClient clientId=adminclient-83] Connection to node 0 (localhost/127.0.0.1:35687) could not be established. Broker may not be available.
2023-07-30 13:50:07,014 - WARN  [kafka-admin-client-thread | adminclient-76:NetworkClient@780] - [AdminClient clientId=adminclient-76] Connection to node 0 (localhost/127.0.0.1:45395) could not be established. Broker may not be available.
2023-07-30 13:50:07,044 - INFO  [ExpirationReaper-0-ElectLeader:Logging@66] - [ExpirationReaper-0-ElectLeader]: Stopped
2023-07-30 13:50:07,044 - INFO  [main:Logging@66] - [ExpirationReaper-0-ElectLeader]: Shutdown completed
2023-07-30 13:50:07,045 - INFO  [main:Logging@66] - [ReplicaManager broker=0] Shut down completely
2023-07-30 13:50:07,045 - INFO  [main:Logging@66] - [broker-0-to-controller-send-thread]: Shutting down
2023-07-30 13:50:07,045 - INFO  [main:Logging@66] - [broker-0-to-controller-send-thread]: Shutdown completed
2023-07-30 13:50:07,046 - INFO  [main:Logging@66] - [broker-0-to-controller-send-thread]: Shutdown completed
2023-07-30 13:50:07,045 - INFO  [broker-0-to-controller-send-thread:Logging@66] - [broker-0-to-controller-send-thread]: Stopped
2023-07-30 13:50:07,046 - INFO  [main:Logging@66] - Shutting down.
2023-07-30 13:50:07,048 - INFO  [pool-38-thread-1:Logging@66] - [ProducerStateManager partition=__consumer_offsets-35] Writing producer snapshot at offset 23
2023-07-30 13:50:07,062 - INFO  [pool-38-thread-1:Logging@66] - [ProducerStateManager partition=__consumer_offsets-17] Writing producer snapshot at offset 15
2023-07-30 13:50:07,077 - INFO  [main:Logging@66] - Shutdown complete.
2023-07-30 13:50:07,080 - INFO  [main:Logging@66] - [feature-zk-node-event-process-thread]: Shutting down
2023-07-30 13:50:07,080 - INFO  [main:Logging@66] - [feature-zk-node-event-process-thread]: Shutdown completed
2023-07-30 13:50:07,080 - INFO  [feature-zk-node-event-process-thread:Logging@66] - [feature-zk-node-event-process-thread]: Stopped
2023-07-30 13:50:07,080 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Closing.
2023-07-30 13:50:07,181 - INFO  [main:ZooKeeper@1422] - Session: 0x1087f31a64a0000 closed
2023-07-30 13:50:07,181 - INFO  [main-EventThread:ClientCnxn$EventThread@524] - EventThread shut down for session: 0x1087f31a64a0000
2023-07-30 13:50:07,182 - INFO  [main:Logging@66] - [ZooKeeperClient Kafka server] Closed.
2023-07-30 13:50:07,182 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Fetch]: Shutting down
2023-07-30 13:50:07,329 - INFO  [ThrottledChannelReaper-Fetch:Logging@66] - [ThrottledChannelReaper-Fetch]: Stopped
2023-07-30 13:50:07,329 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Fetch]: Shutdown completed
2023-07-30 13:50:07,330 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Produce]: Shutting down
2023-07-30 13:50:07,816 - WARN  [kafka-admin-client-thread | adminclient-76:NetworkClient@780] - [AdminClient clientId=adminclient-76] Connection to node 0 (localhost/127.0.0.1:45395) could not be established. Broker may not be available.
2023-07-30 13:50:07,916 - WARN  [kafka-admin-client-thread | adminclient-84:NetworkClient@780] - [AdminClient clientId=adminclient-84] Connection to node 0 (localhost/127.0.0.1:45395) could not be established. Broker may not be available.
2023-07-30 13:50:08,091 - WARN  [kafka-admin-client-thread | adminclient-83:NetworkClient@780] - [AdminClient clientId=adminclient-83] Connection to node 0 (localhost/127.0.0.1:35687) could not be established. Broker may not be available.
2023-07-30 13:50:08,330 - INFO  [ThrottledChannelReaper-Produce:Logging@66] - [ThrottledChannelReaper-Produce]: Stopped
2023-07-30 13:50:08,330 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Produce]: Shutdown completed
2023-07-30 13:50:08,330 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Request]: Shutting down
2023-07-30 13:50:08,330 - INFO  [ThrottledChannelReaper-Request:Logging@66] - [ThrottledChannelReaper-Request]: Stopped
2023-07-30 13:50:08,330 - INFO  [main:Logging@66] - [ThrottledChannelReaper-Request]: Shutdown completed
2023-07-30 13:50:08,330 - INFO  [main:Logging@66] - [ThrottledChannelReaper-ControllerMutation]: Shutting down
2023-07-30 13:50:08,918 - WARN  [kafka-admin-client-thread | adminclient-76:NetworkClient@780] - [AdminClient clientId=adminclient-76] Connection to node 0 (localhost/127.0.0.1:45395) could not be established. Broker may not be available.
2023-07-30 13:50:09,093 - WARN  [kafka-admin-client-thread | adminclient-83:NetworkClient@780] - [AdminClient clientId=adminclient-83] Connection to node 0 (localhost/127.0.0.1:35687) could not be established. Broker may not be available.
2023-07-30 13:50:09,118 - WARN  [kafka-admin-client-thread | adminclient-84:NetworkClient@780] - [AdminClient clientId=adminclient-84] Connection to node 0 (localhost/127.0.0.1:45395) could not be established. Broker may not be available.
2023-07-30 13:50:09,330 - INFO  [ThrottledChannelReaper-ControllerMutation:Logging@66] - [ThrottledChannelReaper-ControllerMutation]: Stopped
2023-07-30 13:50:09,330 - INFO  [main:Logging@66] - [ThrottledChannelReaper-ControllerMutation]: Shutdown completed
2023-07-30 13:50:09,331 - INFO  [main:Logging@66] - [SocketServer brokerId=0] Shutting down socket server
2023-07-30 13:50:09,339 - INFO  [main:Logging@66] - [SocketServer brokerId=0] Shutdown completed
2023-07-30 13:50:09,339 - INFO  [main:Metrics@668] - Metrics scheduler closed
2023-07-30 13:50:09,340 - INFO  [main:Metrics@672] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-07-30 13:50:09,340 - INFO  [main:Metrics@678] - Metrics reporters closed
2023-07-30 13:50:09,340 - INFO  [main:Logging@66] - Broker and topic stats closed
2023-07-30 13:50:09,340 - INFO  [main:AppInfoParser@83] - App info kafka.server for 0 unregistered
2023-07-30 13:50:09,340 - INFO  [main:Logging@66] - [KafkaServer id=0] shut down completed
2023-07-30 13:50:09,340 - INFO  [main:EmbeddedKafkaCluster@198] - Cleaning up kafka log dirs at ArraySeq(/tmp/junit7660765717748375067/junit866559250785637651)
2023-07-30 13:50:09,351 - INFO  [ConnnectionExpirer:NIOServerCnxnFactory$ConnectionExpirerThread@583] - ConnnectionExpirerThread interrupted
2023-07-30 13:50:09,352 - INFO  [NIOServerCxnFactory.SelectorThread-0:NIOServerCnxnFactory$SelectorThread@420] - selector thread exitted run method
2023-07-30 13:50:09,352 - INFO  [NIOServerCxnFactory.SelectorThread-1:NIOServerCnxnFactory$SelectorThread@420] - selector thread exitted run method
2023-07-30 13:50:09,352 - INFO  [NIOServerCxnFactory.SelectorThread-2:NIOServerCnxnFactory$SelectorThread@420] - selector thread exitted run method
2023-07-30 13:50:09,352 - INFO  [NIOServerCxnFactory.AcceptThread:/127.0.0.1:0:NIOServerCnxnFactory$AcceptThread@219] - accept thread exitted run method
2023-07-30 13:50:09,353 - INFO  [main:ZooKeeperServer@558] - shutting down
2023-07-30 13:50:09,353 - INFO  [main:SessionTrackerImpl@237] - Shutting down
2023-07-30 13:50:09,353 - INFO  [main:PrepRequestProcessor@1007] - Shutting down
2023-07-30 13:50:09,353 - INFO  [main:SyncRequestProcessor@191] - Shutting down
2023-07-30 13:50:09,353 - INFO  [ProcessThread(sid:0 cport:40297)::PrepRequestProcessor@155] - PrepRequestProcessor exited loop!
2023-07-30 13:50:09,354 - INFO  [SyncThread:0:SyncRequestProcessor@169] - SyncRequestProcessor exited!
2023-07-30 13:50:09,354 - INFO  [main:FinalRequestProcessor@514] - shutdown of request processor complete

Thanks for using JUnit! Support its development at https://junit.org/sponsoring

[36m[0m
[36m[0m [36mJUnit Jupiter[0m [32m[0m
[36m[0m [36mJUnit Vintage[0m [32m[0m
[36m   [0m [36mMirrorConnectorsIntegrationTest[0m [32m[0m
[36m      [0m [31mtestReplicationWithEmptyPartition[0m [36m31058 ms[0m [31m[0m [31mOffset of last partition is not zero expected:<0> but was:<1>[0m

Failures (1):
  JUnit Vintage:MirrorConnectorsIntegrationTest:testReplicationWithEmptyPartition
    MethodSource [className = 'org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest', methodName = 'testReplicationWithEmptyPartition', methodParameterTypes = '']
    => java.lang.AssertionError: Offset of last partition is not zero expected:<0> but was:<1>
       org.junit.Assert.fail(Assert.java:89)
       org.junit.Assert.failNotEquals(Assert.java:835)
       org.junit.Assert.assertEquals(Assert.java:647)
       org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.testReplicationWithEmptyPartition(MirrorConnectorsIntegrationTest.java:375)
       sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
       sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
       sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
       java.lang.reflect.Method.invoke(Method.java:498)
       org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
       org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
       [...]

Test run finished after 31100 ms
[         3 containers found      ]
[         0 containers skipped    ]
[         3 containers started    ]
[         0 containers aborted    ]
[         3 containers successful ]
[         0 containers failed     ]
[         1 tests found           ]
[         0 tests skipped         ]
[         1 tests started         ]
[         0 tests aborted         ]
[         0 tests successful      ]
[         1 tests failed          ]

